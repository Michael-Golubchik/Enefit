{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from mlxtend.evaluate.time_series import GroupTimeSeriesSplit, plot_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Время старта работы ноутбука\n",
    "notebook_starttime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращает сколько уже работает ноутбук\n",
    "def p_time():\n",
    "    #\n",
    "    run_time = round(time.time() - notebook_starttime)\n",
    "    return str(run_time).zfill(5)+' sec:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка: сабмит или локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ставим is_local в True, если локально работаем, если сабмитим - ставим в False\n",
    "#is_local = False\n",
    "is_local = True\n",
    "\n",
    "# Ставим is_gpu в True, если будем работать на GPU, если на процессоре - ставим в False\n",
    "# is_gpu = False\n",
    "is_gpu = True\n",
    "\n",
    "# Ставим is_tuning в True, если запускаем подбор гиперпараметров в Optuna\n",
    "is_tuning = False\n",
    "\n",
    "# Начальная дата обучения модели\n",
    "training_start_date = 'datetime >= \"2022-01-01 00:00:00\"'\n",
    "\n",
    "# Устанавливаем время начало переобучения. Начинаем переобучать модели заново,\n",
    "# когда подходит дата предсказаний, которая идет в скор.\n",
    "scor_start_time = pd.to_datetime('2023-06-01')\n",
    "\n",
    "# Ставим в False, если не хотитим отключать контроль времени выполнения ноутбука\n",
    "# (не обучать модели заново после 8 часов 30 минут)\n",
    "# Если хотим, чтобы модели обучались заново и после лимита, ставим True\n",
    "# is_disable_run_time_limit = True\n",
    "is_disable_run_time_limit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonthlyKFold:\n",
    "    def __init__(self, n_splits=3):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X, y, groups=None):\n",
    "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
    "        timesteps = sorted(dates.unique().tolist())\n",
    "        X = X.reset_index()\n",
    "        \n",
    "        for t in timesteps[-self.n_splits:]:\n",
    "            idx_train = X[dates.values < t].index\n",
    "            idx_test = X[dates.values == t].index\n",
    "            \n",
    "            yield idx_train, idx_test\n",
    "            \n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_tuning:\n",
    "    cv = MonthlyKFold()\n",
    "    CV = GroupTimeSeriesSplit(test_size=3, n_splits=3, window_type='rolling', shift_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days):\n",
    "    working_days = (\n",
    "        working_days\n",
    "        .with_columns(\n",
    "            pl.col(\"date\").cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_data = (\n",
    "        df_data\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_client = (\n",
    "        df_client\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_gas = (\n",
    "        df_gas\n",
    "        .rename({\"forecast_date\": \"date\"})\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_electricity = (\n",
    "        df_electricity\n",
    "        .rename({\"forecast_date\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\") + pl.duration(days=1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_location = (\n",
    "        df_location\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_forecast = (\n",
    "        df_forecast\n",
    "        .rename({\"forecast_datetime\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            #pl.col('datetime').dt.convert_time_zone(\"Europe/Bucharest\").dt.replace_time_zone(None).cast(pl.Datetime(\"us\")),\n",
    "            pl.col('datetime').dt.replace_time_zone(None).cast(pl.Datetime(\"us\"))\n",
    "            #pl.col('datetime').cast(pl.Datetime)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_historical = (\n",
    "        df_historical\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"datetime\") + pl.duration(hours=37)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_date = (\n",
    "        df_forecast\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_local = (\n",
    "        df_forecast\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    df_historical_date = (\n",
    "        df_historical\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_historical_local = (\n",
    "        df_historical\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    # Объединение всех обработанных данных с основным датафреймом df_data\n",
    "    df_data = (\n",
    "        df_data\n",
    "        .join(df_gas, on=\"date\", how=\"left\")\n",
    "        .join(df_client, on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n",
    "        .join(df_electricity, on=\"datetime\", how=\"left\")\n",
    "        \n",
    "        .join(df_forecast_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n",
    "        .join(df_forecast_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n",
    "        .join(df_historical_date, on=\"datetime\", how=\"left\", suffix=\"_hd\")\n",
    "        .join(df_historical_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl\")\n",
    "        \n",
    "        .join(df_forecast_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fdw\")\n",
    "        .join(df_forecast_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_flw\")\n",
    "        .join(df_historical_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hdw\")\n",
    "        .join(df_historical_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hlw\")\n",
    "        \n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        #.join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=8)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=9)).rename({\"target\": \"target_8\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=10)).rename({\"target\": \"target_9\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=11)).rename({\"target\": \"target_10\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=12)).rename({\"target\": \"target_11\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=13)).rename({\"target\": \"target_12\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_13\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        #.join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=335)).rename({\"target\": \"target_y_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        #.join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=340)).rename({\"target\": \"target_y_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        #.join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=365)).rename({\"target\": \"target_y_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        \n",
    "        #Добавляем коолонку рабочий день или нет.\n",
    "        .join(working_days, on=\"date\", how=\"left\")\n",
    "        # Создание категориальных признаков и тригонометрических функций времени\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"), # Добавление номера дня в году\n",
    "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),# Добавление часа\n",
    "            pl.col(\"datetime\").dt.day().alias(\"day\"),# Добавление дня\n",
    "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),# Добавление дня недели\n",
    "            pl.col(\"datetime\").dt.month().alias(\"month\"),# Добавление месяца\n",
    "            pl.col(\"datetime\").dt.year().alias(\"year\"),# Добавление года\n",
    "        )\n",
    "        # Приведение типов данных\n",
    "        .with_columns(\n",
    "            pl.concat_str(\"county\", \"is_business\", \"product_type\", \"is_consumption\", separator=\"_\").alias(\"category_1\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"), # Тригонометрические функции для дня в году\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).cast(pl.Float32),\n",
    "        )\n",
    "         # Удаление ненужных колонок\n",
    "        .drop(\"date\", \"datetime\", \"hour\", \"dayofyear\")\n",
    "    )\n",
    "    \n",
    "    # return df_data, df_historical_local\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(X, y=None):\n",
    "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
    "    \n",
    "    if y is not None:\n",
    "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
    "    else:\n",
    "        df = X.to_pandas()    \n",
    "\n",
    "\n",
    "    # Признаки из 12-го ноутбука Паши\n",
    "    # ****************************************\n",
    "    df = df.merge(df.groupby(['category_1', 'year', 'month', 'day'])['target_1'].sum().reset_index(),\n",
    "                  on=['category_1', 'year', 'month', 'day'],\n",
    "                  how='left',\n",
    "                  suffixes=['', '_sum'])\n",
    "    # ****************************************\n",
    "    \n",
    "    # Признаки из 14-го ноутбука Паши\n",
    "    # ****************************************\n",
    "    '''\n",
    "    grouped_df = df.groupby(['category_1', 'year', 'month', 'day'])\n",
    "    df = df.merge(grouped_df['target_1'].sum().reset_index(), on=['category_1', 'year', 'month', 'day'], how='left', suffixes=['', '_sum'])\n",
    "    df = df.merge(grouped_df['target_1'].mean().reset_index(), on=['category_1', 'year', 'month', 'day'], how='left', suffixes=['', '_mean'])\n",
    "    df = df.merge(grouped_df['target_6'].sum().reset_index(), on=['category_1', 'year', 'month', 'day'], how='left', suffixes=['', '_sum'])\n",
    "    df = df.merge(grouped_df['target_6'].mean().reset_index(), on=['category_1', 'year', 'month', 'day'], how='left', suffixes=['', '_mean'])\n",
    "    df = df.merge(grouped_df['target_13'].sum().reset_index(), on=['category_1', 'year', 'month', 'day'], how='left', suffixes=['', '_sum'])\n",
    "    df = df.merge(grouped_df['target_13'].mean().reset_index(), on=['category_1', 'year', 'month', 'day'], how='left', suffixes=['', '_mean'])\n",
    "    '''\n",
    "    # ****************************************\n",
    "    \n",
    "    df = df.set_index(\"row_id\")\n",
    "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "    \n",
    "    '''\n",
    "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
    "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
    "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
    "    '''\n",
    "    '''\n",
    "    df[\"target_mean_1\"] = df[[f\"target_{i}\" for i in range(1, 13)]].mean(1)\n",
    "    df[\"target_std_1\"] = df[[f\"target_{i}\" for i in range(1, 13)]].std(1)\n",
    "    df[\"target_mean_2\"] = df[[f\"target_{i}\" for i in range(1, 6)]].mean(1)\n",
    "    df[\"target_std_2\"] = df[[f\"target_{i}\" for i in range(1, 6)]].std(1)\n",
    "    df[\"target_ratio_1\"] = df[\"target_1\"] / (df[\"target_13\"] + 1e-3)\n",
    "    df[\"target_ratio_2\"] = df[\"target_1\"] / (df[\"target_6\"] + 1e-3)\n",
    "    df[\"target_ratio_3\"] = df[\"target_6\"] / (df[\"target_13\"] + 1e-3)\n",
    "    '''\n",
    "\n",
    "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 13)]].mean(1)\n",
    "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 13)]].std(1)\n",
    "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_13\"] + 1e-3)\n",
    "    '''\n",
    "    df[\"target_ratio_y_1\"] = df[\"target_y_1\"] / (df[\"target_y_3\"] + 1e-3)\n",
    "    df[\"target_ratio_y_2\"] = df[\"target_y_2\"] / (df[\"target_y_3\"] + 1e-3)\n",
    "    '''\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "# Для локальных вычислений. Последний data_block_id тренировочной выборки\n",
    "# А начиная со следующего data_block_id и до конца идет тест\n",
    "train_end_data_block_id = 500\n",
    "# train_end_data_block_id = 600\n",
    "\n",
    "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id', 'data_block_id']\n",
    "# В df_data_cols колонки в таком порядке в каком они потом формируются в df_data\n",
    "df_data_cols     = ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'datetime', 'data_block_id', 'row_id']\n",
    "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
    "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
    "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
    "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
    "location_cols    = ['longitude', 'latitude', 'county']\n",
    "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
    "\n",
    "save_path = None\n",
    "load_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для оптуны\n",
    "# Диапазон гиперпараметров для модели обучающейся на данных is_consumption=1 \n",
    "def lgb_objective_cons1(trial):\n",
    "    params = {\n",
    "        'device'            : trial.suggest_categorical('device', ['gpu']),\n",
    "        'gpu_platform_id'   : trial.suggest_int('gpu_platform_id', 1, 1),\n",
    "        'gpu_device_id'     : trial.suggest_int('gpu_device_id', 0, 0),\n",
    "        'n_estimators'      : trial.suggest_int('n_estimators', 1000, 2000),\n",
    "        'verbose'           : trial.suggest_int('verbose', -1, -1),\n",
    "        'random_state'      : trial.suggest_int('random_state', 42, 42),\n",
    "        'objective'         : trial.suggest_categorical('objective', ['l2']),\n",
    "        'num_leaves'        : trial.suggest_int('num_leaves', 20, 50),\n",
    "        'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'colsample_bytree'  : trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'colsample_bynode'  : trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        #'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 20.0),\n",
    "        'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 10.0),\n",
    "        'reg_lambda'        : trial.suggest_float('reg_lambda', 1e-2, 20.0),\n",
    "        'min_child_samples' : trial.suggest_int('min_child_samples', 4, 256),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', -1, -1),\n",
    "        'max_bin'           : trial.suggest_int('max_bin', 32, 200),\n",
    "    }\n",
    "    \n",
    "    model  = lgb.LGBMRegressor(**params)\n",
    "    X      = df_train[df_train['is_consumption']==1].drop(columns=[\"target\", \"datetime\"]).reset_index(drop=True)\n",
    "    y      = df_train[df_train['is_consumption']==1][\"target\"].reset_index(drop=True)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Диапазон гиперпараметров для модели обучающейся на данных is_consumption=0\n",
    "def lgb_objective_cons0(trial):\n",
    "    params = {\n",
    "        'device'            : trial.suggest_categorical('device', ['gpu']),\n",
    "        'gpu_platform_id'   : trial.suggest_int('gpu_platform_id', 1, 1),\n",
    "        'gpu_device_id'     : trial.suggest_int('gpu_device_id', 0, 0),\n",
    "        'n_estimators'      : trial.suggest_int('n_estimators', 1000, 2000),\n",
    "        'verbose'           : trial.suggest_int('verbose', -1, -1),\n",
    "        'random_state'      : trial.suggest_int('random_state', 42, 42),\n",
    "        'objective'         : trial.suggest_categorical('objective', ['l2']),\n",
    "        'num_leaves'        : trial.suggest_int('num_leaves', 20, 50),\n",
    "        'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'colsample_bytree'  : trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'colsample_bynode'  : trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        #'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 20.0),\n",
    "        #'reg_lambda'        : trial.suggest_float('reg_lambda', 1e-2, 20.0),\n",
    "        'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 10.0),\n",
    "        'reg_lambda'        : trial.suggest_float('reg_lambda', 12, 20.0),\n",
    "        'min_child_samples' : trial.suggest_int('min_child_samples', 4, 256),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', -1, -1),\n",
    "        'max_bin'           : trial.suggest_int('max_bin', 32, 200),\n",
    "    }\n",
    "    \n",
    "    model  = lgb.LGBMRegressor(**params)\n",
    "    X      = df_train[df_train['is_consumption']==0].drop(columns=[\"target\", \"datetime\"]).reset_index(drop=True)\n",
    "    y      = df_train[df_train['is_consumption']==0][\"target\"].reset_index(drop=True)\n",
    "    scores = cross_val_score(model, X, y, groups=groups, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Загрузка данных об энергопотреблении\n",
    "    train = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "    \n",
    "    # Создание сводной таблицы с средними значениями целевой переменной (target)\n",
    "    # для каждой комбинации даты, округа, типа продукта, бизнеса и потребления\n",
    "    pivot_train = train.pivot_table(\n",
    "        index='datetime',\n",
    "        columns=['county', 'product_type', 'is_business', 'is_consumption'],\n",
    "        values='target',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Переименование колонок для удобства доступа и интерпретации\n",
    "    pivot_train.columns = ['county{}_productType{}_isBusiness{}_isConsumption{}'.format(*col) for col in pivot_train.columns.values]\n",
    "    pivot_train.index = pd.to_datetime(pivot_train.index)\n",
    "    \n",
    "    pivot_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023 год "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Копирование сводной таблицы для визуализации\n",
    "    df_plot = pivot_train.copy()\n",
    "    \n",
    "    # Нормализация данных для визуализации\n",
    "    df_plot = (df_plot - df_plot.min()) / (df_plot.max() - df_plot.min())\n",
    "    \n",
    "    # Ресемплирование данных по дням и вычисление средних значений\n",
    "    df_plot_resampled_D = df_plot.resample('D').mean()\n",
    "    \n",
    "    # Визуализация нормализованных данных с прозрачностью (alpha=0.1)\n",
    "    df_plot_resampled_D.loc['2022-7':].plot(alpha=0.1, color='green', figsize=(18, 6), legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Выбор колонок, соответствующих различным категориям потребления\n",
    "    columns_consumption_0 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption0')]\n",
    "    columns_consumption_1 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption1')]\n",
    "    \n",
    "    # Создание фигуры для визуализации\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Создание пустых линий для легенды\n",
    "    plt.plot([], color='red', label='is_Consumption = 1')  # Изменено на желтый цвет\n",
    "    plt.plot([], color='black', label='is_Consumption = 0')   # Изменено на черный цвет\n",
    "    \n",
    "    # Отображение легенды\n",
    "    plt.legend()\n",
    "    \n",
    "    # Визуализация данных для 'is_Consumption = 0' черным цветом\n",
    "    for column in columns_consumption_0:\n",
    "        df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='black', legend=False)  # Изменено на черный\n",
    "    \n",
    "    # Визуализация данных для 'is_Consumption = 1' желтым цветом\n",
    "    for column in columns_consumption_1:\n",
    "        df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='red', legend=False)  # Изменено на желтый\n",
    "    \n",
    "    # Отображение графика\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись тестовых и тренировочных csv файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Если выполняем локально\n",
    "    train_path = 'train'\n",
    "    if not os.path.exists(train_path):\n",
    "        # Создание каталога, если его нет\n",
    "        os.makedirs(train_path)\n",
    "    test_path = 'example_test_files'\n",
    "    if not os.path.exists(test_path):\n",
    "        # Создание каталога, если его нет\n",
    "        os.makedirs(test_path)\n",
    "else:\n",
    "    # Если сабмит\n",
    "    train_path = root\n",
    "# Путь, куда запишем csv файлы для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяет датафрейм на тренировочную и тестовую часть\n",
    "# Возвращает часть датафрейма для тренировки, тестовую часть датафрейма записывает в каталог с тестами\n",
    "def split_train_test(filename):\n",
    "    df = pd.read_csv(os.path.join(root, filename))\n",
    "    \n",
    "    #Запишем часть данных для теста\n",
    "    test_df = df[df[\"data_block_id\"] > train_end_data_block_id]\n",
    "    if (filename ==\"train.csv\"):\n",
    "        # Берем только те ячейки где target был не нулевым\n",
    "        test_df = test_df[test_df[\"target\"].notnull()]\n",
    "        \n",
    "    test_df.to_csv(os.path.join(test_path, filename), index=False)\n",
    "\n",
    "    #Запишем часть данных для трейна\n",
    "    train_df = df[df[\"data_block_id\"] <= train_end_data_block_id]\n",
    "    train_df.to_csv(os.path.join(train_path, filename), index=False)\n",
    "\n",
    "# Доводим до ума тестовые таблицы чтобы они были точно такие как в реальном сабмите\n",
    "def test_dfs_tune():\n",
    "    # Делаем таблицу revealed_targets.csv\n",
    "    df = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "    df = df[df[\"data_block_id\"] > train_end_data_block_id - 2]\n",
    "    df[\"data_block_id\"] += 2\n",
    "    df = df[df[\"target\"].notnull()]\n",
    "    df.to_csv(os.path.join(test_path, 'revealed_targets.csv'), index=False)\n",
    "    \n",
    "    # Делаем таблицу test.csv\n",
    "    df = pd.read_csv(os.path.join(test_path, \"train.csv\"))\n",
    "    df.rename(columns={'datetime': 'prediction_datetime'}, inplace=True)\n",
    "    df.drop('target', axis=1, inplace=True)\n",
    "    df['currently_scored'] = False\n",
    "    df.to_csv(os.path.join(test_path, 'test.csv'), index=False)\n",
    "    \n",
    "    # Делаем таблицу sample_submission.csv\n",
    "    selected_columns = ['row_id', 'data_block_id']\n",
    "    df = df[selected_columns]\n",
    "    df['target'] = 0\n",
    "    df.to_csv(os.path.join(test_path, 'sample_submission.csv'), index=False)\n",
    "\n",
    "# Сборка разделения файлов\n",
    "def make_split():\n",
    "    # csv файлы которые будем делить:\n",
    "    csv_names = [\"train.csv\", \"client.csv\", \"gas_prices.csv\", \"electricity_prices.csv\", \"forecast_weather.csv\", \"historical_weather.csv\"]\n",
    "    for csv_name in csv_names:\n",
    "        split_train_test(csv_name)\n",
    "    # Доделываем тестовые таблицы\n",
    "    test_dfs_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Создаем файлы csv c тренировочными и тестовыми таблицами\n",
    "if is_local:\n",
    "    # Пока отключил создание тестовых файлом. У меня локально они есть\n",
    "    #make_split()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data        = pl.read_csv(os.path.join(train_path, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
    "df_client      = pl.read_csv(os.path.join(train_path, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
    "df_gas         = pl.read_csv(os.path.join(train_path, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
    "df_electricity = pl.read_csv(os.path.join(train_path, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
    "df_forecast    = pl.read_csv(os.path.join(train_path, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
    "df_historical  = pl.read_csv(os.path.join(train_path, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
    "#df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
    "df_location    = pl.read_csv('/kaggle/input/locations/county_lon_lats.csv', columns=location_cols, try_parse_dates=True)\n",
    "df_target      = df_data.select(target_cols)\n",
    "working_days   = pl.read_csv('/kaggle/input/working-days/working_days.csv', try_parse_dates=True)\n",
    "\n",
    "schema_data        = df_data.schema\n",
    "schema_client      = df_client.schema\n",
    "schema_gas         = df_gas.schema\n",
    "schema_electricity = df_electricity.schema\n",
    "schema_forecast    = df_forecast.schema\n",
    "schema_historical  = df_historical.schema\n",
    "schema_target      = df_target.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для поиска гиперпараметров\n",
    "if is_tuning:\n",
    "    X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "    X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "    df_train = to_pandas(X, y)\n",
    "\n",
    "    # df_train = df_train[df_train[\"target\"].notnull()].query(training_start_date)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор гиперпараметров для модели is_consumption=1\n",
    "if is_tuning:\n",
    "    study = optuna.create_study(direction='minimize', study_name='Regressor')\n",
    "    study.optimize(lgb_objective_cons1, n_trials=100, show_progress_bar=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор гиперпараметров для модели is_consumption=0\n",
    "if is_tuning:\n",
    "    study = optuna.create_study(direction='minimize', study_name='Regressor')\n",
    "    study.optimize(lgb_objective_cons0, n_trials=100, show_progress_bar=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result = cross_validate(\n",
    "    estimator=lgb.LGBMRegressor(**best_params, random_state=42),\n",
    "    X=df_train.drop(columns=[\"target\"]), \n",
    "    y=df_train[\"target\"],\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=MonthlyKFold(1),\n",
    ")\n",
    "\n",
    "print(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\n",
    "print(f\"Score Time(s): {result['score_time'].mean():.3f}\")\n",
    "print(f\"Error(MAE): {-result['test_score'].mean():.3f}\")'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB is_consumption=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if is_gpu:\n",
    "    p1={'device': 'gpu', 'n_estimators': 1469, 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'num_leaves': 39, 'learning_rate': 0.08040990249470174, 'colsample_bytree': 0.8531849083888259, 'colsample_bynode': 0.9214154009431396, 'reg_alpha': 6.929103452856734, 'reg_lambda': 10.279437815903039, 'min_child_samples': 49, 'max_depth': -1, 'max_bin': 126}\n",
    "\n",
    "    p2={'device': 'gpu', 'n_estimators': 1746, 'verbose': -1, 'random_state': 41, 'objective': 'l2', 'num_leaves': 41, 'learning_rate': 0.08796044370229837, 'colsample_bytree': 0.7546586583194814, 'colsample_bynode': 0.9994687314470938, 'reg_alpha': 6.1003717335294585, 'reg_lambda': 7.696527980193375, 'min_child_samples': 48, 'max_depth': -1, 'max_bin': 127}\n",
    "\n",
    "    p3={'device': 'gpu', 'n_estimators': 1294, 'verbose': -1, 'random_state': 43, 'objective': 'l2', 'num_leaves': 33, 'learning_rate': 0.08737561850018681, 'colsample_bytree': 0.8699017760704414, 'colsample_bynode': 0.9978274134547724, 'reg_alpha': 5.898515471116359, 'reg_lambda': 17.17473296794324, 'min_child_samples': 6, 'max_depth': -1, 'max_bin': 131}\n",
    "\n",
    "    p4={'device': 'gpu', 'n_estimators': 1192, 'verbose': -1, 'random_state': 44, 'objective': 'l2', 'num_leaves': 38, 'learning_rate': 0.0872521925642551, 'colsample_bytree': 0.721555382595741, 'colsample_bynode': 0.9995199827970517, 'reg_alpha': 5.841920140684635, 'reg_lambda': 15.39391905374745, 'min_child_samples': 40, 'max_depth': -1, 'max_bin': 98}\n",
    "\n",
    "    p5={'device': 'gpu', 'n_estimators': 1235, 'verbose': -1, 'random_state': 45, 'objective': 'l2', 'num_leaves': 43, 'learning_rate': 0.0908952879560077, 'colsample_bytree': 0.7105327970235108, 'colsample_bynode': 0.6641934858040608, 'reg_alpha': 5.760171959868691, 'reg_lambda': 6.087301777183299, 'min_child_samples': 70, 'max_depth': -1, 'max_bin': 128}\n",
    "\n",
    "    p6={'device': 'gpu', 'n_estimators': 1474, 'verbose': -1, 'random_state': 46, 'objective': 'l2', 'num_leaves': 31, 'learning_rate': 0.09206716453416329, 'colsample_bytree': 0.7435685123828456, 'colsample_bynode': 0.953901546054432, 'reg_alpha': 6.282388240444042, 'reg_lambda': 11.93184947089518, 'min_child_samples': 55, 'max_depth': -1, 'max_bin': 129}\n",
    "\n",
    "    p7={'device': 'gpu', 'n_estimators': 1425, 'verbose': -1, 'random_state': 47, 'objective': 'l2', 'num_leaves': 29, 'learning_rate': 0.09761288166264291, 'colsample_bytree': 0.7780272286445504, 'colsample_bynode': 0.6312053815371705, 'reg_alpha': 9.269919639881412, 'reg_lambda': 13.182320657371639, 'min_child_samples': 16, 'max_depth': -1, 'max_bin': 153}\n",
    "    \n",
    "    p8={'device': 'gpu', 'verbose': -1, 'random_state' : 48, 'objective': 'l2', 'n_estimators': 1569, 'learning_rate': 0.0954350912359592, 'colsample_bytree': 0.5643173334694846, 'colsample_bynode': 0.8507963404594194, 'reg_alpha': 0.5815184353302838, 'reg_lambda': 19.577675580975, 'min_child_samples': 83, 'max_depth': 15, 'max_bin': 125}\n",
    "    \n",
    "    p9={'device': 'gpu', 'verbose': -1, 'random_state' : 49, 'objective': 'l2', 'n_estimators': 1655, 'learning_rate': 0.09749234861597421, 'colsample_bytree': 0.5878083872493073, 'colsample_bynode': 0.7476463591217907, 'reg_alpha': 0.6253329425224532, 'reg_lambda': 19.31104697178572, 'min_child_samples': 41, 'max_depth': 15, 'max_bin': 116}\n",
    "\n",
    "    p10={'device': 'gpu', 'verbose': -1, 'random_state' : 50, 'objective': 'l2', 'n_estimators': 1702, 'learning_rate': 0.08956547486313553, 'colsample_bytree': 0.553841254939378, 'colsample_bynode': 0.7707977076873439, 'reg_alpha': 3.4503195735482803, 'reg_lambda': 17.09137108374253, 'min_child_samples': 46, 'max_depth': 16, 'max_bin': 123}\n",
    "\n",
    "    p11={'device': 'gpu', 'verbose': -1, 'random_state' : 51, 'objective': 'l2', 'n_estimators': 1662, 'learning_rate': 0.09936096276241443, 'colsample_bytree': 0.5262782304338314, 'colsample_bynode': 0.6861424640373375, 'reg_alpha': 1.1434350179754031, 'reg_lambda': 18.238183112413253, 'min_child_samples': 42, 'max_depth': 16, 'max_bin': 118}\n",
    "\n",
    "    p12={'device': 'gpu', 'verbose': -1, 'random_state' : 52, 'objective': 'l2', 'n_estimators': 1596, 'learning_rate': 0.09145526299958297, 'colsample_bytree': 0.5813223295308532, 'colsample_bynode': 0.8019876742272201, 'reg_alpha': 1.7506271227424264, 'reg_lambda': 19.97101707600914, 'min_child_samples': 45, 'max_depth': 16, 'max_bin': 129}\n",
    "\n",
    "    p13={'device': 'gpu', 'verbose': -1, 'random_state' : 53, 'objective': 'l2', 'n_estimators': 1611, 'learning_rate': 0.09177637954991985, 'colsample_bytree': 0.5406014712979323, 'colsample_bynode': 0.7948709121454631, 'reg_alpha': 1.4162076884265264, 'reg_lambda': 19.18451476080634, 'min_child_samples': 39, 'max_depth': 15, 'max_bin': 128}\n",
    "\n",
    "    p14={'device': 'gpu', 'verbose': -1, 'random_state' : 54, 'objective': 'l2', 'n_estimators': 1722, 'learning_rate': 0.09721924587634734, 'colsample_bytree': 0.581822849292829, 'colsample_bynode': 0.8475502365142857, 'reg_alpha': 0.5558295705320715, 'reg_lambda': 17.676848729848402, 'min_child_samples': 76, 'max_depth': 15, 'max_bin': 121}\n",
    "\n",
    "    p15={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1953, 'learning_rate': 0.06292846025674187, 'colsample_bytree': 0.885312594375526, 'colsample_bynode': 0.7831589504736596, 'reg_alpha': 7.677890737474055, 'reg_lambda': 16.04294344413536, 'min_child_samples': 69, 'max_depth': 13, 'max_bin': 122}\n",
    "    p16={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1781, 'learning_rate': 0.054763617969941024, 'colsample_bytree': 0.9233976730768843, 'colsample_bynode': 0.8969396545503117, 'reg_alpha': 9.893144785987541, 'reg_lambda': 19.956571072303618, 'min_child_samples': 30, 'max_depth': 11, 'max_bin': 128}\n",
    "    p17={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1713, 'learning_rate': 0.053743689746779025, 'colsample_bytree': 0.8813612023144495, 'colsample_bynode': 0.8739181786588937, 'reg_alpha': 6.967728410825889, 'reg_lambda': 18.772923941576302, 'min_child_samples': 8, 'max_depth': 11, 'max_bin': 130}\n",
    "    p18={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1852, 'learning_rate': 0.0592383731526721, 'colsample_bytree': 0.9158803696144284, 'colsample_bynode': 0.8677708234107011, 'reg_alpha': 8.178421543960194, 'reg_lambda': 18.453388032680913, 'min_child_samples': 7, 'max_depth': 13, 'max_bin': 127}\n",
    "    p19={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1854, 'learning_rate': 0.06206288936616207, 'colsample_bytree': 0.8880309956381198, 'colsample_bynode': 0.8902254554150658, 'reg_alpha': 8.741757862603745, 'reg_lambda': 16.131881076970146, 'min_child_samples': 11, 'max_depth': 13, 'max_bin': 133}\n",
    "    p20={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1760, 'learning_rate': 0.05058624430058491, 'colsample_bytree': 0.8623393986024189, 'colsample_bynode': 0.9242352457544074, 'reg_alpha': 10.403491499807716, 'reg_lambda': 17.336966880450976, 'min_child_samples': 16, 'max_depth': 12, 'max_bin': 131}\n",
    "    p21={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1783, 'learning_rate': 0.053984137286090667, 'colsample_bytree': 0.7988556940541043, 'colsample_bynode': 0.8962066698736209, 'reg_alpha': 10.033954743421075, 'reg_lambda': 19.869585514651835, 'min_child_samples': 29, 'max_depth': 11, 'max_bin': 128}\n",
    "    p22={'device': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06258413085998576, 'colsample_bytree': 0.6527661140701613, 'colsample_bynode': 0.8106858631408332, 'lambda_l1': 5.065645378814257, 'lambda_l2': 9.81159370218779, 'min_data_in_leaf': 192, 'max_depth': 10, 'max_bin': 250}\n",
    "    p23={'device': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.0632167263149817, 'colsample_bytree': 0.6958033941948067, 'colsample_bynode': 0.6030801666196094, 'lambda_l1': 7.137580620471935, 'lambda_l2': 9.348169401713742, 'min_data_in_leaf': 74, 'max_depth': 11, 'max_bin': 220}\n",
    "    p24={'device': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.061236402165228264, 'colsample_bytree': 0.81427095118471, 'colsample_bynode': 0.6097376843527067, 'lambda_l1': 6.360490880385201, 'lambda_l2': 9.954136008333839, 'min_data_in_leaf': 238, 'max_depth': 13, 'max_bin': 180}\n",
    "else:\n",
    "    pass\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB is_consumption=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if is_gpu:\n",
    "    # Параметры для lgbm c GPU\n",
    "    c1={'device': 'gpu', 'n_estimators': 1873, 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.06628595395939138, 'colsample_bytree': 0.8413540714224521, 'colsample_bynode': 0.8666339129825499, 'reg_alpha': 9.255223781602002, 'reg_lambda': 16.025856453220072, 'min_child_samples': 124, 'max_depth': -1, 'max_bin': 36}\n",
    "\n",
    "    c2={'device': 'gpu', 'n_estimators': 1706, 'verbose': -1, 'random_state': 41, 'objective': 'l2', 'num_leaves': 40, 'learning_rate': 0.0623359455604657, 'colsample_bytree': 0.9080615436043814, 'colsample_bynode': 0.9159548001426139, 'reg_alpha': 8.494878682040135, 'reg_lambda': 16.767807661924376, 'min_child_samples': 132, 'max_depth': -1, 'max_bin': 35}\n",
    "\n",
    "    c3={'device': 'gpu', 'n_estimators': 1692, 'verbose': -1, 'random_state': 43, 'objective': 'l2', 'num_leaves': 50, 'learning_rate': 0.06467681942915654, 'colsample_bytree': 0.7276162343708921, 'colsample_bynode': 0.8958940747299091, 'reg_alpha': 9.481145856323984, 'reg_lambda': 16.29339982949364, 'min_child_samples': 151, 'max_depth': -1, 'max_bin': 43}\n",
    "\n",
    "    c4={'device': 'gpu', 'n_estimators': 1739, 'verbose': -1, 'random_state': 44, 'objective': 'l2', 'num_leaves': 44, 'learning_rate': 0.06080249451894554, 'colsample_bytree': 0.9074942761725775, 'colsample_bynode': 0.9170081480548385, 'reg_alpha': 8.530769407103739, 'reg_lambda': 16.81737853958101, 'min_child_samples': 109, 'max_depth': -1, 'max_bin': 35}\n",
    "\n",
    "    c5={'device': 'gpu', 'n_estimators': 1826, 'verbose': -1, 'random_state': 45, 'objective': 'l2', 'num_leaves': 49, 'learning_rate': 0.05467489957746241, 'colsample_bytree': 0.7682313698228109, 'colsample_bynode': 0.8873377406908334, 'reg_alpha': 9.966772322142026, 'reg_lambda': 16.04339228378297, 'min_child_samples': 129, 'max_depth': -1, 'max_bin': 32}\n",
    "\n",
    "    c6={'device': 'gpu', 'n_estimators': 1735, 'verbose': -1, 'random_state': 46, 'objective': 'l2', 'num_leaves': 41, 'learning_rate': 0.060385171871545046, 'colsample_bytree': 0.9106107260853121, 'colsample_bynode': 0.9133013005189936, 'reg_alpha': 8.215835720465444, 'reg_lambda': 16.82269739762653, 'min_child_samples': 140, 'max_depth': -1, 'max_bin': 32}\n",
    "\n",
    "    c7={'device': 'gpu', 'n_estimators': 1740, 'verbose': -1, 'random_state': 47, 'objective': 'l2', 'num_leaves': 41, 'learning_rate': 0.06813066410209488, 'colsample_bytree': 0.967948306909385, 'colsample_bynode': 0.9089768944050921, 'reg_alpha': 8.8593247133808, 'reg_lambda': 16.750261541012176, 'min_child_samples': 114, 'max_depth': -1, 'max_bin': 41}\n",
    "\n",
    "    c8={'device': 'gpu', 'verbose': -1, 'random_state' : 48, 'objective': 'l2', 'n_estimators': 1961, 'learning_rate': 0.055041729559669385, 'colsample_bytree': 0.8054200071555299, 'colsample_bynode': 0.8827333010526346, 'reg_alpha': 9.649253442752036, 'reg_lambda': 16.98908601233005, 'min_child_samples': 54, 'max_depth': 12, 'max_bin': 36}\n",
    "\n",
    "    c9={'device': 'gpu', 'verbose': -1, 'random_state' : 49, 'objective': 'l2', 'n_estimators': 1948, 'learning_rate': 0.06233594141892915, 'colsample_bytree': 0.8484245099171761, 'colsample_bynode': 0.899824429438312, 'reg_alpha': 10.7294451589117, 'reg_lambda': 17.69396992827211, 'min_child_samples': 45, 'max_depth': 13, 'max_bin': 32}\n",
    "\n",
    "    c10={'device': 'gpu', 'verbose': -1, 'random_state' : 50, 'objective': 'l2', 'n_estimators': 1995, 'learning_rate': 0.0633853242094111, 'colsample_bytree': 0.9331894149297775, 'colsample_bynode': 0.972632605889707, 'reg_alpha': 8.983517796023829, 'reg_lambda': 18.03334867391121, 'min_child_samples': 33, 'max_depth': 11, 'max_bin': 48}\n",
    "\n",
    "    c11={'device': 'gpu', 'verbose': -1, 'random_state' : 51, 'objective': 'l2', 'n_estimators': 1948, 'learning_rate': 0.04980982203453618, 'colsample_bytree': 0.7850607568025258, 'colsample_bynode': 0.9990467893228645, 'reg_alpha': 8.279232611894738, 'reg_lambda': 18.878856721521696, 'min_child_samples': 85, 'max_depth': 12, 'max_bin': 32}\n",
    "\n",
    "    c12={'device': 'gpu', 'verbose': -1, 'random_state' : 52, 'objective': 'l2', 'n_estimators': 1958, 'learning_rate': 0.06331649649993518, 'colsample_bytree': 0.965107198312526, 'colsample_bynode': 0.9562601410444004, 'reg_alpha': 8.595100697458118, 'reg_lambda': 19.672841466470988, 'min_child_samples': 38, 'max_depth': 14, 'max_bin': 37}\n",
    "\n",
    "    c13={'device': 'gpu', 'verbose': -1, 'random_state' : 53, 'objective': 'l2', 'n_estimators': 1667, 'learning_rate': 0.06761829944908236, 'colsample_bytree': 0.8491878204722972, 'colsample_bynode': 0.7943301198687824, 'reg_alpha': 12.053887346204482, 'reg_lambda': 17.21503593146002, 'min_child_samples': 48, 'max_depth': 12, 'max_bin': 60}\n",
    "\n",
    "    c14={'device': 'gpu', 'verbose': -1, 'random_state' : 54, 'objective': 'l2', 'n_estimators': 1965, 'learning_rate': 0.06236463049661708, 'colsample_bytree': 0.9209979630548757, 'colsample_bynode': 0.9718755549743984, 'reg_alpha': 9.485421137815203, 'reg_lambda': 19.728001606564117, 'min_child_samples': 24, 'max_depth': 14, 'max_bin': 35}\n",
    "\n",
    "    c15={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1928, 'learning_rate': 0.07746737348328141, 'colsample_bytree': 0.78312363495825, 'colsample_bynode': 0.519093615242415, 'reg_alpha': 3.5475701282438967, 'reg_lambda': 17.88655785653258, 'min_child_samples': 20, 'max_depth': 17, 'max_bin': 37}\n",
    "    c16={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1881, 'learning_rate': 0.07864103296902626, 'colsample_bytree': 0.7910482238038328, 'colsample_bynode': 0.48895580052374654, 'reg_alpha': 7.375666493081315, 'reg_lambda': 15.347283246448637, 'min_child_samples': 15, 'max_depth': 15, 'max_bin': 41}\n",
    "    c17={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1959, 'learning_rate': 0.06577095971205155, 'colsample_bytree': 0.8648745524121579, 'colsample_bynode': 0.4208704631805802, 'reg_alpha': 7.334717669004279, 'reg_lambda': 19.53863883138691, 'min_child_samples': 17, 'max_depth': 15, 'max_bin': 37}\n",
    "    c18={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1962, 'learning_rate': 0.06171792620856953, 'colsample_bytree': 0.9584819437986425, 'colsample_bynode': 0.5405402088572034, 'reg_alpha': 3.307191570022457, 'reg_lambda': 16.209300259712922, 'min_child_samples': 43, 'max_depth': 19, 'max_bin': 40}\n",
    "    c19={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1900, 'learning_rate': 0.07441152178622895, 'colsample_bytree': 0.8801531103073503, 'colsample_bynode': 0.337510749744266, 'reg_alpha': 6.105532634402442, 'reg_lambda': 19.489948983222703, 'min_child_samples': 28, 'max_depth': 11, 'max_bin': 51}\n",
    "    c20={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1923, 'learning_rate': 0.07413128975186019, 'colsample_bytree': 0.9108240472209401, 'colsample_bynode': 0.368498860839913, 'reg_alpha': 5.928691934278827, 'reg_lambda': 19.535434444939284, 'min_child_samples': 35, 'max_depth': 16, 'max_bin': 43}\n",
    "    c21={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1899, 'learning_rate': 0.07536122429341643, 'colsample_bytree': 0.8755415544495265, 'colsample_bynode': 0.3971515541568271, 'reg_alpha': 7.825234820629166, 'reg_lambda': 19.539558271427442, 'min_child_samples': 16, 'max_depth': 10, 'max_bin': 67}\n",
    "    c22={'device': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06258413085998576, 'colsample_bytree': 0.6527661140701613, 'colsample_bynode': 0.8106858631408332, 'lambda_l1': 5.065645378814257, 'lambda_l2': 9.81159370218779, 'min_data_in_leaf': 192, 'max_depth': 10, 'max_bin': 250}\n",
    "    c23={'device': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.0632167263149817, 'colsample_bytree': 0.6958033941948067, 'colsample_bynode': 0.6030801666196094, 'lambda_l1': 7.137580620471935, 'lambda_l2': 9.348169401713742, 'min_data_in_leaf': 74, 'max_depth': 11, 'max_bin': 220}\n",
    "    c24={'device': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.061236402165228264, 'colsample_bytree': 0.81427095118471, 'colsample_bynode': 0.6097376843527067, 'lambda_l1': 6.360490880385201, 'lambda_l2': 9.954136008333839, 'min_data_in_leaf': 238, 'max_depth': 13, 'max_bin': 180}\n",
    "    \n",
    "else:\n",
    "    pass\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB is_consumption=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_gpu:\n",
    "    p1={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1569, 'learning_rate': 0.0954350912359592, 'colsample_bytree': 0.5643173334694846, 'colsample_bynode': 0.8507963404594194, 'reg_alpha': 0.5815184353302838, 'reg_lambda': 19.577675580975, 'min_child_samples': 83, 'max_depth': 15, 'max_bin': 125}\n",
    "    \n",
    "    p2={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1655, 'learning_rate': 0.09749234861597421, 'colsample_bytree': 0.5878083872493073, 'colsample_bynode': 0.7476463591217907, 'reg_alpha': 0.6253329425224532, 'reg_lambda': 19.31104697178572, 'min_child_samples': 41, 'max_depth': 15, 'max_bin': 116}\n",
    "\n",
    "    p3={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1702, 'learning_rate': 0.08956547486313553, 'colsample_bytree': 0.553841254939378, 'colsample_bynode': 0.7707977076873439, 'reg_alpha': 3.4503195735482803, 'reg_lambda': 17.09137108374253, 'min_child_samples': 46, 'max_depth': 16, 'max_bin': 123}\n",
    "\n",
    "    p4={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1662, 'learning_rate': 0.09936096276241443, 'colsample_bytree': 0.5262782304338314, 'colsample_bynode': 0.6861424640373375, 'reg_alpha': 1.1434350179754031, 'reg_lambda': 18.238183112413253, 'min_child_samples': 42, 'max_depth': 16, 'max_bin': 118}\n",
    "\n",
    "    p5={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1596, 'learning_rate': 0.09145526299958297, 'colsample_bytree': 0.5813223295308532, 'colsample_bynode': 0.8019876742272201, 'reg_alpha': 1.7506271227424264, 'reg_lambda': 19.97101707600914, 'min_child_samples': 45, 'max_depth': 16, 'max_bin': 129}\n",
    "\n",
    "    p6={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1611, 'learning_rate': 0.09177637954991985, 'colsample_bytree': 0.5406014712979323, 'colsample_bynode': 0.7948709121454631, 'reg_alpha': 1.4162076884265264, 'reg_lambda': 19.18451476080634, 'min_child_samples': 39, 'max_depth': 15, 'max_bin': 128}\n",
    "\n",
    "    p7={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1722, 'learning_rate': 0.09721924587634734, 'colsample_bytree': 0.581822849292829, 'colsample_bynode': 0.8475502365142857, 'reg_alpha': 0.5558295705320715, 'reg_lambda': 17.676848729848402, 'min_child_samples': 76, 'max_depth': 15, 'max_bin': 121}\n",
    "\n",
    "    p8={'device': 'gpu', 'n_estimators': 1656, 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'num_leaves': 41, 'learning_rate': 0.044516384496327305, 'colsample_bytree': 0.8375548988212456, 'colsample_bynode': 0.5667409811919698, 'reg_alpha': 3.1759770693392193, 'reg_lambda': 11.018602402122836, 'min_child_samples': 5, 'max_depth': -1, 'max_bin': 58}\n",
    "\n",
    "    p9={'device': 'gpu', 'n_estimators': 1603, 'verbose': -1, 'random_state': 41, 'objective': 'l2', 'num_leaves': 41, 'learning_rate': 0.03818179150947824, 'colsample_bytree': 0.7488971539445326, 'colsample_bynode': 0.5610283223532974, 'reg_alpha': 3.4833384149996096, 'reg_lambda': 10.927987824861278, 'min_child_samples': 49, 'max_depth': -1, 'max_bin': 58}\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB is_consumption=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_gpu:\n",
    "    # Параметры для lgbm c GPU\n",
    "    c1={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1961, 'learning_rate': 0.055041729559669385, 'colsample_bytree': 0.8054200071555299, 'colsample_bynode': 0.8827333010526346, 'reg_alpha': 9.649253442752036, 'reg_lambda': 16.98908601233005, 'min_child_samples': 54, 'max_depth': 12, 'max_bin': 36}\n",
    "\n",
    "    c2={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1948, 'learning_rate': 0.06233594141892915, 'colsample_bytree': 0.8484245099171761, 'colsample_bynode': 0.899824429438312, 'reg_alpha': 10.7294451589117, 'reg_lambda': 17.69396992827211, 'min_child_samples': 45, 'max_depth': 13, 'max_bin': 32}\n",
    "\n",
    "    c3={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1995, 'learning_rate': 0.0633853242094111, 'colsample_bytree': 0.9331894149297775, 'colsample_bynode': 0.972632605889707, 'reg_alpha': 8.983517796023829, 'reg_lambda': 18.03334867391121, 'min_child_samples': 33, 'max_depth': 11, 'max_bin': 48}\n",
    "\n",
    "    c4={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1948, 'learning_rate': 0.04980982203453618, 'colsample_bytree': 0.7850607568025258, 'colsample_bynode': 0.9990467893228645, 'reg_alpha': 8.279232611894738, 'reg_lambda': 18.878856721521696, 'min_child_samples': 85, 'max_depth': 12, 'max_bin': 32}\n",
    "\n",
    "    c5={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1958, 'learning_rate': 0.06331649649993518, 'colsample_bytree': 0.965107198312526, 'colsample_bynode': 0.9562601410444004, 'reg_alpha': 8.595100697458118, 'reg_lambda': 19.672841466470988, 'min_child_samples': 38, 'max_depth': 14, 'max_bin': 37}\n",
    "\n",
    "    c6={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1667, 'learning_rate': 0.06761829944908236, 'colsample_bytree': 0.8491878204722972, 'colsample_bynode': 0.7943301198687824, 'reg_alpha': 12.053887346204482, 'reg_lambda': 17.21503593146002, 'min_child_samples': 48, 'max_depth': 12, 'max_bin': 60}\n",
    "\n",
    "    c7={'device': 'gpu', 'verbose': -1, 'objective': 'l2', 'n_estimators': 1965, 'learning_rate': 0.06236463049661708, 'colsample_bytree': 0.9209979630548757, 'colsample_bynode': 0.9718755549743984, 'reg_alpha': 9.485421137815203, 'reg_lambda': 19.728001606564117, 'min_child_samples': 24, 'max_depth': 14, 'max_bin': 35}\n",
    "\n",
    "    c8={'device': 'gpu', 'n_estimators': 1947, 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'num_leaves': 54, 'learning_rate': 0.05723355282907335, 'colsample_bytree': 0.9475962639323839, 'colsample_bynode': 0.5979396404779531, 'reg_alpha': 0.907591058077434, 'reg_lambda': 12.818913905376233, 'min_child_samples': 34, 'max_depth': -1, 'max_bin': 33}\n",
    "\n",
    "    c9={'device': 'gpu', 'n_estimators': 1920, 'verbose': -1, 'random_state': 41, 'objective': 'l2', 'num_leaves': 57, 'learning_rate': 0.05006410941730473, 'colsample_bytree': 0.9708958855792246, 'colsample_bynode': 0.5521647573988697, 'reg_alpha': 1.6279412387851688, 'reg_lambda': 12.723104735826002, 'min_child_samples': 99, 'max_depth': -1, 'max_bin': 34}\n",
    "\n",
    "    c10={'device': 'gpu', 'n_estimators': 1942, 'verbose': -1, 'random_state': 43, 'objective': 'l2', 'num_leaves': 50, 'learning_rate': 0.057507379262121315, 'colsample_bytree': 0.9402195759121595, 'colsample_bynode': 0.46574576530217693, 'reg_alpha': 1.5086048112799886, 'reg_lambda': 14.086405703124512, 'min_child_samples': 50, 'max_depth': -1, 'max_bin': 34}\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция выделения интервалов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяыет из данных указанные интервалы.\n",
    "# И в каждом интервале выбирает случайно лишь указанную долю данных\n",
    "# df_train - датафрейм для обработки\n",
    "# is_consumption какие данные возвращать данные в разрезе is_consumption\n",
    "# data_block_id_intervals какие данные возвращать данные в разрезе is_consumption\n",
    "#   это массив масивов. В каждой строке описание периода\n",
    "#   первая колонка на сколько data_block_id в конеце обучени отстоит от доступного конца данных\n",
    "#   вторая колонка сколько data_block_id будет в периоде на котором обучаемся.\n",
    "#   Третья колонка какую долю от данных оставдять (от 0 до 1)\n",
    "#   Периодов (строк) может быть произвольное количество\n",
    "# data_block_id_min минмимальный data_block_id который будет в возвращаемых данных\n",
    "def get_train_intervals(df_train, is_consumption, data_block_id_intervals, data_block_id_min):\n",
    "    max_block_id = df_train[\"data_block_id\"].max()\n",
    "    \n",
    "    df_train_int = df_train[(\n",
    "        #  выбираем только те данные которые больше data_block_id_min\n",
    "        (df_train['data_block_id']>=data_block_id_min)\n",
    "        #  выбираем только те данные для обучения по is_consumption на которых специализируетсмя модель\n",
    "        &(df_train['is_consumption']==is_consumption)\n",
    "        # Оставляем только notnull таргеты\n",
    "        &(df_train[\"target\"].notnull())\n",
    "    )]\n",
    "\n",
    "    ind = 0\n",
    "    for cur_interval in data_block_id_intervals:\n",
    "        # вырезаем очередной дата блок в указанных границах\n",
    "        cur_data_block = df_train_int[(\n",
    "            # До какого data_block_id учим первый блок\n",
    "            (df_train_int['data_block_id']<=max_block_id-cur_interval[0])\n",
    "            # C какого data_block_id учим первый блок\n",
    "            &(df_train_int['data_block_id']>(max_block_id-cur_interval[0]-cur_interval[1]))\n",
    "        )]\n",
    "        # print('1. cur_data_block:', cur_data_block['data_block_id'].nunique())\n",
    "        # print('1.1 cur_data_block:', cur_data_block.shape[0])\n",
    "        # Берем только часть случайную часть данных из блока\n",
    "        cur_data_block = cur_data_block.sample(frac = cur_interval[2])\n",
    "        # print('2. cur_data_block:', cur_data_block['data_block_id'].nunique())\n",
    "        # print('2.1 cur_data_block:', cur_data_block.shape[0])\n",
    "        if ind == 0:\n",
    "            final_train_df = cur_data_block.copy()\n",
    "        else:\n",
    "            final_train_df = pd.concat([final_train_df, cur_data_block], ignore_index=True)\n",
    "        ind += 1\n",
    "    return(final_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция обучения моделей с диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучает модели и данные, записаные на диск\n",
    "# и записывает обученные модели обратно на диск\n",
    "# models_names - список имен моделей\n",
    "# models_path - путь на диске где хроанятся модели и данные\n",
    "\n",
    "def fit_models_from_disk(models_names, models_path):\n",
    "    # Код процесса, который будет выполнятся параллельно\n",
    "    process_code1 = '''\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "while True:\n",
    "    model_name = input()\n",
    "    if model_name == 'exit':\n",
    "        print(f\"Finish\")\n",
    "        break\n",
    "    \n",
    "    # Загружаем модель\n",
    "    #model = load(os.path.join(models_path, f'{model_name}-init.joblib'))\n",
    "    # Загружаем трейн\n",
    "    #X = pd.read_pickle(os.path.join(models_path, f'{model_name}-x.pkl'))\n",
    "    # Загружаем таргет\n",
    "    #y = pd.read_pickle(os.path.join(models_path, f'{model_name}-y.pkl'))\n",
    "    \n",
    "    # Обучаем модель\n",
    "    #model.fit(X, y)\n",
    "    \n",
    "    #dump(model, os.path.join(models_path, f'{model_name}-trained.joblib'))\n",
    "    \n",
    "    print(f\"Complete: {command}\")\n",
    "'''\n",
    "    # Запуск дочернего процесса в котором будут тренироваться моджели\n",
    "    process1 = subprocess.Popen(['python', '-c', process_code],\n",
    "                                stdin=subprocess.PIPE, \n",
    "                                stdout=subprocess.PIPE, text=True)\n",
    "    for model_name in models_names:\n",
    "\n",
    "        print(p_time(), 'fit model:', model_name)\n",
    "        print(p_time(),'Start1')\n",
    "        # Отправка команда обучать модель дочернему процессу\n",
    "        process1.stdin.write(f'{model_name}\\n')\n",
    "        process1.stdin.flush()\n",
    "        print(p_time(),'Start2')\n",
    "        # Ждем сообщения о завершенеии тренировки модели от дочернеего процесса\n",
    "        response1 = process1.stdout.readline().strip()\n",
    "        print(p_time(),'Start3')\n",
    "        \n",
    "        print(p_time(),f\"Дочерний процесс: {response1.strip()}\")\n",
    "        \n",
    "        time.sleep(0)  # Пауза для иллюстрации, вы можете удалить это в реальном приложении\n",
    "        \n",
    "    # Отправка команды завершения\n",
    "    #response, _ = process.communicate(input='exit\\n')\n",
    "    process1.stdin.write('exit\\n')\n",
    "    process1.stdin.flush()\n",
    "    \n",
    "    # Ожидание завершения дочернего процесса\n",
    "    print(p_time(),'Start4')\n",
    "    process1.wait()\n",
    "    print(p_time(),'Start5')\n",
    "    \n",
    "    print(p_time(), \"Обучение моделей завершено\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучает модели и данные, записаные на диск\n",
    "# и записывает обученные модели обратно на диск\n",
    "# models_names - список имен моделей\n",
    "# models_path - путь на диске где хроанятся модели и данные\n",
    "\n",
    "def fit_models_from_disk1(models_names, models_path):\n",
    "    # Код процесса, который будет выполнятся параллельно\n",
    "    process_code = '''\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "while True:\n",
    "    command = input()\n",
    "    if command == 'exit':\n",
    "        print(f\"Дочерний процесс 2 завершил работу\")\n",
    "        break\n",
    "    # Выполнение задачи\n",
    "    time.sleep(1)\n",
    "    print(f\"Дочерний процесс 2 выполнил задачу: {command}\")\n",
    "'''\n",
    "    # Запуск дочернего процесса в котором будут тренироваться моджели\n",
    "    process1 = subprocess.Popen(['python', '-c', process_code],\n",
    "                                stdin=subprocess.PIPE, \n",
    "                                stdout=subprocess.PIPE, text=True)\n",
    "    for model_name in models_names:\n",
    "\n",
    "        print(p_time(), 'fit model:', model_name)\n",
    "        print(p_time(),'Start1')\n",
    "        # Отправка команда обучать модель дочернему процессу\n",
    "        process1.stdin.write(f'{model_name}\\n')\n",
    "        process1.stdin.flush()\n",
    "        print(p_time(),'Start2')\n",
    "        # Ждем сообщения о завершенеии тренировки модели от дочернеего процесса\n",
    "        response1 = process1.stdout.readline().strip()\n",
    "        print(p_time(),'Start3')\n",
    "        \n",
    "        print(p_time(),f\"Дочерний процесс: {response1.strip()}\")\n",
    "        \n",
    "    # Отправка команды завершения\n",
    "    #response, _ = process.communicate(input='exit\\n')\n",
    "    process1.stdin.write('exit\\n')\n",
    "    process1.stdin.flush()\n",
    "    \n",
    "    # Ожидание завершения дочернего процесса\n",
    "    print(p_time(),'Start4')\n",
    "    process1.wait()\n",
    "    print(p_time(),'Start5')\n",
    "    \n",
    "    print(p_time(), \"Обучение моделей завершено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Класс моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***\n",
    "\n",
    "# Класс обертка для моделей. Хранит различные пареметры для обучения моделей.\n",
    "# Например диапазоны данных на которых учить модель, как часто обучать заново и другие параметры\n",
    "class Models:\n",
    "\n",
    "    # Путь где хранятся модели, тренировочные и тестовые выборки для них \n",
    "    models_path = 'models'\n",
    "    \n",
    "    # Инициализирует параметры обучения\n",
    "    # init_model - готовый объект модели для обучения\n",
    "    # alphas_* - Массивы с коэффициентами, на которые домножают предсказания\n",
    "    # alphas_1 - массив для моделей с предсказанием is_consumption == 1\n",
    "    # alphas_0 - массив для моделей с предсказанием is_consumption == 0\n",
    "    def __init__(self, alphas_1, alphas_0):\n",
    "        # Инициализируем словари\n",
    "        # Ключами во всех словарях будет имя модели\n",
    "        \n",
    "        # Словарь с моделями\n",
    "        self.models =  dict()\n",
    "        \n",
    "        # Словарь с описанием периодов обучения модели\n",
    "        # пока это матрица. из двух столбцов и двух строк в каждой строке описание периода\n",
    "        # первая колонка на сколько data_block_id в конеце обучени отстоит от доступного конца данных\n",
    "        # вторая колонка сколько data_block_id будет в периоде на котором обучаемся.\n",
    "        # data_block_id могут быть эквивалентны дням, но могут и отличаться, если данные будут подавать блоками не равными дням\n",
    "        # либо если будут разрывы в данных. Но они точно будут эквиваленты циклам предсказания\n",
    "        self.data_block_id_intervals  = dict()\n",
    "\n",
    "        # Словарь с указанием по какой минимальный data_block_id отрезать данные.\n",
    "        # То есть меньше data_block_id_min не берем данные для обучения в любом случае, чтобы не было определено в data_block_id_intervals\n",
    "        self.data_block_id_min  = dict()\n",
    "        \n",
    "        # Если 1, то модель предназначена для предсказания потребления электричества\n",
    "        # Если 0, то модель предназначена для предсказания производства электричества\n",
    "        self.is_consumption = dict()\n",
    "        \n",
    "        # Раз во сколько иттераций обучат модель\n",
    "        self.learn_again_period = dict()\n",
    "        \n",
    "        # Смещение для начала обучения модели. Добавляется к номеру итерации сабмита.\n",
    "        # Скажем если смещение 6 номер итерации 1, а обучаемся раз в чем итераций. То обучение будет в первуже итерацию сабмита.\n",
    "        self.learn_again_offset = dict()\n",
    "        \n",
    "        # Время в секундах сколько заняло последнее обучение модели\n",
    "        self.last_learn_time = dict()\n",
    "\n",
    "        # Предсказания модели\n",
    "        self.predictions = dict()\n",
    "        \n",
    "        # Коэффициент на который умножать предсказания модели\n",
    "        self.predictions_alpha = dict()\n",
    "        \n",
    "        #Номер итерации. Обновляется при вызове метода fit\n",
    "        self.itter_n = 0\n",
    "        \n",
    "        # Массивы с коэффициентами, на которые домножают предсказания\n",
    "        # В нулевом индексе массива если устаревание 0 дней, в 1 если 1 день устарела и так далее\n",
    "        # self.alphas = [41/240, 39/240]\n",
    "        self.alphas_1 = alphas_1\n",
    "        self.alphas_0 = alphas_0\n",
    "        \n",
    "        # Устанавливаем в True если будем возвращать сохраненные ранее предикты\n",
    "        self.is_saved_predict = False\n",
    "\n",
    "        if not os.path.exists(self.models_path):\n",
    "            # Создание каталога, если его нет\n",
    "            os.makedirs(self.models_path)\n",
    "    \n",
    "    # Добавляет еще одну модель\n",
    "    # model_name - название модели\n",
    "    # new_model - объект модели\n",
    "    def add_model(self, model_name, new_model, is_consumption, data_block_id_intervals,\n",
    "                  data_block_id_min, learn_again_period, learn_again_offset):\n",
    "        \n",
    "        self.models[model_name] = new_model\n",
    "        self.is_consumption[model_name] = is_consumption\n",
    "        self.data_block_id_intervals[model_name] = data_block_id_intervals\n",
    "        self.data_block_id_min[model_name] = data_block_id_min\n",
    "        self.learn_again_period[model_name] = learn_again_period\n",
    "        self.learn_again_offset[model_name] = learn_again_offset\n",
    "        self.last_learn_time[model_name] = 0\n",
    "        self.predictions[model_name] = []\n",
    "        self.predictions_alpha[model_name] = []\n",
    "\n",
    "        # Сохранаяем начальную модель на диске\n",
    "        dump(new_model, os.path.join(self.models_path, f'{model_name}-init.joblib'))\n",
    "        \n",
    "    # Обучает модель\n",
    "    # model_name - название модели\n",
    "    # df_train - датафрейм который содержит данные для обучения и целевой признак\n",
    "    def fit_one_model(self, model_name, df_train):\n",
    "        #print(p_time(), 'fit model:', model_name)\n",
    "        '''\n",
    "        self.models[model_name].fit(\n",
    "            X=df_train.drop(columns=[\"target\", \"data_block_id\"]),\n",
    "            #X=df_train.drop(columns=[\"target\"]),\n",
    "            y=df_train[\"target\"]\n",
    "        )\n",
    "        '''\n",
    "        '''\n",
    "        print(p_time(), 'fit1')\n",
    "        df_train.drop(columns=[\"target\", \"data_block_id\"]).to_csv(\n",
    "            os.path.join(self.models_path, f'{model_name}-x.csv'), index=False\n",
    "        )\n",
    "        print(p_time(), 'fit2')\n",
    "        df_train[\"target\"].to_csv(\n",
    "            os.path.join(self.models_path, f'{model_name}-y.csv'), index=False\n",
    "        )\n",
    "        '''\n",
    "        #print(p_time(), 'fit3')\n",
    "\n",
    "        df_train.drop(columns=[\"target\", \"data_block_id\"]).to_pickle(\n",
    "            os.path.join(self.models_path, f'{model_name}-x.pkl')\n",
    "        )\n",
    "        #print(p_time(), 'fit4')\n",
    "        df_train[\"target\"].to_pickle(\n",
    "            os.path.join(self.models_path, f'{model_name}-y.pkl')\n",
    "        )\n",
    "        #print(p_time(), 'fit5')\n",
    "    \n",
    "    # Делает предсказания от отдельной модели.\n",
    "    # model_name - название модели\n",
    "    # X - признаки на которых нужно сделать предсказание\n",
    "    def predict_one_model(self, model_name, X):\n",
    "        if self.is_saved_predict:\n",
    "            # Загружаем ранее сохраненный предикт\n",
    "            print(p_time(), 'load predict model:', model_name)\n",
    "            y = predictions[model_name][self.itter_n]\n",
    "        else:\n",
    "            # Делаем новый предикт\n",
    "            print(p_time(), 'predict model:', model_name)\n",
    "            model = load(os.path.join(self.models_path, f'{model_name}-trained.joblib'))\n",
    "            y = (model\n",
    "                 .predict(X.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "                 #.predict(X).clip(0)\n",
    "                )\n",
    "            self.predictions[model_name].append(y)\n",
    "\n",
    "        if (((self.itter_n + self.learn_again_offset[model_name])\n",
    "             % self.learn_again_period[model_name]) == 0):\n",
    "            # Если модель только что училась ставим коэффициент побольшще\n",
    "            self.predictions_alpha[model_name] = (self.alphas_0[0]\n",
    "                                                  if self.is_consumption[model_name] == 0\n",
    "                                                  else self.alphas_1[0])\n",
    "        else:\n",
    "            # Если модель только что училась в прошлой итерации\n",
    "            # ставим коэффициент побольшще\n",
    "            self.predictions_alpha[model_name] = (self.alphas_0[1]\n",
    "                                                  if self.is_consumption[model_name] == 0\n",
    "                                                  else self.alphas_1[1])\n",
    "            \n",
    "        return(y)\n",
    "    \n",
    "    # Обучает все добавленные модели для которых наступило время их обучения\n",
    "    # df_train - датафрейм который содержит данные для обучения и целевой признак\n",
    "    # itter_n - номер иттерации в сабмите.\n",
    "    # если itter_n равен <=0. Значит первоначальное обучение и обучаем всем модели\n",
    "    def fit(self, df_train, itter_n):\n",
    "        self.itter_n = itter_n\n",
    "        if self.is_saved_predict:\n",
    "            # Не обучаем модели, если в режиме сохраненных предсказаний\n",
    "            return()\n",
    "            \n",
    "        max_block_id = df_train[\"data_block_id\"].max()\n",
    "        # Перебираем все модели\n",
    "        for m_name in self.learn_again_period:\n",
    "            # Либо сказали все модели учить\n",
    "            if ((itter_n <= 0)\n",
    "                # Либо учим если номер итераиции в сабмите плюс смещение делится на целое на период обучения\n",
    "                or (((itter_n + self.learn_again_offset[m_name])\n",
    "                     % self.learn_again_period[m_name]) == 0)):\n",
    "                \n",
    "                '''\n",
    "                d_i = self.data_block_id_intervals[m_name]\n",
    "                # Выделяем данные для обучения\n",
    "                df_train_int = df_train[\n",
    "                    # До какого data_block_id учим первый блок\n",
    "                    (((df_train['data_block_id']<=max_block_id-d_i[0][0])\n",
    "                     # C какого data_block_id учим первый блок\n",
    "                     &(df_train['data_block_id']>(max_block_id-d_i[0][0]-d_i[0][1])))\n",
    "                    # До какого data_block_id учим второй блок\n",
    "                    |((df_train['data_block_id']<=max_block_id-d_i[1][0])\n",
    "                      # С какого data_block_id учим второй блок\n",
    "                      &(df_train['data_block_id']>(max_block_id-d_i[1][0]-d_i[1][1]))))\n",
    "                #  выбираем только те данные которые больше data_block_id_min\n",
    "                &(df_train['data_block_id']>=self.data_block_id_min[m_name])\n",
    "                #  выбираем только те данные для обучения по is_consumption на которых специализируетсмя модель\n",
    "                &(df_train['is_consumption']==self.is_consumption[m_name])\n",
    "                # Оставляем только notnull таргеты\n",
    "                &(df_train[\"target\"].notnull())\n",
    "                ]\n",
    "                '''\n",
    "                df_train_int = df_train[(\n",
    "                    #  выбираем только те данные которые больше data_block_id_min\n",
    "                    (df_train['data_block_id']>=self.data_block_id_min[m_name])\n",
    "                    #  выбираем только те данные для обучения по is_consumption на которых специализируетсмя модель\n",
    "                    &(df_train['is_consumption']==self.is_consumption[m_name])\n",
    "                    # Оставляем только notnull таргеты\n",
    "                    &(df_train[\"target\"].notnull())\n",
    "                )]\n",
    "\n",
    "                ind = 0\n",
    "                for cur_interval in self.data_block_id_intervals[m_name]:\n",
    "                    # вырезаем очередной дата блок в указанных границах\n",
    "                    cur_data_block = df_train_int[(\n",
    "                        # До какого data_block_id учим первый блок\n",
    "                        (df_train_int['data_block_id']<=max_block_id-cur_interval[0])\n",
    "                        # C какого data_block_id учим первый блок\n",
    "                        &(df_train_int['data_block_id']>(max_block_id-cur_interval[0]-cur_interval[1]))\n",
    "                    )]\n",
    "                    # print('1. cur_data_block:', cur_data_block['data_block_id'].nunique())\n",
    "                    # print('1.1 cur_data_block:', cur_data_block.shape[0])\n",
    "                    # Берем только часть случайную часть данных из блока\n",
    "                    cur_data_block = cur_data_block.sample(frac = cur_interval[2])\n",
    "                    # print('2. cur_data_block:', cur_data_block['data_block_id'].nunique())\n",
    "                    # print('2.1 cur_data_block:', cur_data_block.shape[0])\n",
    "                    if ind == 0:\n",
    "                        final_train_df = cur_data_block.copy()\n",
    "                    else:\n",
    "                        final_train_df = pd.concat([final_train_df, cur_data_block], ignore_index=True)\n",
    "                    ind += 1\n",
    "\n",
    "                '''\n",
    "                print('df_train_int:', df_train_int['data_block_id'].nunique())\n",
    "                print('df_train_int:', df_train_int.shape[0])\n",
    "                print('df_train:', df_train['data_block_id'].nunique())\n",
    "                print('df_train:', df_train.shape[0])\n",
    "                print('final_train_df:', final_train_df['data_block_id'].nunique())\n",
    "                print('final_train_df:', final_train_df.shape[0])\n",
    "                '''\n",
    "                \n",
    "                # Обучаем модель\n",
    "                self.fit_one_model(m_name, final_train_df)\n",
    "        # Обучаем модели и данные, записаные на диск\n",
    "        fit_models_from_disk(\n",
    "            models_names = list(self.models.keys()),\n",
    "            models_path = self.models_path\n",
    "        )\n",
    "    \n",
    "    # Делает предсказание всеми добавленными моделями и сводит их в одно предсказание\n",
    "    def predict(self, X):\n",
    "        # Создаем датафрейм для предсказаний\n",
    "        # Первый столбец содержит информацию 'is_consumption'\n",
    "        # Для каждой модели отдельный столбец с предсказаниями\n",
    "        # В predict_df_0 будут предсказания для моделей с is_consumption == 0\n",
    "        self.predict_df_0 = pd.DataFrame(X['is_consumption'])\n",
    "        # В predict_df_1 будут предсказания для моделей с is_consumption == 1\n",
    "        self.predict_df_1 = pd.DataFrame(X['is_consumption'])\n",
    "        \n",
    "        # Перебираем все модели\n",
    "        for m_name in self.learn_again_period:\n",
    "            # Делаем предсказание в соответвующий датафрейм в столбец модели\n",
    "            if self.is_consumption[m_name] == 0:\n",
    "                self.predict_df_0[m_name] = self.predict_one_model(m_name, X).clip(0)\n",
    "                # Домножаем предсказание на коэффициент\n",
    "                self.predict_df_0[m_name] = self.predict_df_0[m_name] * self.predictions_alpha[m_name]\n",
    "            else:\n",
    "                self.predict_df_1[m_name] = self.predict_one_model(m_name, X).clip(0)\n",
    "                self.predict_df_1[m_name] = self.predict_df_1[m_name] * self.predictions_alpha[m_name]\n",
    "\n",
    "        # Суммируем предсказания моделей раздельно по датафреймам\n",
    "        self.predict_df_0['target'] = self.predict_df_0.iloc[:, 1:].sum(axis=1) #mean(axis=1)\n",
    "        self.predict_df_1['target'] = self.predict_df_1.iloc[:, 1:].sum(axis=1) #mean(axis=1)\n",
    "        \n",
    "        #self.predict_df_0['target'] = self.predict_df_0.iloc[:, 1:].mean(axis=1)\n",
    "        #self.predict_df_1['target'] = self.predict_df_1.iloc[:, 1:].mean(axis=1)\n",
    "        \n",
    "        # Сведение в одно предсказание потребления и производства электричества у просьюмеров\n",
    "        predict_df = self.predict_df_1[['is_consumption', 'target']]\n",
    "\n",
    "        predict_df.loc[predict_df['is_consumption']==0, 'target'] = self.predict_df_0.loc[self.predict_df_0['is_consumption']==0, 'target']\n",
    "        \n",
    "        return predict_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вариант для шести моделей с одинаковыми коэффцициентами на\n",
    "# предсказания для моделей обученный в этой итерации и в прошлой\n",
    "#models = Models(alphas_0=[1/5, 1/5], alphas_1=[1/5, 1/5])\n",
    "#models = Models(alphas_1=[1/6, 1/6], alphas_0=[1/5, 1/5])\n",
    "models = Models(alphas_0=[1,1], alphas_1=[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Вариант 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание параметров моделей\n",
    "'''\n",
    "models_list = [\n",
    "    {'model_name': 'model-1', 'new_model': lgb.LGBMRegressor(**p1), 'learn_again_period': 8, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-2', 'new_model': lgb.LGBMRegressor(**p2), 'learn_again_period': 8, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-3', 'new_model': lgb.LGBMRegressor(**p3), 'learn_again_period': 8, 'learn_again_offset': 2, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-4', 'new_model': lgb.LGBMRegressor(**p4), 'learn_again_period': 8, 'learn_again_offset': 3, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-5', 'new_model': lgb.LGBMRegressor(**p5), 'learn_again_period': 8, 'learn_again_offset': 4, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-6', 'new_model': lgb.LGBMRegressor(**p6), 'learn_again_period': 8, 'learn_again_offset': 5, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-7', 'new_model': lgb.LGBMRegressor(**p7), 'learn_again_period': 8, 'learn_again_offset': 6, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-8', 'new_model': lgb.LGBMRegressor(**p8), 'learn_again_period': 8, 'learn_again_offset': 7, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-9', 'new_model': lgb.LGBMRegressor(**p9), 'learn_again_period': 8, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-10', 'new_model': lgb.LGBMRegressor(**p10), 'learn_again_period': 8, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-11', 'new_model': lgb.LGBMRegressor(**p11), 'learn_again_period': 8, 'learn_again_offset': 2, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-12', 'new_model': lgb.LGBMRegressor(**p12), 'learn_again_period': 8, 'learn_again_offset': 3, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-13', 'new_model': lgb.LGBMRegressor(**p13), 'learn_again_period': 8, 'learn_again_offset': 4, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-14', 'new_model': lgb.LGBMRegressor(**p14), 'learn_again_period': 8, 'learn_again_offset': 5, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-15', 'new_model': lgb.LGBMRegressor(**p15), 'learn_again_period': 8, 'learn_again_offset': 6, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-16', 'new_model': lgb.LGBMRegressor(**p16), 'learn_again_period': 8, 'learn_again_offset': 7, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-17', 'new_model': lgb.LGBMRegressor(**p17), 'learn_again_period': 8, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-18', 'new_model': lgb.LGBMRegressor(**p18), 'learn_again_period': 8, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-19', 'new_model': lgb.LGBMRegressor(**p19), 'learn_again_period': 8, 'learn_again_offset': 2, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-20', 'new_model': lgb.LGBMRegressor(**p20), 'learn_again_period': 8, 'learn_again_offset': 3, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-21', 'new_model': lgb.LGBMRegressor(**p21), 'learn_again_period': 8, 'learn_again_offset': 4, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-22', 'new_model': lgb.LGBMRegressor(**p22), 'learn_again_period': 8, 'learn_again_offset': 5, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-23', 'new_model': lgb.LGBMRegressor(**p23), 'learn_again_period': 8, 'learn_again_offset': 6, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-24', 'new_model': lgb.LGBMRegressor(**p24), 'learn_again_period': 8, 'learn_again_offset': 7, 'is_consumption': 1, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-1', 'new_model': lgb.LGBMRegressor(**c1), 'learn_again_period': 8, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-2', 'new_model': lgb.LGBMRegressor(**c2), 'learn_again_period': 8, 'learn_again_offset': 1, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-3', 'new_model': lgb.LGBMRegressor(**c3), 'learn_again_period': 8, 'learn_again_offset': 2, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-4', 'new_model': lgb.LGBMRegressor(**c4), 'learn_again_period': 8, 'learn_again_offset': 3, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-5', 'new_model': lgb.LGBMRegressor(**c5), 'learn_again_period': 8, 'learn_again_offset': 4, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-6', 'new_model': lgb.LGBMRegressor(**c6), 'learn_again_period': 8, 'learn_again_offset': 5, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-7', 'new_model': lgb.LGBMRegressor(**c7), 'learn_again_period': 8, 'learn_again_offset': 6, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-8', 'new_model': lgb.LGBMRegressor(**c8), 'learn_again_period': 8, 'learn_again_offset': 7, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-9', 'new_model': lgb.LGBMRegressor(**c9), 'learn_again_period': 8, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-10', 'new_model': lgb.LGBMRegressor(**c10), 'learn_again_period': 8, 'learn_again_offset': 1, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-11', 'new_model': lgb.LGBMRegressor(**c11), 'learn_again_period': 8, 'learn_again_offset': 2, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-12', 'new_model': lgb.LGBMRegressor(**c12), 'learn_again_period': 8, 'learn_again_offset': 3, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-13', 'new_model': lgb.LGBMRegressor(**c13), 'learn_again_period': 8, 'learn_again_offset': 4, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-14', 'new_model': lgb.LGBMRegressor(**c14), 'learn_again_period': 8, 'learn_again_offset': 5, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-15', 'new_model': lgb.LGBMRegressor(**c15), 'learn_again_period': 8, 'learn_again_offset': 6, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-16', 'new_model': lgb.LGBMRegressor(**c16), 'learn_again_period': 8, 'learn_again_offset': 7, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-17', 'new_model': lgb.LGBMRegressor(**c17), 'learn_again_period': 8, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-18', 'new_model': lgb.LGBMRegressor(**c18), 'learn_again_period': 8, 'learn_again_offset': 1, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-19', 'new_model': lgb.LGBMRegressor(**c19), 'learn_again_period': 8, 'learn_again_offset': 2, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-20', 'new_model': lgb.LGBMRegressor(**c20), 'learn_again_period': 8, 'learn_again_offset': 3, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-21', 'new_model': lgb.LGBMRegressor(**c21), 'learn_again_period': 8, 'learn_again_offset': 4, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-22', 'new_model': lgb.LGBMRegressor(**c22), 'learn_again_period': 8, 'learn_again_offset': 5, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-23', 'new_model': lgb.LGBMRegressor(**c23), 'learn_again_period': 8, 'learn_again_offset': 6, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "    {'model_name': 'model-solar-24', 'new_model': lgb.LGBMRegressor(**c24), 'learn_again_period': 8, 'learn_again_offset': 7, 'is_consumption': 0, 'data_block_id_intervals': [[0,1000],[0,0]], 'data_block_id_min': 122},\n",
    "]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вариант 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models_list = [\n",
    "    {'model_name': 'model-1', 'new_model': lgb.LGBMRegressor(**p1), 'learn_again_period': 3, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-2', 'new_model': lgb.LGBMRegressor(**p2), 'learn_again_period': 3, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-3', 'new_model': lgb.LGBMRegressor(**p3), 'learn_again_period': 3, 'learn_again_offset': 2, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-4', 'new_model': lgb.LGBMRegressor(**p4), 'learn_again_period': 3, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-5', 'new_model': lgb.LGBMRegressor(**p5), 'learn_again_period': 3, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-6', 'new_model': lgb.LGBMRegressor(**p6), 'learn_again_period': 3, 'learn_again_offset': 2, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-7', 'new_model': lgb.LGBMRegressor(**p7), 'learn_again_period': 3, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-1', 'new_model': lgb.LGBMRegressor(**c1), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-2', 'new_model': lgb.LGBMRegressor(**c2), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-3', 'new_model': lgb.LGBMRegressor(**c3), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-4', 'new_model': lgb.LGBMRegressor(**c4), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-5', 'new_model': lgb.LGBMRegressor(**c5), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-6', 'new_model': lgb.LGBMRegressor(**c6), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-7', 'new_model': lgb.LGBMRegressor(**c7), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-8', 'new_model': lgb.LGBMRegressor(**c8), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "]\n",
    "'''\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вариант 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models_list = [\n",
    "    {'model_name': 'model-1', 'new_model': lgb.LGBMRegressor(**p1), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-2', 'new_model': lgb.LGBMRegressor(**p2), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-3', 'new_model': lgb.LGBMRegressor(**p3), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-4', 'new_model': lgb.LGBMRegressor(**p4), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-5', 'new_model': lgb.LGBMRegressor(**p5), 'learn_again_period': 2, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-6', 'new_model': lgb.LGBMRegressor(**p6), 'learn_again_period': 2, 'learn_again_offset': 1, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-1', 'new_model': lgb.LGBMRegressor(**c1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-2', 'new_model': lgb.LGBMRegressor(**c2), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-3', 'new_model': lgb.LGBMRegressor(**c3), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-4', 'new_model': lgb.LGBMRegressor(**c4), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-5', 'new_model': lgb.LGBMRegressor(**c5), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вариант 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models_list = [\n",
    "    {'model_name': 'model-1', 'new_model': lgb.LGBMRegressor(**p1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-2', 'new_model': lgb.LGBMRegressor(**p2), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-3', 'new_model': lgb.LGBMRegressor(**p3), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-4', 'new_model': lgb.LGBMRegressor(**p4), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-5', 'new_model': lgb.LGBMRegressor(**p5), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-6', 'new_model': lgb.LGBMRegressor(**p6), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-1', 'new_model': lgb.LGBMRegressor(**c1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-2', 'new_model': lgb.LGBMRegressor(**c2), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-3', 'new_model': lgb.LGBMRegressor(**c3), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-4', 'new_model': lgb.LGBMRegressor(**c4), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-5', 'new_model': lgb.LGBMRegressor(**c5), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-6', 'new_model': lgb.LGBMRegressor(**c6), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,90,1],[100,110,0.4],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вариант 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [\n",
    "    {'model_name': 'model-1', 'new_model': lgb.LGBMRegressor(**p1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 1, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "    {'model_name': 'model-solar-1', 'new_model': lgb.LGBMRegressor(**c1), 'learn_again_period': 1, 'learn_again_offset': 0, 'is_consumption': 0, 'data_block_id_intervals': [[0,100,1],[100,100,0.5],[200,180,0.3],[380,1000,0.2]], 'data_block_id_min': 0},\n",
    "]\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем модели в объект класса моделей\n",
    "for model_param in models_list:\n",
    "    models.add_model(**model_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if is_local:\n",
    "#    dump(model_solar, 'model_solar.joblib')\n",
    "#    dump(model, 'model_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расскоментировать при необходимости загрузку ранее сохраненных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для загрузки локально\n",
    "#model_solar = load('model_solar.joblib')\n",
    "#model = load('model_lgbm.joblib')\n",
    "\n",
    "# Для загрузки на kaggle\n",
    "#model_solar = load('/kaggle/input/enefit/model_solar.joblib')\n",
    "#model = load('/kaggle/input/enefit/model_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Если выполняем локально, а не сабмитим на кагл,\n",
    "    # то выбираем другое имя для файла submission.csv.\n",
    "    # Потому что в submission.csv записать прав нет и вылетает по ошибке\n",
    "    submission_name = 'submission_loc.csv'\n",
    "else:\n",
    "    submission_name = 'submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержимое public_timeseries_testing_util.py\n",
    "\n",
    "С необходимыми праками. Решил не импортировать его. а прямо тут. Так удобнее переносить на kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An unlocked version of the timeseries API intended for testing alternate inputs.\n",
    "Mirrors the production timeseries API in the crucial respects, but won't be as fast.\n",
    "\n",
    "ONLY works afer the first three variables in MockAPI.__init__ are populated.\n",
    "'''\n",
    "\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "\n",
    "class MockApi:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n",
    "        They've been intentionally left in an invalid state.\n",
    "\n",
    "        Variables to set:\n",
    "            input_paths: a list of two or more paths to the csv files to be served\n",
    "            group_id_column: the column that identifies which groups of rows the API should serve.\n",
    "                A call to iter_test serves all rows of all dataframes with the current group ID value.\n",
    "            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n",
    "        '''\n",
    "        self.input_paths: Sequence[str] = ['example_test_files/test.csv',\n",
    "                                   'example_test_files/revealed_targets.csv', \n",
    "                                   'example_test_files/client.csv',\n",
    "                                   'example_test_files/historical_weather.csv',\n",
    "                                   'example_test_files/forecast_weather.csv',\n",
    "                                   'example_test_files/electricity_prices.csv',\n",
    "                                   'example_test_files/gas_prices.csv',\n",
    "                                   'example_test_files/sample_submission.csv']\n",
    "        self.group_id_column: str = 'data_block_id'\n",
    "        self.export_group_id_column: bool = False\n",
    "        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n",
    "        assert len(self.input_paths) >= 2\n",
    "\n",
    "        self._status = 'initialized'\n",
    "        self.predictions = []\n",
    "\n",
    "    def iter_test(self) -> Tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Loads all of the dataframes specified in self.input_paths,\n",
    "        then yields all rows in those dataframes that equal the current self.group_id_column value.\n",
    "        '''\n",
    "        if self._status != 'initialized':\n",
    "\n",
    "            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n",
    "\n",
    "        dataframes = []\n",
    "        for pth in self.input_paths:\n",
    "            dataframes.append(pd.read_csv(pth, low_memory=False))\n",
    "        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n",
    "        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n",
    "\n",
    "        for group_id in group_order:\n",
    "            self._status = 'prediction_needed'\n",
    "            current_data = []\n",
    "            for df in dataframes:\n",
    "                cur_df = df.loc[group_id].copy()\n",
    "                # returning single line dataframes from df.loc requires special handling\n",
    "                if not isinstance(cur_df, pd.DataFrame):\n",
    "                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n",
    "                    cur_df.index.name = self.group_id_column\n",
    "                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n",
    "                current_data.append(cur_df)\n",
    "            yield tuple(current_data)\n",
    "\n",
    "            while self._status != 'prediction_received':\n",
    "                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n",
    "                yield None\n",
    "\n",
    "        with open(submission_name, 'w') as f_open:\n",
    "            pd.concat(self.predictions).to_csv(f_open, index=False)\n",
    "        self._status = 'finished'\n",
    "\n",
    "    def predict(self, user_predictions: pd.DataFrame):\n",
    "        '''\n",
    "        Accepts and stores the user's predictions and unlocks iter_test once that is done\n",
    "        '''\n",
    "        if self._status == 'finished':\n",
    "            raise Exception('You have already made predictions for the full test set.')\n",
    "        if self._status != 'prediction_needed':\n",
    "            raise Exception('You must get the next test sample from `iter_test()` first.')\n",
    "        if not isinstance(user_predictions, pd.DataFrame):\n",
    "            raise Exception('You must provide a DataFrame.')\n",
    "\n",
    "        self.predictions.append(user_predictions)\n",
    "        self._status = 'prediction_received'\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return MockApi()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для скора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисляет скор для предсказаний, который были поданы на вход\n",
    "# compare - датафрейм с уже заполенными реальными значениями таргета\n",
    "# и сделанными предсказаниями\n",
    "\n",
    "def compute_compare(compare, index_name):\n",
    "    mae = mean_absolute_error(compare['target'] , compare['predict'])\n",
    "\n",
    "    if (compare['data_block_id'].max() >= 518):\n",
    "        compare_temp = compare[(compare['data_block_id'] >= 518)&(compare['data_block_id'] <= 606)]\n",
    "        mae_518_606 = mean_absolute_error(compare_temp['target'], compare_temp['predict'])\n",
    "    else:\n",
    "        # Если еще не дошли до data_block_id >= 518 не считаем эти величины\n",
    "        mae_518_606 = '-'\n",
    "    \n",
    "    if (compare['data_block_id'].max() > 600):\n",
    "        # Считаем MAE для data_block_id > 600\n",
    "        compare_temp = compare[compare['data_block_id'] > 600]\n",
    "        mae_600 = mean_absolute_error(compare_temp['target'], compare_temp['predict'])\n",
    "    else:\n",
    "        # Если еще не дошли до data_block_id > 600 не считаем эти величины\n",
    "        mae_600 = '-'\n",
    "\n",
    "    mae_df = pd.DataFrame({\n",
    "        '(ALL)': mae,\n",
    "        'Feb - Apr (518 - 606)': mae_518_606,\n",
    "        '(> 600)': mae_600\n",
    "    }, index=[index_name])\n",
    "\n",
    "    # Округляем числа до двух знаков после запятой и преобразуем их в строки\n",
    "    mae_df = mae_df.round(3).astype(str)\n",
    "    \n",
    "    return mae_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчитывает скор.\n",
    "# Возвращает датафрейм compare со сравнением предсказаний\n",
    "def calc_score():\n",
    "    # Загружаем предсказания\n",
    "    #submission = pd.read_csv(submission_name)\n",
    "    submission = pd.concat(env.predictions)\n",
    "    \n",
    "    # Загружаем истинные значения\n",
    "    revealed_targets = pd.read_csv(os.path.join(test_path, \"revealed_targets.csv\"))\n",
    "    revealed_targets['data_block_id'] -= 2\n",
    "    revealed_targets = revealed_targets[revealed_targets[\"data_block_id\"] > train_end_data_block_id]\n",
    "    # Обрезаем реальные предсказания revealed_targets по длине уже сделанных предсказаний submission\n",
    "    revealed_targets = revealed_targets.iloc[:len(submission)]\n",
    "\n",
    "    # print(f'MAE: {mae}')\n",
    "    \n",
    "    # Подготовим данные для анализа изменения ошибки предсказания по мере удаления от времени завершения обучения\n",
    "    compare = revealed_targets[['data_block_id', 'is_consumption', 'target']].copy()\n",
    "    compare['predict'] = submission['target'].values\n",
    "    compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n",
    "    compare['err'] = (compare['predict'] - compare['target']).values\n",
    "\n",
    "    mae_df = compute_compare(compare, 'All Models MAE')\n",
    "\n",
    "    new_mae_df = compute_compare(compare[compare['is_consumption']==0], 'is_consumption == 0')\n",
    "    mae_df = pd.concat([mae_df, new_mae_df])\n",
    "\n",
    "    new_mae_df = compute_compare(compare[compare['is_consumption']==1], 'is_consumption == 1')\n",
    "    mae_df = pd.concat([mae_df, new_mae_df])\n",
    "\n",
    "    # Перебираем предсказания всех моделей и складываем\n",
    "    # скоры их предсказания в один датафрейм mae_df\n",
    "    for model_name in models.predictions:\n",
    "        # заполняем compareпредсказаниями модели\n",
    "        compare['predict'] = np.concatenate(models.predictions[model_name])\n",
    "        compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n",
    "        compare['err'] = (compare['predict'] - compare['target']).values\n",
    "        # Оставляем в предсказаниях только те строки, которые предсказывала модель\n",
    "        new_compare = compare[compare['is_consumption']==models.is_consumption[model_name]] \n",
    "        \n",
    "        new_mae_df = compute_compare(new_compare,\n",
    "                                     model_name+f' ({models.is_consumption[model_name]})')\n",
    "                                                 \n",
    "        mae_df = pd.concat([mae_df, new_mae_df])\n",
    "    \n",
    "    display(mae_df)\n",
    "    print('MAE > 600', mae_df.loc['All Models MAE','(> 600)'])\n",
    "\n",
    "    compare['predict'] = submission['target'].values\n",
    "    compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n",
    "    compare['err'] = (compare['predict'] - compare['target']).values\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация иттераций сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # После этого можно имитировать локально загрузку при собмите на большом числе итераций\n",
    "    # А не только четыре иттерации на 4 дня как в стандартной имитайии на кагле\n",
    "    env = make_env()\n",
    "else:\n",
    "    # загружаем оригинальную библиотеку для сабмита\n",
    "    import enefit\n",
    "    env = enefit.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим последний data_block_id в обучающих данных\n",
    "max_train_data_block_id = df_data[\"data_block_id\"].max()\n",
    "# Устанавливаем первый data_block_id для теста следущим за тренировочным\n",
    "cur_test_data_block_id = max_train_data_block_id + 1\n",
    "cur_test_data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 0\n",
    "\n",
    "# Основной цикл для обработки данных тестового набора\n",
    "for (test, revealed_targets, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
    "    iteration_start_time = time.time()\n",
    "    print(p_time(), '*************** Iteration: ', count, '***************')\n",
    "    \n",
    "    # Переименование столбца для удобства\n",
    "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "    if is_local:\n",
    "        # Если выполняем локально, то преобразуем некоторые типы данных\n",
    "        # На кагле (а может и в линуксе) они и так преобразуются, но на виновс локально\n",
    "        # не преобразуются и выдетают по ощибке\n",
    "        test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "        client['date'] = pd.to_datetime(client['date'])\n",
    "        gas_prices['origin_date'] = pd.to_datetime(gas_prices['origin_date'])\n",
    "        gas_prices['forecast_date'] = pd.to_datetime(gas_prices['forecast_date'])\n",
    "        electricity_prices['origin_date'] = pd.to_datetime(electricity_prices['origin_date'])\n",
    "        electricity_prices['forecast_date'] = pd.to_datetime(electricity_prices['forecast_date'])\n",
    "        forecast_weather['origin_datetime'] = pd.to_datetime(forecast_weather['origin_datetime'])\n",
    "        forecast_weather['forecast_datetime'] = pd.to_datetime(forecast_weather['forecast_datetime'])\n",
    "        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n",
    "        revealed_targets['datetime'] = pd.to_datetime(revealed_targets['datetime'])\n",
    "        \n",
    "    # Добавляем колонку заполненную следующим data_block_id\n",
    "    test[\"data_block_id\"] = cur_test_data_block_id\n",
    "    revealed_targets[\"data_block_id\"] = cur_test_data_block_id\n",
    "    \n",
    "    df_test            = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
    "    df_new_client      = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
    "    df_new_gas         = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
    "    df_new_electricity = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
    "    df_new_forecast    = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
    "    df_new_historical  = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
    "    df_new_target      = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
    "    df_new_data        = pl.from_pandas(revealed_targets[df_data_cols], schema_overrides=schema_data)\n",
    "    # Объединение новых данных с существующими и удаление дубликатов\n",
    "    df_client          = pl.concat([df_client, df_new_client]).unique(subset=[\"county\", \"is_business\", \"product_type\", \"date\"], maintain_order=True)\n",
    "    df_gas             = pl.concat([df_gas, df_new_gas]).unique(subset=[\"forecast_date\"], maintain_order=True)\n",
    "    df_electricity     = pl.concat([df_electricity, df_new_electricity]).unique(subset=[\"forecast_date\"], maintain_order=True)\n",
    "    df_forecast        = pl.concat([df_forecast, df_new_forecast]).unique()\n",
    "    df_historical      = pl.concat([df_historical, df_new_historical]).unique()\n",
    "    df_target          = pl.concat([df_target, df_new_target]).unique()\n",
    "    df_data            = pl.concat([df_data, df_new_data]).unique()\n",
    "    \n",
    "    \n",
    "    #if (not(is_local) or (count == 0) or (count>=98)):\n",
    "    if (not(is_local) or (count == 0) or (count>=17)):\n",
    "    #if (not(is_local) or (count>=0)):\n",
    "        # Если испольняем локально только после\n",
    "        # итерации 100. Потому что интересует как ведет себя моделья через два месяца\n",
    "        # после обучения. Так же обучаем все модели, если в самой первой итерции, если count == 0\n",
    "        cur_time = time.time()\n",
    "        if (((cur_time - notebook_starttime) < (8*60*60 + 60*50)) or is_disable_run_time_limit):\n",
    "            # Не начинаем тренировать модели заново на реальном сабмите пока в датах предсказания не появятся даты идущие в скор\n",
    "            if (df_test['datetime'].max() >= scor_start_time or (count == 0) or is_local):\n",
    "                X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "                X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "                #Добавились новые строки в данные. Учим.\n",
    "                df_train = to_pandas(X, y)\n",
    "                models.fit(df_train=df_train, itter_n=count)\n",
    "            else:\n",
    "                print('Не тренеруем модель', df_test['datetime'].max(), 'не достигла даты начала тренировки:', scor_start_time)\n",
    "                \n",
    "        else:\n",
    "            print('Не тренеруем модель, превышено время выполнения ноутбука:', (cur_time - notebook_starttime))\n",
    "\n",
    "    # Применение функции инженерии признаков и преобразование данных обратно в pandas\n",
    "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "    X_test = to_pandas(X_test)\n",
    "    \n",
    "    # Прогнозирование с использованием модели и ограничение предсказаний нулем\n",
    "    test['target'] = models.predict(X_test)\n",
    "    \n",
    "    # Обновление целевых значений в примере предсказания\n",
    "    sample_prediction[\"target\"] = test['target']\n",
    "    \n",
    "    # Отправка предсказаний в среду выполнения\n",
    "    env.predict(sample_prediction)\n",
    "\n",
    "    if is_local:\n",
    "        # Выводим текущий скор в разных разрезах\n",
    "        compare = calc_score()\n",
    "        pass\n",
    "    \n",
    "    count += 1\n",
    "    # Переходим к следующему data_block_id на итерации тестов\n",
    "    cur_test_data_block_id += 1\n",
    "    print(p_time(), 'Iteration run time:', round(time.time() - iteration_start_time))\n",
    "    print('')\n",
    "    print('________________________________________________')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет скора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = calc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    dump(models.models['model-1'], 'models.joblib')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.models['model-1'].booster_.save_model('моя_модель.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = load('models.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "count = 0\n",
    "for cur_predict in models.predictions['model-1']:\n",
    "    cur_predict = cur_predict * 0\n",
    "    print(cur_predict)\n",
    "    for model_name in models.predictions:\n",
    "        #print(models.predictions[model_name][count])\n",
    "        \n",
    "        break\n",
    "    count += 1\n",
    "    \n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График MAE по дням предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводит график средних ошибок сгруппированных по дням (точнее для блоков данных для предсказаний которые в целом эквиваленты дням)\n",
    "def print_err(err_name, err_lable, err_title):\n",
    "    # Группируем по data_block_id, то есть по дням и считаем отдельно для каждого дня предсказания MAE\n",
    "    grouped_compare = compare.groupby('data_block_id').mean().reset_index()\n",
    "    # Делаем скользящую среднюю\n",
    "    grouped_compare['rolling_mean'] = grouped_compare[err_name].rolling(window=30, min_periods=1).mean()\n",
    "    \n",
    "    \n",
    "    # Plotting the mean absolute errors\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #plt.bar(grouped_compare['data_block_id'], grouped_compare['abs_err'])\n",
    "    plt.bar(grouped_compare['data_block_id'], grouped_compare[err_name], label=err_lable)\n",
    "    plt.plot(grouped_compare['data_block_id'],\n",
    "             grouped_compare['rolling_mean'],\n",
    "             label='Rolling Mean (window=30)',\n",
    "             color='orange',\n",
    "             linestyle='-', linewidth=2)\n",
    "    plt.xlabel('data_block_id')\n",
    "    plt.ylabel(err_lable)\n",
    "    plt.title(err_title)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set ticks every 10 data_block_id\n",
    "    tick_positions_y = np.arange(-40, max(grouped_compare[err_name]) + 1, 10)\n",
    "    plt.yticks(tick_positions_y)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    print_err(err_name='abs_err',\n",
    "              err_lable='Mean Absolute Error',\n",
    "              err_title='Mean Absolute Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = compare[compare[\"data_block_id\"] > 600]\n",
    "    print_err(err_name='err',\n",
    "              err_lable='Mean Error (predict - target)',\n",
    "              err_title='Mean Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
