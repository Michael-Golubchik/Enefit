{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Время старта работы ноутбука\n",
    "notebook_starttime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращает сколько уже работает ноутбук\n",
    "def p_time():\n",
    "    #\n",
    "    run_time = round(time.time() - notebook_starttime)\n",
    "    return str(run_time).zfill(5)+' sec:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка: сабмит или локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ставим is_local в True, если локально работаем, если сабмитим - ставим в False\n",
    "#is_local = False\n",
    "is_local = True\n",
    "\n",
    "# Ставим is_gpu в True, если будем работать на GPU, если на процессоре - ставим в False\n",
    "# is_gpu = False\n",
    "is_gpu = True\n",
    "\n",
    "# Ставим is_tuning в True, если запускаем подбор гиперпараметров в Optuna\n",
    "is_tuning = False\n",
    "\n",
    "# Начальная дата обучения модели\n",
    "training_start_date = 'datetime >= \"2022-01-01 00:00:00\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonthlyKFold:\n",
    "    def __init__(self, n_splits=3):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X, y, groups=None):\n",
    "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
    "        timesteps = sorted(dates.unique().tolist())\n",
    "        X = X.reset_index()\n",
    "        \n",
    "        for t in timesteps[-self.n_splits:]:\n",
    "            idx_train = X[dates.values < t].index\n",
    "            idx_test = X[dates.values == t].index\n",
    "            \n",
    "            yield idx_train, idx_test\n",
    "            \n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days):\n",
    "    working_days = (\n",
    "        working_days\n",
    "        .with_columns(\n",
    "            pl.col(\"date\").cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_data = (\n",
    "        df_data\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_client = (\n",
    "        df_client\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_gas = (\n",
    "        df_gas\n",
    "        .rename({\"forecast_date\": \"date\"})\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_electricity = (\n",
    "        df_electricity\n",
    "        .rename({\"forecast_date\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\") + pl.duration(days=1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_location = (\n",
    "        df_location\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_forecast = (\n",
    "        df_forecast\n",
    "        .rename({\"forecast_datetime\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            #pl.col('datetime').dt.convert_time_zone(\"Europe/Bucharest\").dt.replace_time_zone(None).cast(pl.Datetime(\"us\")),\n",
    "            pl.col('datetime').dt.replace_time_zone(None).cast(pl.Datetime(\"us\"))\n",
    "            #pl.col('datetime').cast(pl.Datetime)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_historical = (\n",
    "        df_historical\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"datetime\") + pl.duration(hours=37)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_date = (\n",
    "        df_forecast\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_local = (\n",
    "        df_forecast\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    df_historical_date = (\n",
    "        df_historical\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_historical_local = (\n",
    "        df_historical\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    # Объединение всех обработанных данных с основным датафреймом df_data\n",
    "    df_data = (\n",
    "        df_data\n",
    "        .join(df_gas, on=\"date\", how=\"left\")\n",
    "        .join(df_client, on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n",
    "        .join(df_electricity, on=\"datetime\", how=\"left\")\n",
    "        \n",
    "        .join(df_forecast_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n",
    "        .join(df_forecast_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n",
    "        .join(df_historical_date, on=\"datetime\", how=\"left\", suffix=\"_hd\")\n",
    "        .join(df_historical_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl\")\n",
    "        \n",
    "        .join(df_forecast_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fdw\")\n",
    "        .join(df_forecast_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_flw\")\n",
    "        .join(df_historical_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hdw\")\n",
    "        .join(df_historical_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hlw\")\n",
    "        \n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        #.join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=8)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=9)).rename({\"target\": \"target_8\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=10)).rename({\"target\": \"target_9\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=11)).rename({\"target\": \"target_10\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=12)).rename({\"target\": \"target_11\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=13)).rename({\"target\": \"target_12\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_13\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        #Добавляем коолонку рабочий день или нет.\n",
    "        .join(working_days, on=\"date\", how=\"left\")\n",
    "        # Создание категориальных признаков и тригонометрических функций времени\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"), # Добавление номера дня в году\n",
    "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),# Добавление часа\n",
    "            pl.col(\"datetime\").dt.day().alias(\"day\"),# Добавление дня\n",
    "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),# Добавление дня недели\n",
    "            pl.col(\"datetime\").dt.month().alias(\"month\"),# Добавление месяца\n",
    "            pl.col(\"datetime\").dt.year().alias(\"year\"),# Добавление года\n",
    "        )\n",
    "        # Приведение типов данных\n",
    "        .with_columns(\n",
    "            pl.concat_str(\"county\", \"is_business\", \"product_type\", \"is_consumption\", separator=\"_\").alias(\"category_1\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"), # Тригонометрические функции для дня в году\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).cast(pl.Float32),\n",
    "        )\n",
    "         # Удаление ненужных колонок\n",
    "        .drop(\"date\", \"datetime\", \"hour\", \"dayofyear\")\n",
    "    )\n",
    "    \n",
    "    # return df_data, df_historical_local\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(X, y=None):\n",
    "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
    "    \n",
    "    if y is not None:\n",
    "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
    "    else:\n",
    "        df = X.to_pandas()    \n",
    "    \n",
    "    df = df.set_index(\"row_id\")\n",
    "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "    \n",
    "    '''\n",
    "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
    "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
    "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
    "    '''\n",
    "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
    "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
    "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для оптуны\n",
    "# Диапазон гиперпараметров для модели обучающейся на данных is_consumption=1 \n",
    "def lgb_objective_cons1(trial):\n",
    "    params = {\n",
    "        'device'            : 'gpu',\n",
    "        'gpu_platform_id'   : 1,\n",
    "        'gpu_device_id'     : 0,\n",
    "        'n_estimators'      : trial.suggest_int('n_estimators', 1000, 2000),\n",
    "        'verbose'           : -1,\n",
    "        'random_state'      : 42,\n",
    "        'objective'         : 'l2',\n",
    "        'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'colsample_bytree'  : trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'colsample_bynode'  : trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 20.0),\n",
    "        'reg_lambda'        : trial.suggest_float('reg_lambda', 1e-2, 20.0),\n",
    "        'min_child_samples' : trial.suggest_int('min_child_samples', 4, 256),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', 5, 20),\n",
    "        'max_bin'           : trial.suggest_int('max_bin', 32, 200),\n",
    "    }\n",
    "\n",
    "    model  = lgb.LGBMRegressor(**params)\n",
    "    X      = df_train[df_train['is_consumption']==1].drop(columns=[\"target\", \"datetime\"]).reset_index(drop=True)\n",
    "    y      = df_train[df_train['is_consumption']==1][\"target\"].reset_index(drop=True)\n",
    "    scores = cross_val_score(model, X, y, groups=groups, cv=CV, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return -np.mean(scores)\n",
    "\n",
    "# Диапазон гиперпараметров для модели обучающейся на данных is_consumption=0\n",
    "def lgb_objective_cons0(trial):\n",
    "    params = {\n",
    "        'device'            : 'gpu',\n",
    "        'gpu_platform_id'   : 1,\n",
    "        'gpu_device_id'     : 0,\n",
    "        'n_estimators'      : trial.suggest_int('n_estimators', 1000, 2000),\n",
    "        'verbose'           : -1,\n",
    "        'random_state'      : 42,\n",
    "        'objective'         : 'l2',\n",
    "        'learning_rate'     : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'colsample_bytree'  : trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'colsample_bynode'  : trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        'reg_alpha'         : trial.suggest_float('reg_alpha', 1e-2, 20.0),\n",
    "        'reg_lambda'        : trial.suggest_float('reg_lambda', 1e-2, 20.0),\n",
    "        'min_child_samples' : trial.suggest_int('min_child_samples', 4, 256),\n",
    "        'max_depth'         : trial.suggest_int('max_depth', 5, 20),\n",
    "        'max_bin'           : trial.suggest_int('max_bin', 32, 200),\n",
    "    }\n",
    "\n",
    "    model  = lgb.LGBMRegressor(**params)\n",
    "    X      = df_train[df_train['is_consumption']==0].drop(columns=[\"target\", \"datetime\"]).reset_index(drop=True)\n",
    "    y      = df_train[df_train['is_consumption']==0][\"target\"].reset_index(drop=True)\n",
    "    scores = cross_val_score(model, X, y, groups=groups, cv=CV, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return -np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "# Для локальных вычислений. Последний data_block_id тренировочной выборки\n",
    "# А начиная со следующего data_block_id и до конца идет тест\n",
    "train_end_data_block_id = 500\n",
    "# train_end_data_block_id = 600\n",
    "\n",
    "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id', 'data_block_id']\n",
    "# В df_data_cols колонки в таком порядке в каком они потом формируются в df_data\n",
    "df_data_cols     = ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'datetime', 'data_block_id', 'row_id']\n",
    "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
    "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
    "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
    "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
    "location_cols    = ['longitude', 'latitude', 'county']\n",
    "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
    "\n",
    "save_path = None\n",
    "load_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Загрузка данных об энергопотреблении\n",
    "    train = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "    \n",
    "    # Создание сводной таблицы с средними значениями целевой переменной (target)\n",
    "    # для каждой комбинации даты, округа, типа продукта, бизнеса и потребления\n",
    "    pivot_train = train.pivot_table(\n",
    "        index='datetime',\n",
    "        columns=['county', 'product_type', 'is_business', 'is_consumption'],\n",
    "        values='target',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Переименование колонок для удобства доступа и интерпретации\n",
    "    pivot_train.columns = ['county{}_productType{}_isBusiness{}_isConsumption{}'.format(*col) for col in pivot_train.columns.values]\n",
    "    pivot_train.index = pd.to_datetime(pivot_train.index)\n",
    "    \n",
    "    pivot_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023 год "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Копирование сводной таблицы для визуализации\n",
    "    df_plot = pivot_train.copy()\n",
    "    \n",
    "    # Нормализация данных для визуализации\n",
    "    df_plot = (df_plot - df_plot.min()) / (df_plot.max() - df_plot.min())\n",
    "    \n",
    "    # Ресемплирование данных по дням и вычисление средних значений\n",
    "    df_plot_resampled_D = df_plot.resample('D').mean()\n",
    "    \n",
    "    # Визуализация нормализованных данных с прозрачностью (alpha=0.1)\n",
    "    df_plot_resampled_D.loc['2022-7':].plot(alpha=0.1, color='green', figsize=(18, 6), legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Выбор колонок, соответствующих различным категориям потребления\n",
    "    columns_consumption_0 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption0')]\n",
    "    columns_consumption_1 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption1')]\n",
    "    \n",
    "    # Создание фигуры для визуализации\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Создание пустых линий для легенды\n",
    "    plt.plot([], color='red', label='is_Consumption = 1')  # Изменено на желтый цвет\n",
    "    plt.plot([], color='black', label='is_Consumption = 0')   # Изменено на черный цвет\n",
    "    \n",
    "    # Отображение легенды\n",
    "    plt.legend()\n",
    "    \n",
    "    # Визуализация данных для 'is_Consumption = 0' черным цветом\n",
    "    for column in columns_consumption_0:\n",
    "        df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='black', legend=False)  # Изменено на черный\n",
    "    \n",
    "    # Визуализация данных для 'is_Consumption = 1' желтым цветом\n",
    "    for column in columns_consumption_1:\n",
    "        df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='red', legend=False)  # Изменено на желтый\n",
    "    \n",
    "    # Отображение графика\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Запись тестовых и тренировочных csv файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Если выполняем локально\n",
    "    train_path = 'train'\n",
    "    if not os.path.exists(train_path):\n",
    "        # Создание каталога, если его нет\n",
    "        os.makedirs(train_path)\n",
    "    test_path = 'example_test_files'\n",
    "    if not os.path.exists(test_path):\n",
    "        # Создание каталога, если его нет\n",
    "        os.makedirs(test_path)\n",
    "else:\n",
    "    # Если сабмит\n",
    "    train_path = root\n",
    "# Путь, куда запишем csv файлы для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяет датафрейм на тренировочную и тестовую часть\n",
    "# Возвращает часть датафрейма для тренировки, тестовую часть датафрейма записывает в каталог с тестами\n",
    "def split_train_test(filename):\n",
    "    df = pd.read_csv(os.path.join(root, filename))\n",
    "    \n",
    "    #Запишем часть данных для теста\n",
    "    test_df = df[df[\"data_block_id\"] > train_end_data_block_id]\n",
    "    if (filename ==\"train.csv\"):\n",
    "        # Берем только те ячейки где target был не нулевым\n",
    "        test_df = test_df[test_df[\"target\"].notnull()]\n",
    "        \n",
    "    test_df.to_csv(os.path.join(test_path, filename), index=False)\n",
    "\n",
    "    #Запишем часть данных для трейна\n",
    "    train_df = df[df[\"data_block_id\"] <= train_end_data_block_id]\n",
    "    train_df.to_csv(os.path.join(train_path, filename), index=False)\n",
    "\n",
    "# Доводим до ума тестовые таблицы чтобы они были точно такие как в реальном сабмите\n",
    "def test_dfs_tune():\n",
    "    # Делаем таблицу revealed_targets.csv\n",
    "    df = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "    df = df[df[\"data_block_id\"] > train_end_data_block_id - 2]\n",
    "    df[\"data_block_id\"] += 2\n",
    "    df = df[df[\"target\"].notnull()]\n",
    "    df.to_csv(os.path.join(test_path, 'revealed_targets.csv'), index=False)\n",
    "    \n",
    "    # Делаем таблицу test.csv\n",
    "    df = pd.read_csv(os.path.join(test_path, \"train.csv\"))\n",
    "    df.rename(columns={'datetime': 'prediction_datetime'}, inplace=True)\n",
    "    df.drop('target', axis=1, inplace=True)\n",
    "    df['currently_scored'] = False\n",
    "    df.to_csv(os.path.join(test_path, 'test.csv'), index=False)\n",
    "    \n",
    "    # Делаем таблицу sample_submission.csv\n",
    "    selected_columns = ['row_id', 'data_block_id']\n",
    "    df = df[selected_columns]\n",
    "    df['target'] = 0\n",
    "    df.to_csv(os.path.join(test_path, 'sample_submission.csv'), index=False)\n",
    "\n",
    "# Сборка разделения файлов\n",
    "def make_split():\n",
    "    # csv файлы которые будем делить:\n",
    "    csv_names = [\"train.csv\", \"client.csv\", \"gas_prices.csv\", \"electricity_prices.csv\", \"forecast_weather.csv\", \"historical_weather.csv\"]\n",
    "    for csv_name in csv_names:\n",
    "        split_train_test(csv_name)\n",
    "    # Доделываем тестовые таблицы\n",
    "    test_dfs_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Создаем файлы csv c тренировочными и тестовыми таблицами\n",
    "if is_local:\n",
    "    # Пока отключил создание тестовых файлом. У меня локально они есть\n",
    "    # make_split()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data        = pl.read_csv(os.path.join(train_path, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
    "df_client      = pl.read_csv(os.path.join(train_path, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
    "df_gas         = pl.read_csv(os.path.join(train_path, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
    "df_electricity = pl.read_csv(os.path.join(train_path, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
    "df_forecast    = pl.read_csv(os.path.join(train_path, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
    "df_historical  = pl.read_csv(os.path.join(train_path, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
    "#df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
    "df_location    = pl.read_csv('/kaggle/input/locations/county_lon_lats.csv', columns=location_cols, try_parse_dates=True)\n",
    "df_target      = df_data.select(target_cols)\n",
    "working_days   = pl.read_csv('/kaggle/input/working-days/working_days.csv', try_parse_dates=True)\n",
    "\n",
    "schema_data        = df_data.schema\n",
    "schema_client      = df_client.schema\n",
    "schema_gas         = df_gas.schema\n",
    "schema_electricity = df_electricity.schema\n",
    "schema_forecast    = df_forecast.schema\n",
    "schema_historical  = df_historical.schema\n",
    "schema_target      = df_target.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор гиперпараметров для модели is_consumption=1\n",
    "if is_tuning:\n",
    "    X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "    X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "    df_train = to_pandas(X, y)\n",
    "\n",
    "    df_train = df_train[df_train[\"target\"].notnull()].query(training_start_date)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize', study_name='Regressor')\n",
    "    study.optimize(lgb_objective_cons1, n_trials=100, show_progress_bar=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result = cross_validate(\n",
    "    estimator=lgb.LGBMRegressor(**best_params, random_state=42),\n",
    "    X=df_train.drop(columns=[\"target\"]), \n",
    "    y=df_train[\"target\"],\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=MonthlyKFold(1),\n",
    ")\n",
    "\n",
    "print(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\n",
    "print(f\"Score Time(s): {result['score_time'].mean():.3f}\")\n",
    "print(f\"Error(MAE): {-result['test_score'].mean():.3f}\")'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB is_consumption=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестируем на GPU\n",
    "if is_gpu:\n",
    "    p1={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1953, 'learning_rate': 0.06292846025674187, 'colsample_bytree': 0.885312594375526, 'colsample_bynode': 0.7831589504736596, 'reg_alpha': 7.677890737474055, 'reg_lambda': 16.04294344413536, 'min_child_samples': 69, 'max_depth': 13, 'max_bin': 122}\n",
    "    p2={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1781, 'learning_rate': 0.054763617969941024, 'colsample_bytree': 0.9233976730768843, 'colsample_bynode': 0.8969396545503117, 'reg_alpha': 9.893144785987541, 'reg_lambda': 19.956571072303618, 'min_child_samples': 30, 'max_depth': 11, 'max_bin': 128}\n",
    "    p3={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1713, 'learning_rate': 0.053743689746779025, 'colsample_bytree': 0.8813612023144495, 'colsample_bynode': 0.8739181786588937, 'reg_alpha': 6.967728410825889, 'reg_lambda': 18.772923941576302, 'min_child_samples': 8, 'max_depth': 11, 'max_bin': 130}\n",
    "    p4={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1852, 'learning_rate': 0.0592383731526721, 'colsample_bytree': 0.9158803696144284, 'colsample_bynode': 0.8677708234107011, 'reg_alpha': 8.178421543960194, 'reg_lambda': 18.453388032680913, 'min_child_samples': 7, 'max_depth': 13, 'max_bin': 127}\n",
    "    p5={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1854, 'learning_rate': 0.06206288936616207, 'colsample_bytree': 0.8880309956381198, 'colsample_bynode': 0.8902254554150658, 'reg_alpha': 8.741757862603745, 'reg_lambda': 16.131881076970146, 'min_child_samples': 11, 'max_depth': 13, 'max_bin': 133}\n",
    "    p6={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1760, 'learning_rate': 0.05058624430058491, 'colsample_bytree': 0.8623393986024189, 'colsample_bynode': 0.9242352457544074, 'reg_alpha': 10.403491499807716, 'reg_lambda': 17.336966880450976, 'min_child_samples': 16, 'max_depth': 12, 'max_bin': 131}\n",
    "    p7={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1783, 'learning_rate': 0.053984137286090667, 'colsample_bytree': 0.7988556940541043, 'colsample_bynode': 0.8962066698736209, 'reg_alpha': 10.033954743421075, 'reg_lambda': 19.869585514651835, 'min_child_samples': 29, 'max_depth': 11, 'max_bin': 128}\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB is_consumption=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_gpu:\n",
    "    # Параметры для catboost c GPU\n",
    "    c1={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1928, 'learning_rate': 0.07746737348328141, 'colsample_bytree': 0.78312363495825, 'colsample_bynode': 0.519093615242415, 'reg_alpha': 3.5475701282438967, 'reg_lambda': 17.88655785653258, 'min_child_samples': 20, 'max_depth': 17, 'max_bin': 37}\n",
    "    c2={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1881, 'learning_rate': 0.07864103296902626, 'colsample_bytree': 0.7910482238038328, 'colsample_bynode': 0.48895580052374654, 'reg_alpha': 7.375666493081315, 'reg_lambda': 15.347283246448637, 'min_child_samples': 15, 'max_depth': 15, 'max_bin': 41}\n",
    "    c3={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1959, 'learning_rate': 0.06577095971205155, 'colsample_bytree': 0.8648745524121579, 'colsample_bynode': 0.4208704631805802, 'reg_alpha': 7.334717669004279, 'reg_lambda': 19.53863883138691, 'min_child_samples': 17, 'max_depth': 15, 'max_bin': 37}\n",
    "    c4={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1962, 'learning_rate': 0.06171792620856953, 'colsample_bytree': 0.9584819437986425, 'colsample_bynode': 0.5405402088572034, 'reg_alpha': 3.307191570022457, 'reg_lambda': 16.209300259712922, 'min_child_samples': 43, 'max_depth': 19, 'max_bin': 40}\n",
    "    c5={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1900, 'learning_rate': 0.07441152178622895, 'colsample_bytree': 0.8801531103073503, 'colsample_bynode': 0.337510749744266, 'reg_alpha': 6.105532634402442, 'reg_lambda': 19.489948983222703, 'min_child_samples': 28, 'max_depth': 11, 'max_bin': 51}\n",
    "    c6={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1923, 'learning_rate': 0.07413128975186019, 'colsample_bytree': 0.9108240472209401, 'colsample_bynode': 0.368498860839913, 'reg_alpha': 5.928691934278827, 'reg_lambda': 19.535434444939284, 'min_child_samples': 35, 'max_depth': 16, 'max_bin': 43}\n",
    "    c7={'device': 'gpu', 'verbose': -1, 'random_state': 42, 'objective': 'l2', 'n_estimators': 1899, 'learning_rate': 0.07536122429341643, 'colsample_bytree': 0.8755415544495265, 'colsample_bynode': 0.3971515541568271, 'reg_alpha': 7.825234820629166, 'reg_lambda': 19.539558271427442, 'min_child_samples': 16, 'max_depth': 10, 'max_bin': 67}\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Класс моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс обертка для моделей. Хранит различные пареметры для обучения моделей.\n",
    "# Например диапазоны данных на которых учить модель, как часто обучать заново и другие параметры\n",
    "class Models:\n",
    "    \n",
    "    # Инициализирует параметры обучения\n",
    "    # init_model - готовый объект модели для обучения\n",
    "    def __init__(self):\n",
    "        # Инициализируем словари\n",
    "        # Ключами во всех словарях будет имя модели\n",
    "        \n",
    "        # Словарь с моделями\n",
    "        self.models =  dict()\n",
    "        \n",
    "        # Словарь с описанием периодов обучения модели\n",
    "        # пока это матрица. из двух столбцов и двух строк в каждой строке описание периода\n",
    "        # первая колонка на сколько data_block_id в конеце обучени отстоит от доступного конца данных\n",
    "        # вторая колонка сколько data_block_id будет в периоде на котором обучаемся.\n",
    "        # data_block_id могут быть эквивалентны дням, но могут и отличаться, если данные будут подавать блоками не равными дням\n",
    "        # либо если будут разрывы в данных. Но они точно будут эквиваленты циклам предсказания\n",
    "        self.data_block_id_intervals  = dict()\n",
    "\n",
    "        # Словарь с указанием по какой минимальный data_block_id отрезать данные.\n",
    "        # То есть меньше data_block_id_min не берем данные для обучения в любом случае, чтобы не было определено в data_block_id_intervals\n",
    "        self.data_block_id_min  = dict()\n",
    "        \n",
    "        # Если 1, то модель предназначена для предсказания потребления электричества\n",
    "        # Если 0, то модель предназначена для предсказания производства электричества\n",
    "        self.is_consumption = dict()\n",
    "        \n",
    "        # Раз во сколько иттераций обучат модель\n",
    "        self.learn_again_period = dict()\n",
    "        \n",
    "        # Смещение для начала обучения модели. Добавляется к номеру итерации сабмита.\n",
    "        # Скажем если смещение 6 номер итерации 1, а обучаемся раз в чем итераций. То обучение будет в первуже итерацию сабмита.\n",
    "        self.learn_again_offset = dict()\n",
    "        \n",
    "        # Время в секундах сколько заняло последнее обучение модели\n",
    "        self.last_learn_time = dict()\n",
    "    \n",
    "    # Добавляет еще одну модель\n",
    "    # model_name - название модели\n",
    "    # new_model - объект модели\n",
    "    def add_model(self, model_name, new_model, is_consumption, data_block_id_intervals,\n",
    "                  data_block_id_min, learn_again_period, learn_again_offset):\n",
    "        \n",
    "        self.models[model_name] = new_model\n",
    "        self.is_consumption[model_name] = is_consumption\n",
    "        self.data_block_id_intervals[model_name] = data_block_id_intervals\n",
    "        self.data_block_id_min[model_name] = data_block_id_min\n",
    "        self.learn_again_period[model_name] = learn_again_period\n",
    "        self.learn_again_offset[model_name] = learn_again_offset\n",
    "        self.last_learn_time[model_name] = 0\n",
    "        \n",
    "    # Обучает модель\n",
    "    # model_name - название модели\n",
    "    # df_train - датафрейм который содержит данные для обучения и целевой признак\n",
    "    def fit_one_model(self, model_name, df_train):\n",
    "        print(p_time(), 'fit model:', model_name)\n",
    "        self.models[model_name].fit(\n",
    "            X=df_train.drop(columns=[\"target\", \"data_block_id\"]),\n",
    "            y=df_train[\"target\"]\n",
    "        )\n",
    "    \n",
    "    # Делает предсказания от отдельной модели.\n",
    "    # model_name - название модели\n",
    "    # X - признаки на которых нужно сделать предсказание\n",
    "    def predict_one_model(self, model_name, X):\n",
    "        print(p_time(), 'predict model:', model_name)\n",
    "        y = (self.models[model_name]\n",
    "             .predict(X.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "            )\n",
    "        return(y)\n",
    "    \n",
    "    # Обучает все добавленные модели для которых наступило время их обучения\n",
    "    # df_train - датафрейм который содержит данные для обучения и целевой признак\n",
    "    # itter_n - номер иттерации в сабмите.\n",
    "    # если itter_n равен <=0. Значит первоначальное обучение и обучаем всем модели\n",
    "    def fit(self, df_train, itter_n):\n",
    "        max_block_id = df_train[\"data_block_id\"].max()\n",
    "        # Перебираем все модели\n",
    "        for m_name in self.learn_again_period:\n",
    "            # Либо сказали все модели учить\n",
    "            if ((itter_n <= 0)\n",
    "                # Либо учим если номер итераиции в сабмите плюс смещение делится на цело на период обучения\n",
    "                or (((itter_n + self.learn_again_offset[m_name])\n",
    "                     % self.learn_again_period[m_name]) == 0)):\n",
    "                \n",
    "                d_i = self.data_block_id_intervals[m_name]\n",
    "                \n",
    "                # Выделяем данные для обучения\n",
    "                df_train_int = df_train[\n",
    "                    # До какого data_block_id учим первый блок\n",
    "                    (((df_train['data_block_id']<=max_block_id-d_i[0][0])\n",
    "                     # C какого data_block_id учим первый блок\n",
    "                     &(df_train['data_block_id']>(max_block_id-d_i[0][0]-d_i[0][1])))\n",
    "                    # До какого data_block_id учим второй блок\n",
    "                    |((df_train['data_block_id']<=max_block_id-d_i[1][0])\n",
    "                      # С какого data_block_id учим второй блок\n",
    "                      &(df_train['data_block_id']>(max_block_id-d_i[1][0]-d_i[1][1]))))\n",
    "                #  выбираем только те данные которые больше data_block_id_min\n",
    "                &(df_train['data_block_id']>=self.data_block_id_min[m_name])\n",
    "                #  выбираем только те данные для обучения по is_consumption на которых специализируетсмя модель\n",
    "                &(df_train['is_consumption']==self.is_consumption[m_name])\n",
    "                # Оставляем только notnull таргеты\n",
    "                &(df_train[\"target\"].notnull())\n",
    "                ]\n",
    "                \n",
    "                # Обучаем модель\n",
    "                self.fit_one_model(m_name, df_train_int)\n",
    "    \n",
    "    # Делает предсказание всеми добавленными моделями и сводит их в одно предсказание\n",
    "    def predict(self, X):\n",
    "        # Создаем датафрейм для предсказаний\n",
    "        # Первый столбец содержит информацию 'is_consumption'\n",
    "        # Для каждой модели отдельный столбец с предсказаниями\n",
    "        # В predict_df_0 будут предсказания для моделей с is_consumption == 0\n",
    "        predict_df_0 = pd.DataFrame(X['is_consumption'])\n",
    "        # В predict_df_1 будут предсказания для моделей с is_consumption == 1\n",
    "        predict_df_1 = pd.DataFrame(X['is_consumption'])\n",
    "        \n",
    "        # Перебираем все модели\n",
    "        for m_name in self.learn_again_period:\n",
    "            # Делаем предсказание в соответвующий датафрейм в столбец модели\n",
    "            if self.is_consumption[m_name] == 0:\n",
    "                predict_df_0[m_name] = self.predict_one_model(m_name, X).clip(0)\n",
    "            else:\n",
    "                predict_df_1[m_name] = self.predict_one_model(m_name, X).clip(0)\n",
    "                \n",
    "        # Суммируем предсказания моделей раздельно по датафреймам\n",
    "        predict_df_0['target'] = predict_df_0.iloc[:, 1:].mean(axis=1)\n",
    "        predict_df_1['target'] = predict_df_1.iloc[:, 1:].mean(axis=1)\n",
    "        \n",
    "        # Сведение в одно предсказание потребления и производства электричества у просьюмеров\n",
    "        predict_df = predict_df_1[['is_consumption', 'target']]\n",
    "\n",
    "        predict_df.loc[predict_df['is_consumption']==0, 'target'] = predict_df_0.loc[predict_df_0['is_consumption']==0, 'target']\n",
    "        \n",
    "        return predict_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем модель для потредление электричества\n",
    "'''\n",
    "models.add_model(\n",
    "    model_name = 'model',\n",
    "    new_model = lgb.LGBMRegressor(**p1, verbosity=-1, random_state=42),\n",
    "    is_consumption = 1,\n",
    "    data_block_id_intervals = [[0,30],[305,90]],\n",
    "    data_block_id_min = 122,\n",
    "    learn_again_period = 7,\n",
    "    learn_again_offset = 0\n",
    ")\n",
    "\n",
    "# Добавляем модель для генерации электричества\n",
    "models.add_model(\n",
    "    model_name = 'model_solar',\n",
    "    new_model = lgb.LGBMRegressor(**p1, verbosity=-1, random_state=42),\n",
    "    is_consumption = 0,\n",
    "    data_block_id_intervals = [[0,30],[305,90]],\n",
    "    data_block_id_min = 122,\n",
    "    learn_again_period = 7,\n",
    "    learn_again_offset = 0\n",
    ")\n",
    "'''\n",
    "\n",
    "model = VotingRegressor([\n",
    "    ('lgb_1', lgb.LGBMRegressor(**p1)), \n",
    "    ('lgb_2', lgb.LGBMRegressor(**p2)), \n",
    "    ('lgb_3', lgb.LGBMRegressor(**p3)), \n",
    "    ('lgb_4', lgb.LGBMRegressor(**p4)), \n",
    "    ('lgb_5', lgb.LGBMRegressor(**p5)),\n",
    "    #('lgb_6', lgb.LGBMRegressor(**p6)),\n",
    "    #('lgb_7', lgb.LGBMRegressor(**p7)),  \n",
    "])\n",
    "\n",
    "model_solar = VotingRegressor([\n",
    "    ('lgb_1', lgb.LGBMRegressor(**c1)), \n",
    "    ('lgb_2', lgb.LGBMRegressor(**c2)), \n",
    "    ('lgb_3', lgb.LGBMRegressor(**c3)), \n",
    "    ('lgb_4', lgb.LGBMRegressor(**c4)), \n",
    "    ('lgb_5', lgb.LGBMRegressor(**c5)),\n",
    "    #('lgb_6', lgb.LGBMRegressor(**c6)),\n",
    "    #('lgb_7', lgb.LGBMRegressor(**c7)),  \n",
    "])\n",
    "\n",
    "models.add_model(\n",
    "    model_name = 'model',\n",
    "    new_model = model,\n",
    "    is_consumption = 1,\n",
    "    data_block_id_intervals = [[0,1000],[0,0]],\n",
    "    data_block_id_min = 0,\n",
    "    learn_again_period = 2,\n",
    "    learn_again_offset = 0\n",
    ")\n",
    "\n",
    "# Добавляем модель для генерации электричества\n",
    "models.add_model(\n",
    "    model_name = 'model_solar',\n",
    "    new_model = model_solar,\n",
    "    is_consumption = 0,\n",
    "    data_block_id_intervals = [[0,1000],[0,0]],\n",
    "    data_block_id_min = 0,\n",
    "    learn_again_period = 2,\n",
    "    learn_again_offset = 0\n",
    ")\n",
    "\n",
    "'''\n",
    "models.add_model(\n",
    "    model_name = 'model',\n",
    "    new_model = lgb.LGBMRegressor(**p1, verbosity=-1, random_state=42),\n",
    "    is_consumption = 1,\n",
    "    data_block_id_intervals = [[0,90],[305,120]],\n",
    "    data_block_id_min = 122,\n",
    "    learn_again_period = 7,\n",
    "    learn_again_offset = 0\n",
    ")\n",
    "\n",
    "# Добавляем модель для генерации электричества\n",
    "models.add_model(\n",
    "    model_name = 'model_solar',\n",
    "    new_model = lgb.LGBMRegressor(**p1, verbosity=-1, random_state=42),\n",
    "    is_consumption = 0,\n",
    "    data_block_id_intervals = [[0,90],[305,120]],\n",
    "    data_block_id_min = 122,\n",
    "    learn_again_period = 7,\n",
    "    learn_again_offset = 0\n",
    ")\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if is_local:\n",
    "#    dump(model_solar, 'model_solar.joblib')\n",
    "#    dump(model, 'model_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расскоментировать при необходимости загрузку ранее сохраненных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для загрузки локально\n",
    "#model_solar = load('model_solar.joblib')\n",
    "#model = load('model_lgbm.joblib')\n",
    "\n",
    "# Для загрузки на kaggle\n",
    "#model_solar = load('/kaggle/input/enefit/model_solar.joblib')\n",
    "#model = load('/kaggle/input/enefit/model_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Если выполняем локально, а не сабмитим на кагл,\n",
    "    # то выбираем другое имя для файла submission.csv.\n",
    "    # Потому что в submission.csv записать прав нет и вылетает по ошибке\n",
    "    submission_name = 'submission_loc.csv'\n",
    "else:\n",
    "    submission_name = 'submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержимое public_timeseries_testing_util.py\n",
    "\n",
    "С необходимыми праками. Решил не импортировать его. а прямо тут. Так удобнее переносить на kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An unlocked version of the timeseries API intended for testing alternate inputs.\n",
    "Mirrors the production timeseries API in the crucial respects, but won't be as fast.\n",
    "\n",
    "ONLY works afer the first three variables in MockAPI.__init__ are populated.\n",
    "'''\n",
    "\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "\n",
    "class MockApi:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n",
    "        They've been intentionally left in an invalid state.\n",
    "\n",
    "        Variables to set:\n",
    "            input_paths: a list of two or more paths to the csv files to be served\n",
    "            group_id_column: the column that identifies which groups of rows the API should serve.\n",
    "                A call to iter_test serves all rows of all dataframes with the current group ID value.\n",
    "            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n",
    "        '''\n",
    "        self.input_paths: Sequence[str] = ['example_test_files/test.csv',\n",
    "                                   'example_test_files/revealed_targets.csv', \n",
    "                                   'example_test_files/client.csv',\n",
    "                                   'example_test_files/historical_weather.csv',\n",
    "                                   'example_test_files/forecast_weather.csv',\n",
    "                                   'example_test_files/electricity_prices.csv',\n",
    "                                   'example_test_files/gas_prices.csv',\n",
    "                                   'example_test_files/sample_submission.csv']\n",
    "        self.group_id_column: str = 'data_block_id'\n",
    "        self.export_group_id_column: bool = False\n",
    "        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n",
    "        assert len(self.input_paths) >= 2\n",
    "\n",
    "        self._status = 'initialized'\n",
    "        self.predictions = []\n",
    "\n",
    "    def iter_test(self) -> Tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Loads all of the dataframes specified in self.input_paths,\n",
    "        then yields all rows in those dataframes that equal the current self.group_id_column value.\n",
    "        '''\n",
    "        if self._status != 'initialized':\n",
    "\n",
    "            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n",
    "\n",
    "        dataframes = []\n",
    "        for pth in self.input_paths:\n",
    "            dataframes.append(pd.read_csv(pth, low_memory=False))\n",
    "        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n",
    "        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n",
    "\n",
    "        for group_id in group_order:\n",
    "            self._status = 'prediction_needed'\n",
    "            current_data = []\n",
    "            for df in dataframes:\n",
    "                cur_df = df.loc[group_id].copy()\n",
    "                # returning single line dataframes from df.loc requires special handling\n",
    "                if not isinstance(cur_df, pd.DataFrame):\n",
    "                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n",
    "                    cur_df.index.name = self.group_id_column\n",
    "                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n",
    "                current_data.append(cur_df)\n",
    "            yield tuple(current_data)\n",
    "\n",
    "            while self._status != 'prediction_received':\n",
    "                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n",
    "                yield None\n",
    "\n",
    "        with open(submission_name, 'w') as f_open:\n",
    "            pd.concat(self.predictions).to_csv(f_open, index=False)\n",
    "        self._status = 'finished'\n",
    "\n",
    "    def predict(self, user_predictions: pd.DataFrame):\n",
    "        '''\n",
    "        Accepts and stores the user's predictions and unlocks iter_test once that is done\n",
    "        '''\n",
    "        if self._status == 'finished':\n",
    "            raise Exception('You have already made predictions for the full test set.')\n",
    "        if self._status != 'prediction_needed':\n",
    "            raise Exception('You must get the next test sample from `iter_test()` first.')\n",
    "        if not isinstance(user_predictions, pd.DataFrame):\n",
    "            raise Exception('You must provide a DataFrame.')\n",
    "\n",
    "        self.predictions.append(user_predictions)\n",
    "        self._status = 'prediction_received'\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return MockApi()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для скора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчитывает скор.\n",
    "# Возвращает датафрейм compare со сравнением предсказаний\n",
    "def calc_score():\n",
    "    # Загружаем предсказания\n",
    "    #submission = pd.read_csv(submission_name)\n",
    "    submission = pd.concat(env.predictions)\n",
    "    \n",
    "    # Загружаем истинные значения\n",
    "    revealed_targets = pd.read_csv(os.path.join(test_path, \"revealed_targets.csv\"))\n",
    "    revealed_targets['data_block_id'] -= 2\n",
    "    revealed_targets = revealed_targets[revealed_targets[\"data_block_id\"] > train_end_data_block_id]\n",
    "    # Обрезаем реальные предсказания revealed_targets по длине уже сделанных предсказаний submission\n",
    "    revealed_targets = revealed_targets.iloc[:len(submission)]\n",
    "\n",
    "    mae = mean_absolute_error(revealed_targets['target'] , submission['target'])\n",
    "    # print(f'MAE: {mae}')\n",
    "    \n",
    "    # Подготовим данные для анализа изменения ошибки предсказания по мере удаления от времени завершения обучения\n",
    "    compare = revealed_targets[['data_block_id', 'is_consumption', 'target']].copy()\n",
    "    compare['predict'] = submission['target'].values\n",
    "    compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n",
    "    compare['err'] = (compare['predict'] - compare['target']).values\n",
    "\n",
    "    if (compare['data_block_id'].max() > 600):\n",
    "        compare_600 = compare[compare['data_block_id'] > 600]\n",
    "        # Выводим MAE для data_block_id > 600\n",
    "        \n",
    "        mae_600 = mean_absolute_error(compare_600['target'], compare_600['predict'])\n",
    "        \n",
    "        # Выводим MAE для data_block_id > 600 и is_consumption == 0\n",
    "        mae_600_cons_0 = mean_absolute_error(compare_600[compare_600['is_consumption']==0]['target'], compare_600[compare_600['is_consumption']==0]['predict'])\n",
    "        \n",
    "        # Выводим MAE для data_block_id > 600 и is_consumption == 0\n",
    "        mae_600_cons_1 = mean_absolute_error(compare_600[compare_600['is_consumption']==1]['target'], compare_600[compare_600['is_consumption']==1]['predict'])\n",
    "    else:\n",
    "        # Если еще не дошли до data_block_id > 600 не считаем эти величины\n",
    "        mae_600, mae_600_cons_0, mae_600_cons_1 = '-', '-', '-'\n",
    "\n",
    "    mae_df = pd.DataFrame({\n",
    "        '(ALL)': mae,\n",
    "        '(> 600)': mae_600,\n",
    "        '(> 600, is_cons==0)': mae_600_cons_0,\n",
    "        '(> 600, is_cons==1)': mae_600_cons_1\n",
    "    }, index=['MAE'])\n",
    "\n",
    "    # Округляем числа до двух знаков после запятой и преобразуем их в строки\n",
    "    mae_df = mae_df.round(3).astype(str)\n",
    "    \n",
    "    display(mae_df)\n",
    "    return compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация иттераций сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # После этого можно имитировать локально загрузку при собмите на большом числе итераций\n",
    "    # А не только четыре иттерации на 4 дня как в стандартной имитайии на кагле\n",
    "    env = make_env()\n",
    "else:\n",
    "    # загружаем оригинальную библиотеку для сабмита\n",
    "    import enefit\n",
    "    env = enefit.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим последний data_block_id в обучающих данных\n",
    "max_train_data_block_id = df_data[\"data_block_id\"].max()\n",
    "# Устанавливаем первый data_block_id для теста следущим за тренировочным\n",
    "cur_test_data_block_id = max_train_data_block_id + 1\n",
    "cur_test_data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 0\n",
    "\n",
    "# Основной цикл для обработки данных тестового набора\n",
    "for (test, revealed_targets, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
    "    print(p_time(), '*** Iteration: ', count, '***')\n",
    "    # Переименование столбца для удобства\n",
    "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "    if is_local:\n",
    "        # Если выполняем локально, то преобразуем некоторые типы данных\n",
    "        # На кагле (а может и в линуксе) они и так преобразуются, но на виновс локально\n",
    "        # не преобразуются и выдетают по ощибке\n",
    "        test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "        client['date'] = pd.to_datetime(client['date'])\n",
    "        gas_prices['origin_date'] = pd.to_datetime(gas_prices['origin_date'])\n",
    "        gas_prices['forecast_date'] = pd.to_datetime(gas_prices['forecast_date'])\n",
    "        electricity_prices['origin_date'] = pd.to_datetime(electricity_prices['origin_date'])\n",
    "        electricity_prices['forecast_date'] = pd.to_datetime(electricity_prices['forecast_date'])\n",
    "        forecast_weather['origin_datetime'] = pd.to_datetime(forecast_weather['origin_datetime'])\n",
    "        forecast_weather['forecast_datetime'] = pd.to_datetime(forecast_weather['forecast_datetime'])\n",
    "        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n",
    "        revealed_targets['datetime'] = pd.to_datetime(revealed_targets['datetime'])\n",
    "        \n",
    "    # Добавляем колонку заполненную следующим data_block_id\n",
    "    test[\"data_block_id\"] = cur_test_data_block_id\n",
    "    revealed_targets[\"data_block_id\"] = cur_test_data_block_id\n",
    "    \n",
    "    df_test            = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
    "    df_new_client      = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
    "    df_new_gas         = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
    "    df_new_electricity = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
    "    df_new_forecast    = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
    "    df_new_historical  = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
    "    df_new_target      = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
    "    df_new_data        = pl.from_pandas(revealed_targets[df_data_cols], schema_overrides=schema_data)\n",
    "    # Объединение новых данных с существующими и удаление дубликатов\n",
    "    df_client          = pl.concat([df_client, df_new_client]).unique(subset=[\"county\", \"is_business\", \"product_type\", \"date\"], maintain_order=True)\n",
    "    df_gas             = pl.concat([df_gas, df_new_gas]).unique(subset=[\"forecast_date\"], maintain_order=True)\n",
    "    df_electricity     = pl.concat([df_electricity, df_new_electricity]).unique(subset=[\"forecast_date\"], maintain_order=True)\n",
    "    df_forecast        = pl.concat([df_forecast, df_new_forecast]).unique()\n",
    "    df_historical      = pl.concat([df_historical, df_new_historical]).unique()\n",
    "    df_target          = pl.concat([df_target, df_new_target]).unique()\n",
    "    df_data            = pl.concat([df_data, df_new_data]).unique()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if (count == 0):\n",
    "        X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "        X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "        df_train = to_pandas(X, y)\n",
    "        \n",
    "        models.fit(df_train=df_train, itter_n=count)\n",
    "    \n",
    "    # Применение функции инженерии признаков и преобразование данных обратно в pandas\n",
    "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "    X_test = to_pandas(X_test)\n",
    "    \n",
    "    # Прогнозирование с использованием модели и ограничение предсказаний нулем\n",
    "    #test['target'] = model.predict(X_test.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "    test['target'] = models.predict(X_test)\n",
    "    \n",
    "    # Обновление целевых значений в примере предсказания\n",
    "    sample_prediction[\"target\"] = test['target']\n",
    "    '''\n",
    "    \n",
    "    if (not(is_local) or (count == 0) or (count>=100)):\n",
    "    #if (not(is_local) or (count>=0)):\n",
    "        # Если испольняем локально только после\n",
    "        # итерации 100. Потому что интересует как ведет себя моделья через два месяца\n",
    "        # после обучения\n",
    "        cur_time = time.time()\n",
    "        if ((cur_time - notebook_starttime) < (8*60*60 + 60*30)):\n",
    "            X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "            X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "            df_train = to_pandas(X, y)\n",
    "            \n",
    "            models.fit(df_train=df_train, itter_n=count)\n",
    "        else:\n",
    "            print('Не тренеруем модель, превышено время выполнения ноутбука:', (cur_time - notebook_starttime))\n",
    "\n",
    "    # Применение функции инженерии признаков и преобразование данных обратно в pandas\n",
    "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target, working_days)\n",
    "    X_test = to_pandas(X_test)\n",
    "    \n",
    "    # Прогнозирование с использованием модели и ограничение предсказаний нулем\n",
    "    #test['target'] = model.predict(X_test.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "    test['target'] = models.predict(X_test)\n",
    "    \n",
    "    # Обновление целевых значений в примере предсказания\n",
    "    sample_prediction[\"target\"] = test['target']\n",
    "    \n",
    "    # Отправка предсказаний в среду выполнения\n",
    "    env.predict(sample_prediction)\n",
    "\n",
    "    if is_local:\n",
    "        # Выводим текущий скор в разных разрезах\n",
    "        compare = calc_score()\n",
    "    \n",
    "    count += 1\n",
    "    # Переходим к следующему data_block_id на итерации тестов\n",
    "    cur_test_data_block_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет скора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = calc_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График MAE по дням предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводит график средних ошибок сгруппированных по дням (точнее для блоков данных для предсказаний которые в целом эквиваленты дням)\n",
    "def print_err(err_name, err_lable, err_title):\n",
    "    # Группируем по data_block_id, то есть по дням и считаем отдельно для каждого дня предсказания MAE\n",
    "    grouped_compare = compare.groupby('data_block_id').mean().reset_index()\n",
    "    # Делаем скользящую среднюю\n",
    "    grouped_compare['rolling_mean'] = grouped_compare[err_name].rolling(window=30, min_periods=1).mean()\n",
    "    \n",
    "    \n",
    "    # Plotting the mean absolute errors\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #plt.bar(grouped_compare['data_block_id'], grouped_compare['abs_err'])\n",
    "    plt.bar(grouped_compare['data_block_id'], grouped_compare[err_name], label=err_lable)\n",
    "    plt.plot(grouped_compare['data_block_id'],\n",
    "             grouped_compare['rolling_mean'],\n",
    "             label='Rolling Mean (window=30)',\n",
    "             color='orange',\n",
    "             linestyle='-', linewidth=2)\n",
    "    plt.xlabel('data_block_id')\n",
    "    plt.ylabel(err_lable)\n",
    "    plt.title(err_title)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set ticks every 10 data_block_id\n",
    "    tick_positions_y = np.arange(-40, max(grouped_compare[err_name]) + 1, 10)\n",
    "    plt.yticks(tick_positions_y)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    print_err(err_name='abs_err',\n",
    "              err_lable='Mean Absolute Error',\n",
    "              err_title='Mean Absolute Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = compare[compare[\"data_block_id\"] > 600]\n",
    "    print_err(err_name='err',\n",
    "              err_lable='Mean Error (predict - target)',\n",
    "              err_title='Mean Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
