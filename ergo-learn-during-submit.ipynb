{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Время старта работы ноутбука\n",
    "notebook_starttime = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройка: сабмит или локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ставим is_local в True, если локально работаем, если сабмитим - ставим в False\n",
    "#is_local = False\n",
    "is_local = True\n",
    "\n",
    "# Ставим is_gpu в True, если будем работать на GPU, если на процессоре - ставим в False\n",
    "is_gpu = False\n",
    "#is_gpu = True\n",
    "\n",
    "# Ставим only_one_model = True, если обучается только дна модель без ансбамбля.\n",
    "# only_one_model = False\n",
    "only_one_model = True\n",
    "\n",
    "# Раз во сколько циклов сабмита (дней) учить модели заново с учетом новый данных, добавленных к старым\n",
    "# Скажем если учить заново каждые 30 циклов (после каждых 30 предсказанных дней) то ставим 30\n",
    "learn_again_period = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonthlyKFold:\n",
    "    def __init__(self, n_splits=3):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X, y, groups=None):\n",
    "        dates = 12 * X[\"year\"] + X[\"month\"]\n",
    "        timesteps = sorted(dates.unique().tolist())\n",
    "        X = X.reset_index()\n",
    "        \n",
    "        for t in timesteps[-self.n_splits:]:\n",
    "            idx_train = X[dates.values < t].index\n",
    "            idx_test = X[dates.values == t].index\n",
    "            \n",
    "            yield idx_train, idx_test\n",
    "            \n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target):\n",
    "    df_data = (\n",
    "        df_data\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_client = (\n",
    "        df_client\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_gas = (\n",
    "        df_gas\n",
    "        .rename({\"forecast_date\": \"date\"})\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_electricity = (\n",
    "        df_electricity\n",
    "        .rename({\"forecast_date\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\") + pl.duration(days=1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_location = (\n",
    "        df_location\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_forecast = (\n",
    "        df_forecast\n",
    "        .rename({\"forecast_datetime\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            #pl.col('datetime').dt.convert_time_zone(\"Europe/Bucharest\").dt.replace_time_zone(None).cast(pl.Datetime(\"us\")),\n",
    "            pl.col('datetime').dt.replace_time_zone(None).cast(pl.Datetime(\"us\"))\n",
    "            #pl.col('datetime').cast(pl.Datetime)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_historical = (\n",
    "        df_historical\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"datetime\") + pl.duration(hours=37)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_date = (\n",
    "        df_forecast\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_local = (\n",
    "        df_forecast\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    df_historical_date = (\n",
    "        df_historical\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_historical_local = (\n",
    "        df_historical\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    # Объединение всех обработанных данных с основным датафреймом df_data\n",
    "    df_data = (\n",
    "        df_data\n",
    "        .join(df_gas, on=\"date\", how=\"left\")\n",
    "        .join(df_client, on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n",
    "        .join(df_electricity, on=\"datetime\", how=\"left\")\n",
    "        \n",
    "        .join(df_forecast_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n",
    "        .join(df_forecast_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n",
    "        .join(df_historical_date, on=\"datetime\", how=\"left\", suffix=\"_hd\")\n",
    "        .join(df_historical_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl\")\n",
    "        \n",
    "        .join(df_forecast_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fdw\")\n",
    "        .join(df_forecast_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_flw\")\n",
    "        .join(df_historical_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hdw\")\n",
    "        .join(df_historical_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hlw\")\n",
    "        \n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # Создание категориальных признаков и тригонометрических функций времени\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"), # Добавление номера дня в году\n",
    "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),# Добавление часа\n",
    "            pl.col(\"datetime\").dt.day().alias(\"day\"),# Добавление дня\n",
    "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),# Добавление дня недели\n",
    "            pl.col(\"datetime\").dt.month().alias(\"month\"),# Добавление месяца\n",
    "            pl.col(\"datetime\").dt.year().alias(\"year\"),# Добавление года\n",
    "        )\n",
    "        # Приведение типов данных\n",
    "        .with_columns(\n",
    "            pl.concat_str(\"county\", \"is_business\", \"product_type\", \"is_consumption\", separator=\"_\").alias(\"category_1\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"), # Тригонометрические функции для дня в году\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).cast(pl.Float32),\n",
    "        )\n",
    "         # Удаление ненужных колонок\n",
    "        .drop(\"date\", \"datetime\", \"hour\", \"dayofyear\")\n",
    "    )\n",
    "    \n",
    "    # return df_data, df_historical_local\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(X, y=None):\n",
    "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
    "    \n",
    "    if y is not None:\n",
    "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
    "    else:\n",
    "        df = X.to_pandas()    \n",
    "    \n",
    "    df = df.set_index(\"row_id\")\n",
    "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "    \n",
    "    df[\"target_mean\"] = df[[f\"target_{i}\" for i in range(1, 7)]].mean(1)\n",
    "    df[\"target_std\"] = df[[f\"target_{i}\" for i in range(1, 7)]].std(1)\n",
    "    df[\"target_ratio\"] = df[\"target_6\"] / (df[\"target_7\"] + 1e-3)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для оптуны\n",
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'n_iter'           : 1000,\n",
    "        'verbose'          : -1,\n",
    "        'random_state'     : 42,\n",
    "        'objective'        : 'l2',\n",
    "        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
    "        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n",
    "        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n",
    "        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 256),\n",
    "        'max_depth'        : trial.suggest_int('max_depth', 5, 10),\n",
    "        'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n",
    "    }\n",
    "    \n",
    "    model  = lgb.LGBMRegressor(**params)\n",
    "    X, y   = df_train.drop(columns=[\"target\"]), df_train[\"target\"]\n",
    "    cv     = MonthlyKFold(1)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    return -1 * np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "# Для локальных вычислений. Последний data_block_id тренировочной выборки\n",
    "# А начиная со следующего data_block_id и до конца идет тест\n",
    "# train_end_data_block_id = 500\n",
    "train_end_data_block_id = 600\n",
    "\n",
    "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id', 'data_block_id']\n",
    "# В df_data_cols колонки в таком порядке в каком они потом формируются в df_data\n",
    "df_data_cols     = ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'datetime', 'data_block_id', 'row_id']\n",
    "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
    "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
    "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
    "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
    "location_cols    = ['longitude', 'latitude', 'county']\n",
    "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
    "\n",
    "save_path = None\n",
    "load_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Загрузка данных об энергопотреблении\n",
    "train = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "\n",
    "# Создание сводной таблицы с средними значениями целевой переменной (target)\n",
    "# для каждой комбинации даты, округа, типа продукта, бизнеса и потребления\n",
    "pivot_train = train.pivot_table(\n",
    "    index='datetime',\n",
    "    columns=['county', 'product_type', 'is_business', 'is_consumption'],\n",
    "    values='target',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Переименование колонок для удобства доступа и интерпретации\n",
    "pivot_train.columns = ['county{}_productType{}_isBusiness{}_isConsumption{}'.format(*col) for col in pivot_train.columns.values]\n",
    "pivot_train.index = pd.to_datetime(pivot_train.index)\n",
    "\n",
    "pivot_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2023 год "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Копирование сводной таблицы для визуализации\n",
    "df_plot = pivot_train.copy()\n",
    "\n",
    "# Нормализация данных для визуализации\n",
    "df_plot = (df_plot - df_plot.min()) / (df_plot.max() - df_plot.min())\n",
    "\n",
    "# Ресемплирование данных по дням и вычисление средних значений\n",
    "df_plot_resampled_D = df_plot.resample('D').mean()\n",
    "\n",
    "# Визуализация нормализованных данных с прозрачностью (alpha=0.1)\n",
    "df_plot_resampled_D.loc['2022-7':].plot(alpha=0.1, color='green', figsize=(18, 6), legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Выбор колонок, соответствующих различным категориям потребления\n",
    "columns_consumption_0 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption0')]\n",
    "columns_consumption_1 = df_plot_resampled_D.columns[df_plot_resampled_D.columns.str.contains('isConsumption1')]\n",
    "\n",
    "# Создание фигуры для визуализации\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Создание пустых линий для легенды\n",
    "plt.plot([], color='red', label='is_Consumption = 1')  # Изменено на желтый цвет\n",
    "plt.plot([], color='black', label='is_Consumption = 0')   # Изменено на черный цвет\n",
    "\n",
    "# Отображение легенды\n",
    "plt.legend()\n",
    "\n",
    "# Визуализация данных для 'is_Consumption = 0' черным цветом\n",
    "for column in columns_consumption_0:\n",
    "    df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='black', legend=False)  # Изменено на черный\n",
    "\n",
    "# Визуализация данных для 'is_Consumption = 1' желтым цветом\n",
    "for column in columns_consumption_1:\n",
    "    df_plot_resampled_D.loc['2022-7':, column].plot(alpha=0.1, color='red', legend=False)  # Изменено на желтый\n",
    "\n",
    "# Отображение графика\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись тестовых и тренировочных csv файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Если выполняем локально\n",
    "    train_path = 'train'\n",
    "    if not os.path.exists(train_path):\n",
    "        # Создание каталога, если его нет\n",
    "        os.makedirs(train_path)\n",
    "    test_path = 'example_test_files'\n",
    "    if not os.path.exists(test_path):\n",
    "        # Создание каталога, если его нет\n",
    "        os.makedirs(test_path)\n",
    "else:\n",
    "    # Если сабмит\n",
    "    train_path = root\n",
    "# Путь, куда запишем csv файлы для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяет датафрейм на тренировочную и тестовую часть\n",
    "# Возвращает часть датафрейма для тренировки, тестовую часть датафрейма записывает в каталог с тестами\n",
    "def split_train_test(filename):\n",
    "    df = pd.read_csv(os.path.join(root, filename))\n",
    "    \n",
    "    #Запишем часть данных для теста\n",
    "    test_df = df[df[\"data_block_id\"] > train_end_data_block_id]\n",
    "    if (filename ==\"train.csv\"):\n",
    "        # Берем только те ячейки где target был не нулевым\n",
    "        test_df = test_df[test_df[\"target\"].notnull()]\n",
    "        \n",
    "    test_df.to_csv(os.path.join(test_path, filename), index=False)\n",
    "\n",
    "    #Запишем часть данных для трейна\n",
    "    train_df = df[df[\"data_block_id\"] <= train_end_data_block_id]\n",
    "    train_df.to_csv(os.path.join(train_path, filename), index=False)\n",
    "\n",
    "# Доводим до ума тестовые таблицы чтобы они были точно такие как в реальном сабмите\n",
    "def test_dfs_tune():\n",
    "    # Делаем таблицу revealed_targets.csv\n",
    "    df = pd.read_csv(os.path.join(test_path, \"train.csv\"))\n",
    "    df.to_csv(os.path.join(test_path, 'revealed_targets.csv'), index=False)\n",
    "    \n",
    "    # Делаем таблицу test.csv\n",
    "    df.rename(columns={'datetime': 'prediction_datetime'}, inplace=True)\n",
    "    df.drop('target', axis=1, inplace=True)\n",
    "    df['currently_scored'] = False\n",
    "    df.to_csv(os.path.join(test_path, 'test.csv'), index=False)\n",
    "    \n",
    "    # Делаем таблицу sample_submission.csv\n",
    "    selected_columns = ['row_id', 'data_block_id']\n",
    "    df = df[selected_columns]\n",
    "    df['target'] = 0\n",
    "    df.to_csv(os.path.join(test_path, 'sample_submission.csv'), index=False)\n",
    "\n",
    "# Сборка разделения файлов\n",
    "def make_split():\n",
    "    # csv файлы которые будем делить:\n",
    "    csv_names = [\"train.csv\", \"client.csv\", \"gas_prices.csv\", \"electricity_prices.csv\", \"forecast_weather.csv\", \"historical_weather.csv\"]\n",
    "    for csv_name in csv_names:\n",
    "        split_train_test(csv_name)\n",
    "    # Доделываем тестовые таблицы\n",
    "    test_dfs_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Создаем файлы csv c тренировочными и тестовыми таблицами\n",
    "if is_local:\n",
    "    # Пока отключил создание тестовых файлом. У меня локально они есть\n",
    "    make_split()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data        = pl.read_csv(os.path.join(train_path, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
    "df_client      = pl.read_csv(os.path.join(train_path, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
    "df_gas         = pl.read_csv(os.path.join(train_path, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
    "df_electricity = pl.read_csv(os.path.join(train_path, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
    "df_forecast    = pl.read_csv(os.path.join(train_path, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
    "df_historical  = pl.read_csv(os.path.join(train_path, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
    "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
    "#df_location    = pl.read_csv('/kaggle/input/locations/county_lon_lats.csv', columns=location_cols, try_parse_dates=True)\n",
    "df_target      = df_data.select(target_cols)\n",
    "\n",
    "schema_data        = df_data.schema\n",
    "schema_client      = df_client.schema\n",
    "schema_gas         = df_gas.schema\n",
    "schema_electricity = df_electricity.schema\n",
    "schema_forecast    = df_forecast.schema\n",
    "schema_historical  = df_historical.schema\n",
    "schema_target      = df_target.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "\n",
    "X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
    "\n",
    "df_train = to_pandas(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(df_train.columns.to_list())\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#df_train.loc[174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='minimize', study_name='Regressor')\n",
    "# study.optimize(lgb_objective, n_trials=100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''result = cross_validate(\n",
    "    estimator=lgb.LGBMRegressor(**best_params, random_state=42),\n",
    "    X=df_train.drop(columns=[\"target\"]), \n",
    "    y=df_train[\"target\"],\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=MonthlyKFold(1),\n",
    ")\n",
    "\n",
    "print(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\n",
    "print(f\"Score Time(s): {result['score_time'].mean():.3f}\")\n",
    "print(f\"Error(MAE): {-result['test_score'].mean():.3f}\")'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Параметры для LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LGB\n",
    "if is_gpu:\n",
    "    # max_bin пришлось уменьшить для GPU ниже 250. Вообщедля GPU рекомендуют 63.\n",
    "    # Не факт что этот новый max_bin хорошо сочетается с другими параметрами\n",
    "    p1={'device_type': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06258413085998576, 'colsample_bytree': 0.6527661140701613, 'colsample_bynode': 0.8106858631408332, 'lambda_l1': 5.065645378814257, 'lambda_l2': 9.81159370218779, 'min_data_in_leaf': 192, 'max_depth': 10, 'max_bin': 250}\n",
    "    p2={'device_type': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.0632167263149817, 'colsample_bytree': 0.6958033941948067, 'colsample_bynode': 0.6030801666196094, 'lambda_l1': 7.137580620471935, 'lambda_l2': 9.348169401713742, 'min_data_in_leaf': 74, 'max_depth': 11, 'max_bin': 220}\n",
    "    p3={'device_type': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.061236402165228264, 'colsample_bytree': 0.81427095118471, 'colsample_bynode': 0.6097376843527067, 'lambda_l1': 6.360490880385201, 'lambda_l2': 9.954136008333839, 'min_data_in_leaf': 238, 'max_depth': 13, 'max_bin': 180}\n",
    "    p4={'device_type': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06753282378023663, 'colsample_bytree': 0.7508715107428325, 'colsample_bynode': 0.6831819500325418, 'lambda_l1': 8.679353563755722, 'lambda_l2': 6.105008696961338, 'min_data_in_leaf': 198, 'max_depth': 15, 'max_bin': 190}\n",
    "    p5={'device_type': 'gpu', 'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.05129380742257108, 'colsample_bytree': 0.5101576947777211, 'colsample_bynode': 0.8052639518604396, 'lambda_l1': 8.087311995794915, 'lambda_l2': 5.067361158677095, 'min_data_in_leaf': 222, 'max_depth': 8, 'max_bin': 45}\n",
    "    p6={'device_type': 'gpu', 'n_estimators': 900,'verbose': -1,'objective': 'l2','learning_rate': 0.05689066836106983,'colsample_bytree': 0.8915976762048253,'colsample_bynode': 0.5942203285139224,'lambda_l1': 3.6277555139102864,'lambda_l2': 1.6591278779517808,'min_data_in_leaf' : 186,'max_depth': 9,'max_bin': 100,}\n",
    "    p_day={'device_type': 'gpu', 'learning_rate': 0.050239193018201116, 'colsample_bytree': 0.7523230869476827, 'colsample_bynode': 0.8016401710184272, 'lambda_l1': 0.804941519994492, 'lambda_l2': 5.420391522845777, 'min_data_in_leaf': 53, 'max_depth': 15, 'max_bin': 250, 'n_estimators': 1367, 'num_leaves': 75, 'feature_fraction': 0.7660656830160648, 'bagging_fraction': 0.8829219702163389, 'bagging_freq': 1}\n",
    "else:\n",
    "    p1={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06258413085998576, 'colsample_bytree': 0.6527661140701613, 'colsample_bynode': 0.8106858631408332, 'lambda_l1': 5.065645378814257, 'lambda_l2': 9.81159370218779, 'min_data_in_leaf': 192, 'max_depth': 10, 'max_bin': 1800}\n",
    "    p2={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.0632167263149817, 'colsample_bytree': 0.6958033941948067, 'colsample_bynode': 0.6030801666196094, 'lambda_l1': 7.137580620471935, 'lambda_l2': 9.348169401713742, 'min_data_in_leaf': 74, 'max_depth': 11, 'max_bin': 530}\n",
    "    p3={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.061236402165228264, 'colsample_bytree': 0.81427095118471, 'colsample_bynode': 0.6097376843527067, 'lambda_l1': 6.360490880385201, 'lambda_l2': 9.954136008333839, 'min_data_in_leaf': 238, 'max_depth': 13, 'max_bin': 649}\n",
    "    p4={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.06753282378023663, 'colsample_bytree': 0.7508715107428325, 'colsample_bynode': 0.6831819500325418, 'lambda_l1': 8.679353563755722, 'lambda_l2': 6.105008696961338, 'min_data_in_leaf': 198, 'max_depth': 15, 'max_bin': 835}\n",
    "    p5={'n_estimators': 1000,'verbose': -1,'objective': 'l2','learning_rate': 0.05129380742257108, 'colsample_bytree': 0.5101576947777211, 'colsample_bynode': 0.8052639518604396, 'lambda_l1': 8.087311995794915, 'lambda_l2': 5.067361158677095, 'min_data_in_leaf': 222, 'max_depth': 8, 'max_bin': 97}\n",
    "    p6={'n_estimators': 900,'verbose': -1,'objective': 'l2','learning_rate': 0.05689066836106983,'colsample_bytree': 0.8915976762048253,'colsample_bynode': 0.5942203285139224,'lambda_l1': 3.6277555139102864,'lambda_l2': 1.6591278779517808,'min_data_in_leaf' : 186,'max_depth': 9,'max_bin': 813,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Параметры для catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_gpu:\n",
    "    # Параметры для catboost c GPU\n",
    "    c1 = {\n",
    "        'learning_rate': 0.06258413085998576,\n",
    "        'depth': 10,\n",
    "        'l2_leaf_reg': 8,\n",
    "        'border_count': 211,\n",
    "        'random_strength': 6,\n",
    "        'bagging_temperature': 0.13029094645654574,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1'],\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    c2 = {\n",
    "        'learning_rate': 0.08766355644863072,\n",
    "        'depth': 10,\n",
    "        'l2_leaf_reg': 8,\n",
    "        'border_count': 231,\n",
    "        'random_strength': 5,\n",
    "        'bagging_temperature': 0.20294125980762928,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1'],\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    c3 = {\n",
    "        'learning_rate': 0.14331003896351277,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 10,\n",
    "        'border_count': 162,\n",
    "        'random_strength': 2,\n",
    "        'bagging_temperature': 0.3016878017499466,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1'],\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    c4 = {\n",
    "        'learning_rate': 0.13382144579754543,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 10,\n",
    "        'border_count': 179,\n",
    "        'random_strength': 5,\n",
    "        'bagging_temperature': 0.30199258643156335,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1'],\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "\n",
    "    c5 = {\n",
    "        'learning_rate': 0.12358952478027072,\n",
    "        'depth': 11,\n",
    "        'l2_leaf_reg': 8,\n",
    "        'border_count': 191,\n",
    "        'random_strength': 3,\n",
    "        'bagging_temperature': 0.41774414265586035,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1'],\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    \n",
    "    c6 = {\n",
    "        'learning_rate': 0.13767418996955944,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 10,\n",
    "        'border_count': 182,\n",
    "        'random_strength': 3,\n",
    "        'bagging_temperature': 0.42796507669281386,\n",
    "        'iterations': 500,\n",
    "        'eval_metric': 'RMSE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1'],\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "else:\n",
    "    # Параметры для catboost без GPU\n",
    "    c1 = {\n",
    "        'learning_rate': 0.06258413085998576,\n",
    "        'depth': 10,\n",
    "        'l2_leaf_reg': 8,\n",
    "        'border_count': 211,\n",
    "        'random_strength': 6,\n",
    "        'bagging_temperature': 0.13029094645654574,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'MAE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n",
    "    }\n",
    "    \n",
    "    c2 = {\n",
    "        'learning_rate': 0.08766355644863072,\n",
    "        'depth': 10,\n",
    "        'l2_leaf_reg': 8,\n",
    "        'border_count': 231,\n",
    "        'random_strength': 5,\n",
    "        'bagging_temperature': 0.20294125980762928,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'MAE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n",
    "    }\n",
    "    \n",
    "    c3 = {\n",
    "        'learning_rate': 0.14331003896351277,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 10,\n",
    "        'border_count': 162,\n",
    "        'random_strength': 2,\n",
    "        'bagging_temperature': 0.3016878017499466,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'MAE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n",
    "    }\n",
    "    \n",
    "    c4 = {\n",
    "        'learning_rate': 0.13382144579754543,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 10,\n",
    "        'border_count': 179,\n",
    "        'random_strength': 5,\n",
    "        'bagging_temperature': 0.30199258643156335,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'MAE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n",
    "    }\n",
    "    \n",
    "    c5 = {\n",
    "        'learning_rate': 0.12358952478027072,\n",
    "        'depth': 11,\n",
    "        'l2_leaf_reg': 8,\n",
    "        'border_count': 191,\n",
    "        'random_strength': 3,\n",
    "        'bagging_temperature': 0.41774414265586035,\n",
    "        'iterations': 1500,\n",
    "        'eval_metric': 'MAE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n",
    "    }\n",
    "    \n",
    "    c6 = {\n",
    "        'learning_rate': 0.13767418996955944,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 10,\n",
    "        'border_count': 182,\n",
    "        'random_strength': 3,\n",
    "        'bagging_temperature': 0.42796507669281386,\n",
    "        'iterations': 500,\n",
    "        'eval_metric': 'MAE',\n",
    "        'cat_features': ['county', 'is_business', 'product_type', 'is_consumption', 'category_1']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Класс моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс обертка для моделей. Хранит различные пареметры для обучения моделей.\n",
    "# Например диапазоны данных на которых учить модель, как часто обучать заново и другие параметры\n",
    "class Models:\n",
    "    \n",
    "    # Инициализирует параметры обучения\n",
    "    # init_model - готовый объект модели для обучения\n",
    "    def __init__(self):\n",
    "        # Инициализируем словари\n",
    "        # Ключами во всех словарях будет имя модели\n",
    "        \n",
    "        # Словарь с моделями\n",
    "        self.models =  dict()\n",
    "        \n",
    "        # Словарь с описанием периодов обучения модели\n",
    "        # пока это матрица. из двух столбцов и двух строк в каждой строке описание периода\n",
    "        # первая колонка на сколько data_block_id в конеце обучени отстоит от доступного конца данных\n",
    "        # вторая колонка сколько data_block_id будет в периоде на котором обучаемся.\n",
    "        # data_block_id могут быть эквивалентны дням, но могут и отличаться, если данные будут подавать блоками не равными дням\n",
    "        # либо если будут разрывы в данных. Но они точно будут эквиваленты циклам предсказания\n",
    "        self.data_block_id_intervals  = dict()\n",
    "        \n",
    "        # Если 1, то модель предназначена для предсказания потребления электричества\n",
    "        # Если 0, то модель предназначена для предсказания производства электричества\n",
    "        self.is_consumption = dict()\n",
    "        \n",
    "        # Раз во сколько иттераций обучат модель\n",
    "        self.learn_again_period = dict()\n",
    "        \n",
    "        # Смещение для начала обучения модели. Добавляется к номеру итерации сабмита.\n",
    "        # Скажем если смещение 6 номер итерации 1, а обучаемся раз в чем итераций. То обучение будет в первуже итерацию сабмита.\n",
    "        self.learn_again_offset = dict()\n",
    "        \n",
    "        # Время в секундах сколько заняло последнее обучение модели\n",
    "        self.last_learn_time = dict()\n",
    "    \n",
    "    # Добавляет еще одну модель\n",
    "    # model_name - название модели\n",
    "    # new_model - объект модели\n",
    "    def add_model(self, model_name, new_model, is_consumption, data_block_id_intervals\n",
    "                  ,learn_again_period, learn_again_offset):\n",
    "        \n",
    "        self.models[model_name] = new_model\n",
    "        self.is_consumption[model_name] = is_consumption\n",
    "        self.data_block_id_intervals[model_name] = data_block_id_intervals\n",
    "        self.learn_again_period[model_name] = learn_again_period\n",
    "        self.learn_again_offset[model_name] = learn_again_offset\n",
    "        self.last_learn_time[model_name] = 0\n",
    "        \n",
    "    # Обучает модель\n",
    "    # model_name - название модели\n",
    "    # df_train - датафрейм который содержит данные для обучения и целевой признак\n",
    "    def fit_one_model(self, model_name, df_train):\n",
    "        print('fit model:', model_name)\n",
    "        self.models[model_name].fit(\n",
    "            X=df_train.drop(columns=[\"target\", \"data_block_id\"]),\n",
    "            y=df_train[\"target\"]\n",
    "        )\n",
    "    \n",
    "    # Делает предсказания от отдельной модели.\n",
    "    # model_name - название модели\n",
    "    # X - признаки на которых нужно сделать предсказание\n",
    "    def predict_one_model(self, model_name, X):\n",
    "        print('predict model:', model_name)\n",
    "        y = (self.models[model_name]\n",
    "             .predict(X.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "            )\n",
    "        return(y)\n",
    "    \n",
    "    # Обучает все добавленные модели для которых наступило время их обучения\n",
    "    # df_train - датафрейм который содержит данные для обучения и целевой признак\n",
    "    # itter_n - номер иттерации в сабмите.\n",
    "    # если itter_n равен -1. Значит первоначальное обучение и обучаем всем модели\n",
    "    def fit(self, df_train, itter_n):\n",
    "        max_block_id = df_train[\"data_block_id\"].max()\n",
    "        # Перебираем все модели\n",
    "        for m_name in self.learn_again_period:\n",
    "            # Либо сказали все модели учить\n",
    "            if ((itter_n == -1)\n",
    "                # Либо учим если номер итераиции в сабмите плюс смещение делится на цело на период обучения\n",
    "                or (((itter_n + self.learn_again_offset[m_name])\n",
    "                     % self.learn_again_period[m_name]) == 0)):\n",
    "                \n",
    "                print('fit model:', m_name)\n",
    "                d_i = self.data_block_id_intervals[m_name]\n",
    "                print(d_i[0][0], d_i[0][1], d_i[1][0], d_i[1][1])\n",
    "                \n",
    "                # Выделяем интервалы для обучения1\n",
    "                # df_train_int = df_train[df_train['is_consumption']==0]\n",
    "                df_train_int = df_train[\n",
    "                    # До какого data_block_id учим первый блок\n",
    "                    (((df_train['data_block_id']<=max_block_id-d_i[0][0])\n",
    "                     # C какого data_block_id учим первый блок\n",
    "                     &(df_train['data_block_id']>(max_block_id-d_i[0][0]-d_i[0][1])))\n",
    "                    # До какого data_block_id учим второй блок\n",
    "                    |((df_train['data_block_id']<=max_block_id-d_i[1][0])\n",
    "                      # С какого data_block_id учим второй блок\n",
    "                      &(df_train['data_block_id']>(max_block_id-d_i[1][0]-d_i[1][1]))))\n",
    "                &df_train[df_train['is_consumption']==self.is_consumption[model_name]]\n",
    "                ]\n",
    "                \n",
    "                print(df_train_int['data_block_id'].unique())\n",
    "    \n",
    "    # Делает предсказание всеми добавленными моделями и сводит их в одно предсказание\n",
    "    def predict(self, df_train):\n",
    "        pass\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
    "df_train = to_pandas(X, y)\n",
    "df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df_train['data_block_id'].unique())\n",
    "# df_train['data_block_id'].unique()\n",
    "df_train['is_consumption'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.add_model(\n",
    "    model_name = 'first',\n",
    "    new_model = lgb.LGBMRegressor(**p5, verbosity=-1, random_state=42),\n",
    "    is_consumption = 0,\n",
    "    data_block_id_intervals = [[0,10],[30,20]],\n",
    "    learn_again_period = 7,\n",
    "    learn_again_offset = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.fit(df_train=df_train, itter_n=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# models.fit_one_model(model_name='first', df_train=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if only_one_model:\n",
    "    # Если только одна модель\n",
    "    model = lgb.LGBMRegressor(**p5, verbosity=-1, random_state=42)\n",
    "    model.fit(\n",
    "        X=df_train[df_train['is_consumption']==1].drop(columns=[\"target\", \"data_block_id\"]),\n",
    "        y=df_train[df_train['is_consumption']==1][\"target\"]\n",
    "    )\n",
    "else:\n",
    "    model = VotingRegressor([\n",
    "        ('lgb_1', lgb.LGBMRegressor(**p1, random_state=42)), \n",
    "        ('lgb_2', lgb.LGBMRegressor(**p2, random_state=42)), \n",
    "        ('lgb_3', lgb.LGBMRegressor(**p3, random_state=42)), \n",
    "        ('lgb_4', lgb.LGBMRegressor(**p4, random_state=42)), \n",
    "        ('lgb_5', lgb.LGBMRegressor(**p5, random_state=42)), \n",
    "    ])\n",
    "    \n",
    "    model.fit(\n",
    "        X=df_train[df_train['is_consumption']==1].drop(columns=[\"target\", \"data_block_id\"]),\n",
    "        y=df_train[df_train['is_consumption']==1][\"target\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not(only_one_model):\n",
    "    # Если только одна модель\n",
    "    model_solar = VotingRegressor([\n",
    "        ('catboost_1', cb.CatBoostRegressor(**c1, verbose=False, random_state=42)),\n",
    "        ('catboost_2', cb.CatBoostRegressor(**c2, verbose=False, random_state=42)),\n",
    "        ('catboost_3', cb.CatBoostRegressor(**c3, verbose=False, random_state=42)),\n",
    "        ('catboost_4', cb.CatBoostRegressor(**c4, verbose=False, random_state=42)),\n",
    "        ('catboost_5', cb.CatBoostRegressor(**c5, verbose=False, random_state=42))\n",
    "    ])\n",
    "    model_solar.fit(\n",
    "        X=df_train[df_train['is_consumption']==0].drop(columns=[\"target\", \"data_block_id\"]),\n",
    "        y=df_train[df_train['is_consumption']==0][\"target\"]\n",
    "    )\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if is_local:\n",
    "#    dump(model_solar, 'model_solar.joblib')\n",
    "#    dump(model, 'model_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расскоментировать при необходимости загрузку ранее сохраненных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для загрузки локально\n",
    "#model_solar = load('model_solar.joblib')\n",
    "#model = load('model_lgbm.joblib')\n",
    "\n",
    "# Для загрузки на kaggle\n",
    "#model_solar = load('/kaggle/input/enefit/model_solar.joblib')\n",
    "#model = load('/kaggle/input/enefit/model_lgbm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Если выполняем локально, а не сабмитим на кагл,\n",
    "    # то выбираем другое имя для файла submission.csv.\n",
    "    # Потому что в submission.csv записать прав нет и вылетает по ошибке\n",
    "    submission_name = 'submission_loc.csv'\n",
    "else:\n",
    "    submission_name = 'submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Содержимое public_timeseries_testing_util.py\n",
    "\n",
    "С необходимыми праками. Решил не импортировать его. а прямо тут. Так удобнее переносить на kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An unlocked version of the timeseries API intended for testing alternate inputs.\n",
    "Mirrors the production timeseries API in the crucial respects, but won't be as fast.\n",
    "\n",
    "ONLY works afer the first three variables in MockAPI.__init__ are populated.\n",
    "'''\n",
    "\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "\n",
    "class MockApi:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n",
    "        They've been intentionally left in an invalid state.\n",
    "\n",
    "        Variables to set:\n",
    "            input_paths: a list of two or more paths to the csv files to be served\n",
    "            group_id_column: the column that identifies which groups of rows the API should serve.\n",
    "                A call to iter_test serves all rows of all dataframes with the current group ID value.\n",
    "            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n",
    "        '''\n",
    "        self.input_paths: Sequence[str] = ['example_test_files/test.csv',\n",
    "                                   'example_test_files/revealed_targets.csv', \n",
    "                                   'example_test_files/client.csv',\n",
    "                                   'example_test_files/historical_weather.csv',\n",
    "                                   'example_test_files/forecast_weather.csv',\n",
    "                                   'example_test_files/electricity_prices.csv',\n",
    "                                   'example_test_files/gas_prices.csv',\n",
    "                                   'example_test_files/sample_submission.csv']\n",
    "        self.group_id_column: str = 'data_block_id'\n",
    "        self.export_group_id_column: bool = False\n",
    "        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n",
    "        assert len(self.input_paths) >= 2\n",
    "\n",
    "        self._status = 'initialized'\n",
    "        self.predictions = []\n",
    "\n",
    "    def iter_test(self) -> Tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Loads all of the dataframes specified in self.input_paths,\n",
    "        then yields all rows in those dataframes that equal the current self.group_id_column value.\n",
    "        '''\n",
    "        if self._status != 'initialized':\n",
    "\n",
    "            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n",
    "\n",
    "        dataframes = []\n",
    "        for pth in self.input_paths:\n",
    "            dataframes.append(pd.read_csv(pth, low_memory=False))\n",
    "        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n",
    "        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n",
    "\n",
    "        for group_id in group_order:\n",
    "            self._status = 'prediction_needed'\n",
    "            current_data = []\n",
    "            for df in dataframes:\n",
    "                cur_df = df.loc[group_id].copy()\n",
    "                # returning single line dataframes from df.loc requires special handling\n",
    "                if not isinstance(cur_df, pd.DataFrame):\n",
    "                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n",
    "                    cur_df.index.name = self.group_id_column\n",
    "                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n",
    "                current_data.append(cur_df)\n",
    "            yield tuple(current_data)\n",
    "\n",
    "            while self._status != 'prediction_received':\n",
    "                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n",
    "                yield None\n",
    "\n",
    "        with open(submission_name, 'w') as f_open:\n",
    "            pd.concat(self.predictions).to_csv(f_open, index=False)\n",
    "        self._status = 'finished'\n",
    "\n",
    "    def predict(self, user_predictions: pd.DataFrame):\n",
    "        '''\n",
    "        Accepts and stores the user's predictions and unlocks iter_test once that is done\n",
    "        '''\n",
    "        if self._status == 'finished':\n",
    "            raise Exception('You have already made predictions for the full test set.')\n",
    "        if self._status != 'prediction_needed':\n",
    "            raise Exception('You must get the next test sample from `iter_test()` first.')\n",
    "        if not isinstance(user_predictions, pd.DataFrame):\n",
    "            raise Exception('You must provide a DataFrame.')\n",
    "\n",
    "        self.predictions.append(user_predictions)\n",
    "        self._status = 'prediction_received'\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return MockApi()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация иттераций сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # После этого можно имитировать локально загрузку при собмите на большом числе итераций\n",
    "    # А не только четыре иттерации на 4 дня как в стандартной имитайии на кагле\n",
    "    env = make_env()\n",
    "else:\n",
    "    # загружаем оригинальную библиотеку для сабмита\n",
    "    import enefit\n",
    "    env = enefit.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренирует модели заново\n",
    "def train_models():\n",
    "    # Подготавливаем данные\n",
    "    global  X, y, df_client, df_gas, df_electricity, df_forecast,\\\n",
    "            df_historical, df_location, df_target, df_train, model, model_solar\n",
    "\n",
    "    print('Начали обучение моделей')\n",
    "    X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "    X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
    "    df_train = to_pandas(X, y)\n",
    "    df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]\n",
    "\n",
    "    # Обучаем модели\n",
    "    print('LGB')\n",
    "    model.fit(\n",
    "        X=df_train[df_train['is_consumption']==1].drop(columns=[\"target\", \"data_block_id\"]),\n",
    "        y=df_train[df_train['is_consumption']==1][\"target\"]\n",
    "    )\n",
    "    if not(only_one_model):\n",
    "        print('Catboost')\n",
    "        model_solar.fit(\n",
    "            X=df_train[df_train['is_consumption']==0].drop(columns=[\"target\", \"data_block_id\"]),\n",
    "            y=df_train[df_train['is_consumption']==0][\"target\"]\n",
    "        )\n",
    "    print('Завершили обучение моделей')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим последний data_block_id в обучающих данных\n",
    "max_train_data_block_id = df_data[\"data_block_id\"].max()\n",
    "# Устанавливаем первый data_block_id для теста следущим за тренировочным\n",
    "cur_test_data_block_id = max_train_data_block_id + 1\n",
    "cur_test_data_block_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 0\n",
    "\n",
    "# Основной цикл для обработки данных тестового набора\n",
    "for (test, revealed_targets, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
    "    # Переименование столбца для удобства\n",
    "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "    if is_local:\n",
    "        # Если выполняем локально, то преобразуем некоторые типы данных\n",
    "        # На кагле (а может и в линуксе) они и так преобразуются, но на виновс локально\n",
    "        # не преобразуются и выдетают по ощибке\n",
    "        test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "        client['date'] = pd.to_datetime(client['date'])\n",
    "        gas_prices['origin_date'] = pd.to_datetime(gas_prices['origin_date'])\n",
    "        gas_prices['forecast_date'] = pd.to_datetime(gas_prices['forecast_date'])\n",
    "        electricity_prices['origin_date'] = pd.to_datetime(electricity_prices['origin_date'])\n",
    "        electricity_prices['forecast_date'] = pd.to_datetime(electricity_prices['forecast_date'])\n",
    "        forecast_weather['origin_datetime'] = pd.to_datetime(forecast_weather['origin_datetime'])\n",
    "        forecast_weather['forecast_datetime'] = pd.to_datetime(forecast_weather['forecast_datetime'])\n",
    "        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n",
    "        revealed_targets['datetime'] = pd.to_datetime(revealed_targets['datetime'])\n",
    "        \n",
    "    # Добавляем колонку заполненную следующим data_block_id\n",
    "    test[\"data_block_id\"] = cur_test_data_block_id\n",
    "    revealed_targets[\"data_block_id\"] = cur_test_data_block_id\n",
    "    \n",
    "    df_test            = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
    "    df_new_client      = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
    "    df_new_gas         = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
    "    df_new_electricity = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
    "    df_new_forecast    = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
    "    df_new_historical  = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
    "    df_new_target      = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
    "    df_new_data        = pl.from_pandas(revealed_targets[df_data_cols], schema_overrides=schema_data)\n",
    "    # Объединение новых данных с существующими и удаление дубликатов\n",
    "    df_client          = pl.concat([df_client, df_new_client]).unique(subset=[\"county\", \"is_business\", \"product_type\", \"date\"], maintain_order=True)\n",
    "    df_gas             = pl.concat([df_gas, df_new_gas]).unique(subset=[\"forecast_date\"], maintain_order=True)\n",
    "    df_electricity     = pl.concat([df_electricity, df_new_electricity]).unique(subset=[\"forecast_date\"], maintain_order=True)\n",
    "    df_forecast        = pl.concat([df_forecast, df_new_forecast]).unique()\n",
    "    df_historical      = pl.concat([df_historical, df_new_historical]).unique()\n",
    "    df_target          = pl.concat([df_target, df_new_target]).unique()\n",
    "    df_data            = pl.concat([df_data, df_new_data]).unique()\n",
    "    \n",
    "    # Применение функции инженерии признаков и преобразование данных обратно в pandas\n",
    "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
    "    X_test = to_pandas(X_test)\n",
    "    \n",
    "    # Прогнозирование с использованием модели и ограничение предсказаний нулем\n",
    "    test['target'] = model.predict(X_test.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "    \n",
    "    if not(only_one_model):\n",
    "        test['target_solar'] = model_solar.predict(X_test.drop(columns=[\"data_block_id\"])).clip(0)\n",
    "        \n",
    "        # Замена прогнозов для непотребляющих клиентов на прогнозы солнечной энергии\n",
    "        test.loc[test['is_consumption']==0, \"target\"] = test.loc[test['is_consumption']==0, \"target_solar\"]    \n",
    "    \n",
    "    # Обновление целевых значений в примере предсказания\n",
    "    sample_prediction[\"target\"] = test['target']\n",
    "    \n",
    "    # Отправка предсказаний в среду выполнения\n",
    "    env.predict(sample_prediction)\n",
    "\n",
    "    count += 1\n",
    "    print(count)\n",
    "\n",
    "    if is_local:\n",
    "        if (((count % learn_again_period) == 0)\n",
    "            and (count>=100)):\n",
    "            # Учим модели заново, с учетом новых данных но только после\n",
    "            # итерации 100. Потому что интересует как ведет себя моделья через два месяца\n",
    "            # после обучения\n",
    "            train_models()\n",
    "    else:\n",
    "        # На реальном cабмите начинаем тренировать модели сразу.\n",
    "        # Кроме того, если работаем больше 8 часов прекращаем тренировать модели на реальном сабмите\n",
    "        # Потому что после 9 часов сабмит вылетит по таймауту\n",
    "        cur_time = time.time()\n",
    "        if ((cur_time - notebook_starttime) < (8*60*60)):\n",
    "            if ((count % learn_again_period) == 0):\n",
    "                train_models()\n",
    "        else:\n",
    "            print('Не тренеруем модель, превышено время выполнения ноутбука:', (cur_time - notebook_starttime))\n",
    "    \n",
    "    # Переходим к следующему data_block_id на итерации тестов\n",
    "    cur_test_data_block_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет скора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # Загружаем предсказания\n",
    "    submission = pd.read_csv(submission_name)\n",
    "    # Загружаем истинные значения\n",
    "    revealed_targets = pd.read_csv(os.path.join(test_path, \"revealed_targets.csv\"))\n",
    "\n",
    "    mae = mean_absolute_error(revealed_targets['target'] , submission['target'])\n",
    "    print(f'MAE: {mae}')\n",
    "    \n",
    "    # Подготовим данные для анализа изменения ошибки предсказания по мере удаления от времени завершения обучения\n",
    "    compare = pd.DataFrame(revealed_targets['data_block_id'])\n",
    "    compare['target'] = revealed_targets['target']\n",
    "    compare['predict'] = submission['target']\n",
    "    compare['abs_err'] = abs(compare['predict'] - compare['target'])\n",
    "    compare['err'] = compare['predict'] - compare['target']\n",
    "    \n",
    "    compare_600 = compare[compare['data_block_id'] > 600]\n",
    "    # Выводим MAE для data_block_id > 600\n",
    "    mae_600 = mean_absolute_error(compare_600['target'], compare_600['predict'])\n",
    "    \n",
    "    print(f'MAE после 600: {mae_600}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### График MAE по дням предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводит график средних ошибок сгруппированных по дням (точнее для блоков данных для предсказаний которые в целом эквиваленты дням)\n",
    "def print_err(err_name, err_lable, err_title):\n",
    "    # Группируем по data_block_id, то есть по дням и считаем отдельно для каждого дня предсказания MAE\n",
    "    grouped_compare = compare.groupby('data_block_id').mean().reset_index()\n",
    "    # Делаем скользящую среднюю\n",
    "    grouped_compare['rolling_mean'] = grouped_compare[err_name].rolling(window=30, min_periods=1).mean()\n",
    "    \n",
    "    \n",
    "    # Plotting the mean absolute errors\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #plt.bar(grouped_compare['data_block_id'], grouped_compare['abs_err'])\n",
    "    plt.bar(grouped_compare['data_block_id'], grouped_compare[err_name], label=err_lable)\n",
    "    plt.plot(grouped_compare['data_block_id'],\n",
    "             grouped_compare['rolling_mean'],\n",
    "             label='Rolling Mean (window=30)',\n",
    "             color='orange',\n",
    "             linestyle='-', linewidth=2)\n",
    "    plt.xlabel('data_block_id')\n",
    "    plt.ylabel(err_lable)\n",
    "    plt.title(err_title)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set ticks every 10 data_block_id\n",
    "    tick_positions_y = np.arange(-40, max(grouped_compare[err_name]) + 1, 10)\n",
    "    plt.yticks(tick_positions_y)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    print_err(err_name='abs_err',\n",
    "              err_lable='Mean Absolute Error',\n",
    "              err_title='Mean Absolute Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = compare[compare[\"data_block_id\"] > 600]\n",
    "    print_err(err_name='err',\n",
    "              err_lable='Mean Error (predict - target)',\n",
    "              err_title='Mean Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
