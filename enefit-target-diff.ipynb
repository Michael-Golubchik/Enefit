{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holiday features from** [Enefit Estonian Holidays LB=65.79](https://www.kaggle.com/code/albansteff/enefit-estonian-holidays-lb-65-79)üôè\n",
    "\n",
    "\n",
    "### What's new?\n",
    "The main idea of this notebook is to predict the difference between the last available `target_48h` and the `target` inplace of directly predictiting the `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ —Ä–∞–±–æ—Ç—ã –Ω–æ—É—Ç–±—É–∫–∞\n",
    "notebook_starttime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—É—Ç–±—É–∫\n",
    "def p_time():\n",
    "    #\n",
    "    run_time = round(time.time() - notebook_starttime)\n",
    "    return str(run_time).zfill(5)+' sec:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°—Ç–∞–≤–∏–º is_local –≤ True, –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ–º, –µ—Å–ª–∏ —Å–∞–±–º–∏—Ç–∏–º - —Å—Ç–∞–≤–∏–º –≤ False\n",
    "#is_local = False\n",
    "is_local = True\n",
    "\n",
    "# –°—Ç–∞–≤–∏–º is_gpu –≤ True, –µ—Å–ª–∏ –±—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ GPU, –µ—Å–ª–∏ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ - —Å—Ç–∞–≤–∏–º –≤ False\n",
    "# is_gpu = False\n",
    "is_gpu = True\n",
    "\n",
    "# –°—Ç–∞–≤–∏–º is_tuning –≤ True, –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ Optuna\n",
    "is_tuning = False\n",
    "\n",
    "# –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "training_start_date = 'datetime >= \"2022-01-01 00:00:00\"'\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –ù–∞—á–∏–Ω–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –∑–∞–Ω–æ–≤–æ,\n",
    "# –∫–æ–≥–¥–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–∞—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∏–¥–µ—Ç –≤ —Å–∫–æ—Ä.\n",
    "scor_start_time = pd.to_datetime('2023-06-01')\n",
    "\n",
    "# –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ü–æ—Å–ª–µ–¥–Ω–∏–π data_block_id —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "# –ê –Ω–∞—á–∏–Ω–∞—è —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ data_block_id –∏ –¥–æ –∫–æ–Ω—Ü–∞ –∏–¥–µ—Ç —Ç–µ—Å—Ç\n",
    "train_end_data_block_id = 500\n",
    "# train_end_data_block_id = 600\n",
    "\n",
    "# –°—Ç–∞–≤–∏–º –≤ False, –µ—Å–ª–∏ –Ω–µ —Ö–æ—Ç–∏—Ç–∏–º –æ—Ç–∫–ª—é—á–∞—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ—É—Ç–±—É–∫–∞\n",
    "# (–Ω–µ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –∑–∞–Ω–æ–≤–æ –ø–æ—Å–ª–µ 8 —á–∞—Å–æ–≤ 30 –º–∏–Ω—É—Ç)\n",
    "# –ï—Å–ª–∏ —Ö–æ—Ç–∏–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å –∑–∞–Ω–æ–≤–æ –∏ –ø–æ—Å–ª–µ –ª–∏–º–∏—Ç–∞, —Å—Ç–∞–≤–∏–º True\n",
    "is_disable_run_time_limit = True\n",
    "# is_disable_run_time_limit = False\n",
    "\n",
    "# –†–∞–∑ –≤–æ —Å–∫–æ–ª—å–∫–æ —Ü–∏–∫–ª–æ–≤ —Å–∞–±–º–∏—Ç–∞ (–¥–Ω–µ–π) —É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –∑–∞–Ω–æ–≤–æ —Å —É—á–µ—Ç–æ–º –Ω–æ–≤—ã–π –¥–∞–Ω–Ω—ã—Ö, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º\n",
    "# –°–∫–∞–∂–µ–º –µ—Å–ª–∏ —É—á–∏—Ç—å –∑–∞–Ω–æ–≤–æ –∫–∞–∂–¥—ã–µ 30 —Ü–∏–∫–ª–æ–≤ (–ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 30 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–Ω–µ–π) —Ç–æ —Å—Ç–∞–≤–∏–º 30\n",
    "learn_again_period = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–ø–∏—Å—å —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö csv —Ñ–∞–π–ª–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # –ï—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "    train_path = 'train'\n",
    "    if not os.path.exists(train_path):\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "        os.makedirs(train_path)\n",
    "    test_path = 'example_test_files'\n",
    "    if not os.path.exists(test_path):\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "        os.makedirs(test_path)\n",
    "else:\n",
    "    # –ï—Å–ª–∏ —Å–∞–±–º–∏—Ç\n",
    "    train_path = root\n",
    "# –ü—É—Ç—å, –∫—É–¥–∞ –∑–∞–ø–∏—à–µ–º csv —Ñ–∞–π–ª—ã –¥–ª—è —Ç–µ—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å\n",
    "# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏, —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ –∫–∞—Ç–∞–ª–æ–≥ —Å —Ç–µ—Å—Ç–∞–º–∏\n",
    "def split_train_test(filename):\n",
    "    df = pd.read_csv(os.path.join(root, filename))\n",
    "    \n",
    "    #–ó–∞–ø–∏—à–µ–º —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "    test_df = df[df[\"data_block_id\"] > train_end_data_block_id]\n",
    "    if (filename ==\"train.csv\"):\n",
    "        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —è—á–µ–π–∫–∏ –≥–¥–µ target –±—ã–ª –Ω–µ –Ω—É–ª–µ–≤—ã–º\n",
    "        test_df = test_df[test_df[\"target\"].notnull()]\n",
    "        \n",
    "    test_df.to_csv(os.path.join(test_path, filename), index=False)\n",
    "\n",
    "    #–ó–∞–ø–∏—à–µ–º —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–π–Ω–∞\n",
    "    train_df = df[df[\"data_block_id\"] <= train_end_data_block_id]\n",
    "    train_df.to_csv(os.path.join(train_path, filename), index=False)\n",
    "\n",
    "# –î–æ–≤–æ–¥–∏–º –¥–æ —É–º–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã —á—Ç–æ–±—ã –æ–Ω–∏ –±—ã–ª–∏ —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–µ –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å–∞–±–º–∏—Ç–µ\n",
    "def test_dfs_tune():\n",
    "    # –î–µ–ª–∞–µ–º —Ç–∞–±–ª–∏—Ü—É revealed_targets.csv\n",
    "    df = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "    df = df[df[\"data_block_id\"] > train_end_data_block_id - 2]\n",
    "    df[\"data_block_id\"] += 2\n",
    "    df = df[df[\"target\"].notnull()]\n",
    "    df.to_csv(os.path.join(test_path, 'revealed_targets.csv'), index=False)\n",
    "    \n",
    "    # –î–µ–ª–∞–µ–º —Ç–∞–±–ª–∏—Ü—É test.csv\n",
    "    df = pd.read_csv(os.path.join(test_path, \"train.csv\"))\n",
    "    df.rename(columns={'datetime': 'prediction_datetime'}, inplace=True)\n",
    "    df.drop('target', axis=1, inplace=True)\n",
    "    df['currently_scored'] = False\n",
    "    df.to_csv(os.path.join(test_path, 'test.csv'), index=False)\n",
    "    \n",
    "    # –î–µ–ª–∞–µ–º —Ç–∞–±–ª–∏—Ü—É sample_submission.csv\n",
    "    selected_columns = ['row_id', 'data_block_id']\n",
    "    df = df[selected_columns]\n",
    "    df['target'] = 0\n",
    "    df.to_csv(os.path.join(test_path, 'sample_submission.csv'), index=False)\n",
    "\n",
    "# –°–±–æ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n",
    "def make_split():\n",
    "    # csv —Ñ–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥–µ–º –¥–µ–ª–∏—Ç—å:\n",
    "    csv_names = [\"train.csv\", \"client.csv\", \"gas_prices.csv\", \"electricity_prices.csv\", \"forecast_weather.csv\", \"historical_weather.csv\"]\n",
    "    for csv_name in csv_names:\n",
    "        split_train_test(csv_name)\n",
    "    # –î–æ–¥–µ–ª—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã\n",
    "    test_dfs_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã csv c —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º–∏ –∏ —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏\n",
    "if is_local:\n",
    "    # –ü–æ–∫–∞ –æ—Ç–∫–ª—é—á–∏–ª —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–º. –£ –º–µ–Ω—è –ª–æ–∫–∞–ª—å–Ω–æ –æ–Ω–∏ –µ—Å—Ç—å\n",
    "    # make_split()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "if is_local:\n",
    "    # –ï—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "    train_path = 'train'\n",
    "    if not os.path.exists(train_path):\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "        os.makedirs(train_path)\n",
    "    test_path = 'example_test_files'\n",
    "    if not os.path.exists(test_path):\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç\n",
    "        os.makedirs(test_path)\n",
    "else:\n",
    "    # –ï—Å–ª–∏ —Å–∞–±–º–∏—Ç\n",
    "    train_path = root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "\n",
    "    data_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "        \"row_id\",\n",
    "    ]\n",
    "    \n",
    "    df_data_cols     = ['county', 'is_business', 'product_type', 'target', 'is_consumption', 'datetime', 'row_id']\n",
    "    \n",
    "    client_cols = [\n",
    "        \"product_type\",\n",
    "        \"county\",\n",
    "        \"eic_count\",\n",
    "        \"installed_capacity\",\n",
    "        \"is_business\",\n",
    "        \"date\",\n",
    "    ]\n",
    "    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n",
    "    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n",
    "    forecast_weather_cols = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"hours_ahead\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"cloudcover_high\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_total\",\n",
    "        \"10_metre_u_wind_component\",\n",
    "        \"10_metre_v_wind_component\",\n",
    "        \"forecast_datetime\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"surface_solar_radiation_downwards\",\n",
    "        \"snowfall\",\n",
    "        \"total_precipitation\",\n",
    "    ]\n",
    "    historical_weather_cols = [\n",
    "        \"datetime\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloudcover_total\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"windspeed_10m\",\n",
    "        \"winddirection_10m\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ]\n",
    "    location_cols = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(\n",
    "            os.path.join(train_path, \"train.csv\"),\n",
    "            columns=self.data_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_client = pl.read_csv(\n",
    "            os.path.join(train_path, \"client.csv\"),\n",
    "            columns=self.client_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_gas_prices = pl.read_csv(\n",
    "            os.path.join(train_path, \"gas_prices.csv\"),\n",
    "            columns=self.gas_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_electricity_prices = pl.read_csv(\n",
    "            os.path.join(train_path, \"electricity_prices.csv\"),\n",
    "            columns=self.electricity_prices_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_forecast_weather = pl.read_csv(\n",
    "            os.path.join(train_path, \"forecast_weather.csv\"),\n",
    "            columns=self.forecast_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_historical_weather = pl.read_csv(\n",
    "            os.path.join(train_path, \"historical_weather.csv\"),\n",
    "            columns=self.historical_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(\n",
    "            os.path.join(train_path, \"weather_station_to_county_mapping.csv\"),\n",
    "            columns=self.location_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_data = self.df_data.filter(\n",
    "            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n",
    "        )\n",
    "        self.df_target = self.df_data.select(self.target_cols)\n",
    "\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_gas_prices = self.df_gas_prices.schema\n",
    "        self.schema_electricity_prices = self.df_electricity_prices.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (\n",
    "            self.df_weather_station_to_county_mapping.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_with_new_data(\n",
    "        self,\n",
    "        df_new_client,\n",
    "        df_new_gas_prices,\n",
    "        df_new_electricity_prices,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target,\n",
    "    ):\n",
    "        df_new_client = pl.from_pandas(\n",
    "            df_new_client[self.client_cols], schema_overrides=self.schema_client\n",
    "        )\n",
    "        df_new_gas_prices = pl.from_pandas(\n",
    "            df_new_gas_prices[self.gas_prices_cols],\n",
    "            schema_overrides=self.schema_gas_prices,\n",
    "        )\n",
    "        df_new_electricity_prices = pl.from_pandas(\n",
    "            df_new_electricity_prices[self.electricity_prices_cols],\n",
    "            schema_overrides=self.schema_electricity_prices,\n",
    "        )\n",
    "        df_new_forecast_weather = pl.from_pandas(\n",
    "            df_new_forecast_weather[self.forecast_weather_cols],\n",
    "            schema_overrides=self.schema_forecast_weather,\n",
    "        )\n",
    "        df_new_historical_weather = pl.from_pandas(\n",
    "            df_new_historical_weather[self.historical_weather_cols],\n",
    "            schema_overrides=self.schema_historical_weather,\n",
    "        )\n",
    "\n",
    "        df_new_data = pl.from_pandas(df_new_target[self.df_data_cols], schema_overrides=self.schema_data)\n",
    "\n",
    "        df_new_target = pl.from_pandas(\n",
    "            df_new_target[self.target_cols], schema_overrides=self.schema_target\n",
    "        )\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n",
    "            [\"date\", \"county\", \"is_business\", \"product_type\"]\n",
    "        )\n",
    "        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n",
    "            [\"forecast_date\"]\n",
    "        )\n",
    "        self.df_electricity_prices = pl.concat(\n",
    "            [self.df_electricity_prices, df_new_electricity_prices]\n",
    "        ).unique([\"forecast_date\"])\n",
    "        self.df_forecast_weather = pl.concat(\n",
    "            [self.df_forecast_weather, df_new_forecast_weather]\n",
    "        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        self.df_historical_weather = pl.concat(\n",
    "            [self.df_historical_weather, df_new_historical_weather]\n",
    "        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n",
    "            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "        )\n",
    "        self.df_data = pl.concat([self.df_data, df_new_data]).unique(\n",
    "            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "        )\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(\n",
    "            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n",
    "        )\n",
    "        return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeaturesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesGenerator:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(\n",
    "            holidays.country_holidays(\"EE\", years=range(2021, 2026)).keys()\n",
    "        )\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def is_country_holiday(self, row):\n",
    "        return (\n",
    "            datetime.date(row[\"year\"], row[\"month\"], row[\"day\"])\n",
    "            in self.estonian_holidays\n",
    "        )\n",
    "\n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.struct([\"year\", \"month\", \"day\"])\n",
    "            .apply(self.is_country_holiday)\n",
    "            .alias(\"is_country_holiday\")\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "        ]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    \"datetime\",\n",
    "                ],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_nominator, lag_denomonator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model_parameters = {'device': 'gpu',\n",
    "                                 'n_estimators': 1656,\n",
    "                                 'verbose': -1,\n",
    "                                 'objective': 'l2', \n",
    "                                 'num_leaves': 41,\n",
    "                                 'learning_rate': 0.044516384496327305,\n",
    "                                 'colsample_bytree': 0.8375548988212456,\n",
    "                                 'colsample_bynode': 0.5667409811919698,\n",
    "                                 'reg_alpha': 3.1759770693392193,\n",
    "                                 'reg_lambda': 11.018602402122836,\n",
    "                                 'min_child_samples': 5,\n",
    "                                 'max_depth': -1,\n",
    "                                 'max_bin': 58\n",
    "                                }\n",
    "\n",
    "        self.model_consumption = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"consumption_lgb_{i}\",\n",
    "                    lgb.LGBMRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(42, 48)\n",
    "                # for i in range(42, 43)\n",
    "            ]\n",
    "        )\n",
    "        self.model_production = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"production_lgb_{i}\",\n",
    "                    lgb.LGBMRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(42, 48)\n",
    "                # for i in range(42, 43)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def fit(self, df_train_features):\n",
    "        mask = df_train_features[\"is_consumption\"] == 1\n",
    "        self.model_consumption.fit(\n",
    "            X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "            y=df_train_features[mask][\"target\"]\n",
    "            - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "        )\n",
    "\n",
    "        mask = df_train_features[\"is_consumption\"] == 0\n",
    "        self.model_production.fit(\n",
    "            X=df_train_features[mask].drop(columns=[\"target\"]),\n",
    "            y=df_train_features[mask][\"target\"]\n",
    "            - df_train_features[mask][\"target_48h\"].fillna(0),\n",
    "        )\n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = np.zeros(len(df_features))\n",
    "\n",
    "        mask = df_features[\"is_consumption\"] == 1\n",
    "        predictions[mask.values] = np.clip(\n",
    "            df_features[mask][\"target_48h\"].fillna(0).values\n",
    "            + self.model_consumption.predict(df_features[mask]),\n",
    "            0,\n",
    "            np.inf,\n",
    "        )\n",
    "\n",
    "        mask = df_features[\"is_consumption\"] == 0\n",
    "        predictions[mask.values] = np.clip(\n",
    "            df_features[mask][\"target_48h\"].fillna(0).values\n",
    "            + self.model_production.predict(df_features[mask]),\n",
    "            0,\n",
    "            np.inf,\n",
    "        )\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_storage = DataStorage()\n",
    "features_generator = FeaturesGenerator(data_storage=data_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "df_train_features = features_generator.generate_features(data_storage.df_data)\n",
    "df_train_features = df_train_features[df_train_features['target'].notnull()]\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Model()\n",
    "'''\n",
    "model.fit(df_train_features)\n",
    "\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # –ï—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ, –∞ –Ω–µ —Å–∞–±–º–∏—Ç–∏–º –Ω–∞ –∫–∞–≥–ª,\n",
    "    # —Ç–æ –≤—ã–±–∏—Ä–∞–µ–º –¥—Ä—É–≥–æ–µ –∏–º—è –¥–ª—è —Ñ–∞–π–ª–∞ submission.csv.\n",
    "    # –ü–æ—Ç–æ–º—É —á—Ç–æ –≤ submission.csv –∑–∞–ø–∏—Å–∞—Ç—å –ø—Ä–∞–≤ –Ω–µ—Ç –∏ –≤—ã–ª–µ—Ç–∞–µ—Ç –ø–æ –æ—à–∏–±–∫–µ\n",
    "    submission_name = 'submission_loc.csv'\n",
    "else:\n",
    "    submission_name = 'submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–¥–µ—Ä–∂–∏–º–æ–µ public_timeseries_testing_util.py\n",
    "\n",
    "–° –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø—Ä–∞–∫–∞–º–∏. –†–µ—à–∏–ª –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ. –∞ –ø—Ä—è–º–æ —Ç—É—Ç. –¢–∞–∫ —É–¥–æ–±–Ω–µ–µ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –Ω–∞ kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "An unlocked version of the timeseries API intended for testing alternate inputs.\n",
    "Mirrors the production timeseries API in the crucial respects, but won't be as fast.\n",
    "\n",
    "ONLY works afer the first three variables in MockAPI.__init__ are populated.\n",
    "'''\n",
    "\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "\n",
    "class MockApi:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n",
    "        They've been intentionally left in an invalid state.\n",
    "\n",
    "        Variables to set:\n",
    "            input_paths: a list of two or more paths to the csv files to be served\n",
    "            group_id_column: the column that identifies which groups of rows the API should serve.\n",
    "                A call to iter_test serves all rows of all dataframes with the current group ID value.\n",
    "            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n",
    "        '''\n",
    "        self.input_paths: Sequence[str] = ['example_test_files/test.csv',\n",
    "                                   'example_test_files/revealed_targets.csv', \n",
    "                                   'example_test_files/client.csv',\n",
    "                                   'example_test_files/historical_weather.csv',\n",
    "                                   'example_test_files/forecast_weather.csv',\n",
    "                                   'example_test_files/electricity_prices.csv',\n",
    "                                   'example_test_files/gas_prices.csv',\n",
    "                                   'example_test_files/sample_submission.csv']\n",
    "        self.group_id_column: str = 'data_block_id'\n",
    "        self.export_group_id_column: bool = False\n",
    "        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n",
    "        assert len(self.input_paths) >= 2\n",
    "\n",
    "        self._status = 'initialized'\n",
    "        self.predictions = []\n",
    "\n",
    "    def iter_test(self) -> Tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Loads all of the dataframes specified in self.input_paths,\n",
    "        then yields all rows in those dataframes that equal the current self.group_id_column value.\n",
    "        '''\n",
    "        if self._status != 'initialized':\n",
    "\n",
    "            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n",
    "\n",
    "        dataframes = []\n",
    "        for pth in self.input_paths:\n",
    "            dataframes.append(pd.read_csv(pth, low_memory=False))\n",
    "        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n",
    "        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n",
    "\n",
    "        for group_id in group_order:\n",
    "            self._status = 'prediction_needed'\n",
    "            current_data = []\n",
    "            for df in dataframes:\n",
    "                cur_df = df.loc[group_id].copy()\n",
    "                # returning single line dataframes from df.loc requires special handling\n",
    "                if not isinstance(cur_df, pd.DataFrame):\n",
    "                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n",
    "                    cur_df.index.name = self.group_id_column\n",
    "                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n",
    "                current_data.append(cur_df)\n",
    "            yield tuple(current_data)\n",
    "\n",
    "            while self._status != 'prediction_received':\n",
    "                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n",
    "                yield None\n",
    "\n",
    "        with open(submission_name, 'w') as f_open:\n",
    "            pd.concat(self.predictions).to_csv(f_open, index=False)\n",
    "        self._status = 'finished'\n",
    "\n",
    "    def predict(self, user_predictions: pd.DataFrame):\n",
    "        '''\n",
    "        Accepts and stores the user's predictions and unlocks iter_test once that is done\n",
    "        '''\n",
    "        if self._status == 'finished':\n",
    "            raise Exception('You have already made predictions for the full test set.')\n",
    "        if self._status != 'prediction_needed':\n",
    "            raise Exception('You must get the next test sample from `iter_test()` first.')\n",
    "        if not isinstance(user_predictions, pd.DataFrame):\n",
    "            raise Exception('You must provide a DataFrame.')\n",
    "\n",
    "        self.predictions.append(user_predictions)\n",
    "        self._status = 'prediction_received'\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return MockApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    # –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º–æ–∂–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∫—É –ø—Ä–∏ —Å–æ–±–º–∏—Ç–µ –Ω–∞ –±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ –∏—Ç–µ—Ä–∞—Ü–∏–π\n",
    "    # –ê –Ω–µ —Ç–æ–ª—å–∫–æ —á–µ—Ç—ã—Ä–µ –∏—Ç—Ç–µ—Ä–∞—Ü–∏–∏ –Ω–∞ 4 –¥–Ω—è –∫–∞–∫ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∏–º–∏—Ç–∞–π–∏–∏ –Ω–∞ –∫–∞–≥–ª–µ\n",
    "    env = make_env()\n",
    "else:\n",
    "    # –∑–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è —Å–∞–±–º–∏—Ç–∞\n",
    "    import enefit\n",
    "    env = enefit.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∫–æ—Ä.\n",
    "# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º compare —Å–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "def calc_score():\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    #submission = pd.read_csv(submission_name)\n",
    "    submission = pd.concat(env.predictions)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    revealed_targets = pd.read_csv(os.path.join(test_path, \"revealed_targets.csv\"))\n",
    "    revealed_targets['data_block_id'] -= 2\n",
    "    revealed_targets = revealed_targets[revealed_targets[\"data_block_id\"] > train_end_data_block_id]\n",
    "    # –û–±—Ä–µ–∑–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è revealed_targets –ø–æ –¥–ª–∏–Ω–µ —É–∂–µ —Å–¥–µ–ª–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π submission\n",
    "    revealed_targets = revealed_targets.iloc[:len(submission)]\n",
    "\n",
    "    mae = mean_absolute_error(revealed_targets['target'] , submission['target'])\n",
    "    # print(f'MAE: {mae}')\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –º–µ—Ä–µ —É–¥–∞–ª–µ–Ω–∏—è –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    compare = revealed_targets[['data_block_id', 'is_consumption', 'target']].copy()\n",
    "    compare['predict'] = submission['target'].values\n",
    "    compare['abs_err'] = abs(compare['predict'] - compare['target']).values\n",
    "    compare['err'] = (compare['predict'] - compare['target']).values\n",
    "\n",
    "    if (compare['data_block_id'].max() > 600):\n",
    "        compare_600 = compare[compare['data_block_id'] > 600]\n",
    "        # –í—ã–≤–æ–¥–∏–º MAE –¥–ª—è data_block_id > 600\n",
    "        \n",
    "        mae_600 = mean_absolute_error(compare_600['target'], compare_600['predict'])\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º MAE –¥–ª—è data_block_id > 600 –∏ is_consumption == 0\n",
    "        mae_600_cons_0 = mean_absolute_error(compare_600[compare_600['is_consumption']==0]['target'], compare_600[compare_600['is_consumption']==0]['predict'])\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º MAE –¥–ª—è data_block_id > 600 –∏ is_consumption == 0\n",
    "        mae_600_cons_1 = mean_absolute_error(compare_600[compare_600['is_consumption']==1]['target'], compare_600[compare_600['is_consumption']==1]['predict'])\n",
    "    else:\n",
    "        # –ï—Å–ª–∏ –µ—â–µ –Ω–µ –¥–æ—à–ª–∏ –¥–æ data_block_id > 600 –Ω–µ —Å—á–∏—Ç–∞–µ–º —ç—Ç–∏ –≤–µ–ª–∏—á–∏–Ω—ã\n",
    "        mae_600, mae_600_cons_0, mae_600_cons_1 = '-', '-', '-'\n",
    "\n",
    "    mae_df = pd.DataFrame({\n",
    "        '(ALL)': mae,\n",
    "        '(> 600)': mae_600,\n",
    "        '(> 600, is_cons==0)': mae_600_cons_0,\n",
    "        '(> 600, is_cons==1)': mae_600_cons_1\n",
    "    }, index=['MAE'])\n",
    "\n",
    "    # –û–∫—Ä—É–≥–ª—è–µ–º —á–∏—Å–ª–∞ –¥–æ –¥–≤—É—Ö –∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ —Å—Ç—Ä–æ–∫–∏\n",
    "    mae_df = mae_df.round(3).astype(str)\n",
    "    \n",
    "    display(mae_df)\n",
    "    return compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for (\n",
    "    df_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices, \n",
    "    df_sample_prediction\n",
    ") in iter_test:\n",
    "    iteration_start_time = time.time()\n",
    "    print(p_time(), '*************** Iteration: ', count, '***************')\n",
    "    \n",
    "    if is_local:\n",
    "        # –ï—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ, —Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "        # –ù–∞ –∫–∞–≥–ª–µ (–∞ –º–æ–∂–µ—Ç –∏ –≤ –ª–∏–Ω—É–∫—Å–µ) –æ–Ω–∏ –∏ —Ç–∞–∫ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è, –Ω–æ –Ω–∞ –≤–∏–Ω–æ–≤—Å –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "        # –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –∏ –≤—ã–¥–µ—Ç–∞—é—Ç –ø–æ –æ—â–∏–±–∫–µ\n",
    "        df_test['prediction_datetime'] = pd.to_datetime(df_test['prediction_datetime'])\n",
    "        df_new_client['date'] = pd.to_datetime(df_new_client['date'])\n",
    "        df_new_gas_prices['origin_date'] = pd.to_datetime(df_new_gas_prices['origin_date'])\n",
    "        df_new_gas_prices['forecast_date'] = pd.to_datetime(df_new_gas_prices['forecast_date'])\n",
    "        df_new_electricity_prices['origin_date'] = pd.to_datetime(df_new_electricity_prices['origin_date'])\n",
    "        df_new_electricity_prices['forecast_date'] = pd.to_datetime(df_new_electricity_prices['forecast_date'])\n",
    "        df_new_forecast_weather['origin_datetime'] = pd.to_datetime(df_new_forecast_weather['origin_datetime'])\n",
    "        df_new_forecast_weather['forecast_datetime'] = pd.to_datetime(df_new_forecast_weather['forecast_datetime'])\n",
    "        df_new_historical_weather['datetime'] = pd.to_datetime(df_new_historical_weather['datetime'])\n",
    "        df_new_target['datetime'] = pd.to_datetime(df_new_target['datetime'])\n",
    "        \n",
    "    data_storage.update_with_new_data(\n",
    "        df_new_client=df_new_client,\n",
    "        df_new_gas_prices=df_new_gas_prices,\n",
    "        df_new_electricity_prices=df_new_electricity_prices,\n",
    "        df_new_forecast_weather=df_new_forecast_weather,\n",
    "        df_new_historical_weather=df_new_historical_weather,\n",
    "        df_new_target=df_new_target\n",
    "    )\n",
    "    \n",
    "    if (not(is_local) or (count == 0) or (count>=100)):\n",
    "    #if (not(is_local) or (count == 0) or (count>=0)):\n",
    "        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ\n",
    "        # –∏—Ç–µ—Ä–∞—Ü–∏–∏ 100. –ü–æ—Ç–æ–º—É —á—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∫–∞–∫ –≤–µ–¥–µ—Ç —Å–µ–±—è –º–æ–¥–µ–ª—å—è —á–µ—Ä–µ–∑ –¥–≤–∞ –º–µ—Å—è—Ü–∞\n",
    "        # –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "        cur_time = time.time()\n",
    "        if (((cur_time - notebook_starttime) < (8*60*60 + 60*30)) or is_disable_run_time_limit):\n",
    "            if ((count % learn_again_period) == 0):\n",
    "                if (df_test['prediction_datetime'].max() >= scor_start_time or (count == 0) or is_local):\n",
    "                    print(p_time(), '–§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏')\n",
    "                    df_train_features = features_generator.generate_features(data_storage.df_data)\n",
    "                    df_train_features = df_train_features[df_train_features['target'].notnull()]\n",
    "                    print('df_train_features shape', df_train_features.shape[0])\n",
    "                    print(p_time(), '–¢—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å')\n",
    "                    model.fit(df_train_features)\n",
    "                else:\n",
    "                    print('–ù–µ —Ç—Ä–µ–Ω–µ—Ä—É–µ–º –º–æ–¥–µ–ª—å', df_test['datetime'].max(), '–Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∞ –¥–∞—Ç—ã –Ω–∞—á–∞–ª–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏:', scor_start_time)\n",
    "\n",
    "        else:\n",
    "            print('–ù–µ —Ç—Ä–µ–Ω–µ—Ä—É–µ–º –º–æ–¥–µ–ª—å, –ø—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–æ—É—Ç–±—É–∫–∞:', (cur_time - notebook_starttime))\n",
    "\n",
    "    print(p_time(), '–î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')\n",
    "    df_test = data_storage.preprocess_test(df_test)\n",
    "    \n",
    "    df_test_features = features_generator.generate_features(df_test)\n",
    "    df_sample_prediction[\"target\"] = model.predict(df_test_features)\n",
    "    \n",
    "    env.predict(df_sample_prediction)\n",
    "    if is_local:\n",
    "        # –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—É—â–∏–π —Å–∫–æ—Ä –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑—Ä–µ–∑–∞—Ö\n",
    "        compare = calc_score()\n",
    "        \n",
    "    count += 1\n",
    "    print(p_time(), 'Iteration run time:', round(time.time() - iteration_start_time))\n",
    "    print('')\n",
    "    print('________________________________________________')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥—Å—á–µ—Ç —Å–∫–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = calc_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ì—Ä–∞—Ñ–∏–∫ MAE –ø–æ –¥–Ω—è–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –≤—ã–≤–æ–¥–∏—Ç –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –æ—à–∏–±–æ–∫ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º (—Ç–æ—á–Ω–µ–µ –¥–ª—è –±–ª–æ–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫–æ—Ç–æ—Ä—ã–µ –≤ —Ü–µ–ª–æ–º —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã –¥–Ω—è–º)\n",
    "def print_err(err_name, err_lable, err_title):\n",
    "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ data_block_id, —Ç–æ –µ—Å—Ç—å –ø–æ –¥–Ω—è–º –∏ —Å—á–∏—Ç–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–Ω—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è MAE\n",
    "    grouped_compare = compare.groupby('data_block_id').mean().reset_index()\n",
    "    # –î–µ–ª–∞–µ–º —Å–∫–æ–ª—å–∑—è—â—É—é —Å—Ä–µ–¥–Ω—é—é\n",
    "    grouped_compare['rolling_mean'] = grouped_compare[err_name].rolling(window=30, min_periods=1).mean()\n",
    "    \n",
    "    \n",
    "    # Plotting the mean absolute errors\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    #plt.bar(grouped_compare['data_block_id'], grouped_compare['abs_err'])\n",
    "    plt.bar(grouped_compare['data_block_id'], grouped_compare[err_name], label=err_lable)\n",
    "    plt.plot(grouped_compare['data_block_id'],\n",
    "             grouped_compare['rolling_mean'],\n",
    "             label='Rolling Mean (window=30)',\n",
    "             color='orange',\n",
    "             linestyle='-', linewidth=2)\n",
    "    plt.xlabel('data_block_id')\n",
    "    plt.ylabel(err_lable)\n",
    "    plt.title(err_title)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set ticks every 10 data_block_id\n",
    "    tick_positions_y = np.arange(-40, max(grouped_compare[err_name]) + 1, 10)\n",
    "    plt.yticks(tick_positions_y)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    print_err(err_name='abs_err',\n",
    "              err_lable='Mean Absolute Error',\n",
    "              err_title='Mean Absolute Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_local:\n",
    "    compare = compare[compare[\"data_block_id\"] > 600]\n",
    "    print_err(err_name='err',\n",
    "              err_lable='Mean Error (predict - target)',\n",
    "              err_title='Mean Error by data_block_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
